{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPR2lo0QE+X48b+UwRTh04t"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Link al libro que estoy usando para armar el LLM: [Build a Large Language Model from Scratch - Sebastian Raschka](https://drive.google.com/file/d/1WkucQZgK4RENhGG_lm75yRqcQWw-Pxzk/view?usp=sharing)\n",
        "\n",
        "# **Capítulo 1:** El capítulo 1 se encuentra el link al pdf dispinible para descarga en el link anterior. Es puramente introductorio les recomiendo leer el primer capítulo.\n",
        "\n",
        "# **Capitulo 2:** [Celdas de código mas comentarios explicativos.](#scrollTo=9l55Q2tNWDdU)\n",
        "\n",
        "# **Capitulo 3:** [Celdas de código mas comentarios explicativos.](#scrollTo=LQEg0nPD6-ux)\n"
      ],
      "metadata": {
        "id": "fXmyeoyi73kj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l55Q2tNWDdU",
        "outputId": "44829222-53ac-4fd9-f0d5-8fffbc711ea6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de caracteres: 20479.\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open('./sample_data/llm_from_scratch/the-verdict.txt', 'r', encoding='utf-8') as file:\n",
        "    raw_text = file.read()\n",
        "\n",
        "print(f'Número total de caracteres: {len(raw_text)}.')\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Hace esto, línea por línea:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "with open('./sample_data/llm_from_scratch/the-verdict.txt', 'r', encoding='utf-8') as file:\n",
        "```\n",
        "\n",
        "* `open(path, mode, encoding=...)`: abre un archivo y devuelve un objeto fichero.\n",
        "\n",
        "  * `path`: `'./sample_data/llm_from_scratch/the-verdict.txt'`. Ruta relativa al **directorio de trabajo actual**. En Colab suele ser `/content`. `./` apunta ahí.\n",
        "  * `mode='r'`: modo lectura de texto. No crea ni modifica el archivo.\n",
        "  * `encoding='utf-8'`: decodifica bytes a texto Unicode con UTF-8. Evita errores de acentos.\n",
        "* `with ... as file:` usa un **context manager**. Garantiza cierre del archivo al salir del bloque, incluso si hay excepciones. Evita fugas de descriptores.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "    raw_text = file.read()\n",
        "```\n",
        "\n",
        "* `file.read()` lee **todo** el contenido en memoria y retorna un `str` (Unicode). Complejidad O(n).\n",
        "* Útil para archivos pequeños/medianos. Para archivos muy grandes, preferir lectura por trozos o iterar líneas.\n",
        "\n",
        "3.\n",
        "\n",
        "```python\n",
        "print(f'Número total de caracteres: {len(raw_text)}.')\n",
        "```\n",
        "\n",
        "* `len(raw_text)`: número de **caracteres Unicode** del `str`. No son bytes.\n",
        "* Un “carácter” aquí es un **code point**. Algunos grafemas visibles pueden componerse de varios code points; `len` contaría más de uno en ese caso.\n",
        "* `f'...'`: f-string. Evalúa expresiones dentro de `{}` y las inserta en el texto.\n",
        "\n",
        "4.\n",
        "\n",
        "```python\n",
        "print(raw_text[:99])\n",
        "```\n",
        "\n",
        "* `raw_text[:99]`: **slice** desde el inicio hasta el índice 99 **excluido**. Devuelve como mucho 99 caracteres. Si el texto es más corto, no falla.\n",
        "* Útil para previsualizar el inicio del archivo.\n",
        "\n",
        "Notas prácticas en Colab:\n",
        "\n",
        "* Verifica que el archivo exista en esa ruta. Si falla, `FileNotFoundError`.\n",
        "* Si ves errores de codificación, confirma UTF-8 del archivo. Alternativa: `encoding='utf-8-sig'` si trae BOM, o `errors='replace'` para caracteres inválidos.\n",
        "* Ruta alternativa robusta:\n",
        "\n",
        "  ```python\n",
        "  from pathlib import Path\n",
        "  p = Path('sample_data/llm_from_scratch/the-verdict.txt')\n",
        "  with p.open('r', encoding='utf-8') as f:\n",
        "      raw_text = f.read()\n",
        "  ```\n",
        "* Si necesitas memoria estable, no uses `read()` con archivos muy grandes. Mejor:\n",
        "\n",
        "  ```python\n",
        "  size = 0\n",
        "  with open(path, 'r', encoding='utf-8') as f:\n",
        "      for line in f:\n",
        "          size += len(line)\n",
        "  ```"
      ],
      "metadata": {
        "id": "zrG1S5Qo72Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = 'Hey, you. This, is a test.'\n",
        "result = re.split(r'\\s', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRAeN9-M5aPd",
        "outputId": "a5fe4330-387e-4cd7-bfa3-bff31c46031a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey,', 'you.', 'This,', 'is', 'a', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación precisa línea por línea:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "import re\n",
        "```\n",
        "\n",
        "* Importa el módulo estándar **`re`** (regular expressions). Permite búsqueda, coincidencia y manipulación de texto mediante **expresiones regulares**.\n",
        "* Internamente compila patrones a autómatas finitos optimizados.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "text = 'Hey, you. This, is a test.'\n",
        "```\n",
        "\n",
        "* Declara una variable `text` tipo `str` con el contenido literal indicado.\n",
        "* Contiene palabras, comas, puntos y espacios.\n",
        "\n",
        "3.\n",
        "\n",
        "```python\n",
        "result = re.split(r'\\s', text)\n",
        "```\n",
        "\n",
        "* `re.split(pattern, string)` divide `string` en una lista, usando **las coincidencias del patrón** como separadores.\n",
        "* `r'\\s'` es un *raw string literal*: el prefijo `r` indica que `\\` se trata como carácter literal y no como escape de Python.\n",
        "* En una expresión regular, `\\s` coincide con **cualquier carácter de espacio en blanco**: espacio, tabulador, salto de línea, retorno de carro o tab vertical.\n",
        "* Por tanto, esta instrucción separa el texto cada vez que encuentra un espacio o cualquier carácter blanco.\n",
        "* Devuelve una lista de subcadenas sin los delimitadores (los espacios no se incluyen en la salida).\n",
        "\n",
        "4.\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* Imprime la lista resultante.\n",
        "* Salida esperada:\n",
        "\n",
        "  ```\n",
        "  ['Hey,', 'you.', 'This,', 'is', 'a', 'test.']\n",
        "  ```\n",
        "\n",
        "  Cada palabra o palabra con puntuación permanece unida porque `re.split` solo corta por espacios.\n",
        "\n",
        "Detalles adicionales:\n",
        "\n",
        "* Si el texto tuviera varios espacios seguidos, el resultado incluiría elementos vacíos (`''`) donde se encuentran separadores consecutivos.\n",
        "* Ejemplo: `'a  b'` → `['a', '', 'b']`.\n",
        "* Para evitar vacíos: `re.split(r'\\s+', text.strip())`, que divide por uno o más espacios y elimina espacios iniciales/finales.\n"
      ],
      "metadata": {
        "id": "3E7-Bb1gB19q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuVAWSDq53M0",
        "outputId": "667a3002-40c7-4347-d30b-c61fb1d210ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', ',', '', ' ', 'you', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explicación detallada:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "```\n",
        "\n",
        "* `re.split(patrón, texto)` divide `texto` usando las **coincidencias del patrón** como separadores.\n",
        "\n",
        "* El patrón `r'([,.]|\\s)'` se interpreta así:\n",
        "\n",
        "  * `r'...'`: *raw string literal* evita que `\\s` sea procesado como escape por Python.\n",
        "  * `(...)`: **grupo de captura**. Si el patrón contiene paréntesis, los separadores encontrados se **conservan** en la lista de salida.\n",
        "  * `[,.]`: clase de caracteres; coincide con **una coma (`,`) o un punto (`.`)**.\n",
        "  * `|`: operador **OR** lógico en expresiones regulares.\n",
        "  * `\\s`: coincide con **cualquier espacio en blanco** (espacio, tabulador, salto de línea, etc.).\n",
        "\n",
        "* En conjunto, el patrón significa:\n",
        "  “Divide el texto cada vez que aparezca una coma, un punto o un espacio, e incluye ese carácter separador como elemento independiente en el resultado.”\n",
        "\n",
        "* El grupo de captura `(...)` es lo que diferencia este ejemplo del anterior: sin él, los separadores se descartarían; con él, se **mantienen** en la lista.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* Muestra la lista completa generada.\n",
        "* Para `text = 'Hey, you. This, is a test.'`, el resultado es:\n",
        "\n",
        "  ```\n",
        "  ['Hey', ',', '', ' ', 'you', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n",
        "  ```\n",
        "\n",
        "Análisis del resultado:\n",
        "\n",
        "* Cada palabra o signo está separado individualmente.\n",
        "* Los elementos vacíos (`''`) aparecen porque `re.split` puede generar subcadenas vacías cuando dos separadores se suceden (por ejemplo, una coma seguida de un espacio).\n",
        "* Los separadores detectados (`,`, `.`, `' '`) se mantienen en la lista.\n",
        "\n",
        "Uso posterior:\n",
        "\n",
        "* Estos resultados suelen limpiarse con una lista por comprensión:\n",
        "\n",
        "  ```python\n",
        "  result = [item for item in result if item.strip()]\n",
        "  ```\n",
        "\n",
        "  que elimina los vacíos y deja solo palabras y signos de puntuación separados.\n"
      ],
      "metadata": {
        "id": "dyc_-X-rD9pJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()] # list comprehension\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V__CB8yR63b3",
        "outputId": "5539b3f6-655d-4f6d-cfd1-25915142177d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', ',', 'you', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exacta:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "result = [item for item in result if item.strip()]\n",
        "```\n",
        "\n",
        "* Es una **lista por comprensión** (*list comprehension*).\n",
        "* Recorre cada elemento `item` dentro de la lista `result` anterior.\n",
        "* `item.strip()` ejecuta el método `str.strip()` de Python, que:\n",
        "\n",
        "  * Elimina todos los caracteres de espacio en blanco al inicio y final del string (`' '`, `\\t`, `\\n`, etc.).\n",
        "  * Devuelve el texto sin esos espacios.\n",
        "* En un contexto booleano, un string vacío `''` evalúa como `False`.\n",
        "* Así, la condición `if item.strip()` **filtra** y mantiene solo los elementos cuyo resultado no está vacío.\n",
        "* Resultado: se eliminan todos los tokens vacíos (`''`) generados por separadores consecutivos o espacios redundantes.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* Imprime la nueva lista limpia, sin elementos vacíos.\n",
        "* Con `text = 'Hey, you. This, is a test.'`, la salida será:\n",
        "\n",
        "  ```\n",
        "  ['Hey', ',', 'you', '.', 'This', ',', 'is', 'a', 'test', '.']\n",
        "  ```\n",
        "\n",
        "Resultado final:\n",
        "\n",
        "* Cada palabra y signo de puntuación es un token independiente.\n",
        "* Se conserva el orden original del texto.\n",
        "* Ya no hay espacios ni cadenas vacías.\n",
        "\n",
        "Importancia:\n",
        "\n",
        "* Este filtrado prepara el texto para construir vocabularios o convertir tokens en IDs numéricos, pasos previos al entrenamiento de un modelo de lenguaje.\n"
      ],
      "metadata": {
        "id": "XMiEsTBmQDG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Hey, you. Is ths-- a test?'\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSf0Tw7ADtHg",
        "outputId": "b5c1fe8c-e82d-4b3f-9463-9c47d31ff2c2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hey', ',', 'you', '.', 'Is', 'ths', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva:\n",
        "\n",
        "1.\n",
        "\n",
        "```python\n",
        "text = 'Hey, you. Is ths-- a test?'\n",
        "```\n",
        "\n",
        "* Define la variable `text` como una cadena (`str`).\n",
        "* Contiene palabras, signos de puntuación, un doble guion (`--`) y espacios.\n",
        "* Este tipo de texto se usa para probar una tokenización más compleja.\n",
        "\n",
        "---\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "```\n",
        "\n",
        "* Usa el método `re.split()` del módulo `re` para **dividir el texto** con una expresión regular avanzada.\n",
        "\n",
        "* Desglose del patrón `r'([,.:;?_!\"()\\']|--|\\s)'`:\n",
        "\n",
        "  * `r'...'`: *raw string literal*, evita que Python interprete `\\` como secuencia de escape.\n",
        "  * `(...)`: **grupo de captura**, conserva los separadores en la salida.\n",
        "  * `[ ,.:;?_!\"()\\' ]`: **clase de caracteres**, coincide con cualquiera de estos signos de puntuación:\n",
        "    `, . : ; ? _ ! \" ( ) '`\n",
        "  * `|--`: el operador `|` indica “o”. Aquí captura explícitamente la secuencia de **doble guion** `--`.\n",
        "  * `|\\s`: también divide donde haya **espacios en blanco** (espacios, tabuladores o saltos de línea).\n",
        "\n",
        "* En conjunto, el patrón significa:\n",
        "  “Divide el texto en cada espacio o signo de puntuación del listado, y conserva esos signos como elementos separados en la lista resultante.”\n",
        "\n",
        "---\n",
        "\n",
        "3.\n",
        "\n",
        "```python\n",
        "result = [item for item in result if item.strip()]\n",
        "```\n",
        "\n",
        "* Lista por comprensión para eliminar tokens vacíos (`''`) o compuestos solo por espacios.\n",
        "* `item.strip()` elimina espacios al inicio y al final; si el resultado está vacío, se descarta.\n",
        "* Resultado: una lista de tokens limpios (palabras y signos).\n",
        "\n",
        "---\n",
        "\n",
        "4.\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* Imprime la lista final de tokens.\n",
        "* Salida esperada:\n",
        "\n",
        "  ```\n",
        "  ['Hey', ',', 'you', '.', 'Is', 'ths', '--', 'a', 'test', '?']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen funcional:**\n",
        "\n",
        "* La expresión regular separa palabras y signos de puntuación en elementos individuales.\n",
        "* El filtrado posterior elimina espacios vacíos.\n",
        "* El resultado es una **tokenización básica** útil para construir vocabularios o convertir texto en secuencias de IDs numéricos en el preprocesamiento de un LLM.\n"
      ],
      "metadata": {
        "id": "-O6Poj9LF41M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizando el texto\n",
        "\n"
      ],
      "metadata": {
        "id": "9dYa5PE2KNZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8zBEv2NJm5t",
        "outputId": "b661d668-5a9d-42e7-e943-2a659765e5f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explicación exhaustiva, línea por línea:\n",
        "\n",
        "\n",
        "### 1)\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Tokenizar el texto completo cargado en `raw_text` (por ejemplo, el cuento *The Verdict*).\n",
        "Divide el texto en unidades mínimas (*tokens*) —palabras y signos de puntuación— que luego se usarán como entrada para un modelo de lenguaje.\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* **`re.split(patrón, cadena)`**\n",
        "  Función del módulo estándar `re` (*regular expressions*) que divide una cadena (`cadena`) en una lista, usando las coincidencias del `patrón` como delimitadores.\n",
        "  A diferencia de `str.split()`, que solo separa por espacios, `re.split()` puede usar expresiones regulares complejas y permite conservar los delimitadores si están dentro de **paréntesis de captura**.\n",
        "\n",
        "* **`r'([,.:;?_!\"()\\']|--|\\s)'`**\n",
        "  Es un *raw string literal* (`r'...'`), lo que evita que Python interprete `\\` como secuencia de escape (por ejemplo, `\\s` no se convierte en “espacio”, sino que se pasa literalmente a la expresión regular).\n",
        "\n",
        "  Desglose del patrón:\n",
        "\n",
        "  * **`(...)`** → Grupo de captura. Todo lo que coincida dentro de este grupo se incluye en la lista de salida.\n",
        "  * **`[,.:;?_!\"()\\' ]`** → Clase de caracteres que captura **cualquiera** de los signos de puntuación listados:\n",
        "    `, . : ; ? _ ! \" ( ) '`\n",
        "  * **`|--`** → Alternativa literal que detecta la secuencia exacta de **doble guion** `--` (frecuente en narrativa).\n",
        "  * **`|\\s`** → Detecta cualquier carácter de espacio en blanco (`' '`, `\\t`, `\\n`, etc.).\n",
        "\n",
        "  En conjunto, el patrón significa:\n",
        "\n",
        "  > Divide el texto cada vez que aparezca una coma, punto, signo de interrogación, doble guion o espacio, e incluye esos separadores en la lista resultante.\n",
        "\n",
        "**Resultado intermedio:**\n",
        "La variable `preprocessed` se convierte en una lista en la que cada elemento puede ser:\n",
        "\n",
        "* una palabra,\n",
        "* un signo de puntuación aislado (`,`, `.`, `--`, etc.), o\n",
        "* una cadena vacía si se encuentran separadores consecutivos.\n",
        "\n",
        "---\n",
        "\n",
        "### 2)\n",
        "\n",
        "```python\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Limpiar la lista de tokens eliminando elementos vacíos o compuestos solo por espacios.\n",
        "\n",
        "**Explicación técnica:**\n",
        "\n",
        "* `item.strip()` ejecuta el método `str.strip()` sobre cada `item`:\n",
        "\n",
        "  * Quita caracteres de espacio en blanco al inicio y final del string.\n",
        "  * Devuelve una nueva cadena sin esos caracteres.\n",
        "* En un contexto booleano, un string vacío (`''`) se evalúa como `False`.\n",
        "* Por tanto, la condición `if item.strip()` **filtra** y conserva solo los elementos con contenido textual o de puntuación válido.\n",
        "* El resultado final es una lista limpia con todos los tokens “reales” que el modelo procesará.\n",
        "\n",
        "**Complejidad temporal:**\n",
        "O(n) respecto al número de tokens, ya que el filtrado recorre cada elemento una vez.\n",
        "\n",
        "---\n",
        "\n",
        "### 3)\n",
        "\n",
        "```python\n",
        "print(len(preprocessed))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Mostrar cuántos tokens (palabras y signos) contiene el texto tras la tokenización y limpieza.\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `len(preprocessed)` devuelve el número de elementos de la lista (un entero).\n",
        "* Este número equivale a la **longitud del corpus tokenizado**.\n",
        "* Permite comprobar si la división del texto produjo una cantidad razonable de tokens.\n",
        "\n",
        "  * Por ejemplo, el cuento *The Verdict* produce ~4 690 tokens sin espacios.\n",
        "\n",
        "**Consideraciones de uso:**\n",
        "\n",
        "* Este conteo no equivale a palabras “semánticas”, ya que incluye signos de puntuación como tokens separados.\n",
        "* El resultado sirve para determinar el tamaño del vocabulario base que se usará en la siguiente etapa (construcción del diccionario de tokens únicos).\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen funcional global:**\n",
        "\n",
        "1. Divide el texto en palabras y signos mediante una expresión regular amplia.\n",
        "2. Elimina vacíos y espacios innecesarios.\n",
        "3. Devuelve una lista limpia de tokens que representa la forma mínima procesable del corpus textual.\n",
        "4. Muestra el número total de tokens para verificar la segmentación antes de construir el vocabulario del modelo.\n"
      ],
      "metadata": {
        "id": "nY8ESHqiHHXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.3 Convirtiendo tokens en ID's"
      ],
      "metadata": {
        "id": "T0FNOVz5KR_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)\n",
        "\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)} # dict comprehension\n",
        "\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frSDUAJOKeIr",
        "outputId": "d61a5b0d-2115-4fb9-8287-6a4012959b27"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n",
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n",
            "('Hermia', 50)\n",
            "('His', 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### 1)\n",
        "\n",
        "```python\n",
        "all_words = sorted(set(preprocessed))\n",
        "```\n",
        "\n",
        "**Objetivo:**\n",
        "Extraer el **vocabulario único** del corpus tokenizado `preprocessed` y ordenarlo alfabéticamente.\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* `set(preprocessed)`\n",
        "\n",
        "  * Convierte la lista `preprocessed` (que contiene tokens repetidos) en un **conjunto** (`set`), una estructura de datos desordenada y sin duplicados.\n",
        "  * Cada elemento del conjunto es **único**, por lo que esta operación elimina repeticiones.\n",
        "  * Implementación interna: los `set` de Python están basados en **tablas hash**.\n",
        "\n",
        "    * La inserción y la verificación de unicidad (`hashing`) tienen complejidad media **O(1)**.\n",
        "  * Complejidad total de conversión: **O(n)**, donde *n* es el número de tokens en `preprocessed`.\n",
        "\n",
        "* `sorted(...)`\n",
        "\n",
        "  * Toma el conjunto resultante y devuelve una **lista ordenada lexicográficamente**.\n",
        "  * La ordenación se realiza con el algoritmo **Timsort** (fusión adaptativa, estable y de complejidad O(n log n)).\n",
        "  * El orden lexicográfico depende de los códigos Unicode de los caracteres, por lo que letras mayúsculas (`'A'`) aparecen antes que minúsculas (`'a'`).\n",
        "\n",
        "**Resultado:**\n",
        "Una lista (`list`) llamada `all_words` que contiene todos los **tokens únicos** (palabras y signos de puntuación), ordenados.\n",
        "\n",
        "---\n",
        "\n",
        "### 2)\n",
        "\n",
        "```python\n",
        "vocab_size = len(all_words)\n",
        "```\n",
        "\n",
        "**Objetivo:**\n",
        "Calcular el tamaño del vocabulario, es decir, cuántos tokens distintos existen.\n",
        "\n",
        "**Detalles:**\n",
        "\n",
        "* `len()` accede al atributo de longitud de la lista (`PyObject_VAR_HEAD->ob_size` en CPython).\n",
        "* Complejidad temporal: **O(1)**, ya que `len()` no recorre la lista; accede directamente al valor almacenado internamente.\n",
        "* `vocab_size` es un entero (`int`) que representa el número total de tokens únicos.\n",
        "\n",
        "---\n",
        "\n",
        "### 3)\n",
        "\n",
        "```python\n",
        "print(vocab_size)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Mostrar la cantidad de elementos únicos (palabras, signos y símbolos) en el vocabulario.\n",
        "\n",
        "* En el ejemplo del libro, para *The Verdict*, este valor es aproximadamente **1 130** tokens.\n",
        "* Este número será fundamental para:\n",
        "\n",
        "  * definir el tamaño de las matrices de embeddings,\n",
        "  * establecer la dimensionalidad de las capas de entrada y salida del modelo (por ejemplo, `nn.Embedding(vocab_size, embed_dim)` en PyTorch).\n",
        "\n",
        "---\n",
        "\n",
        "### 4)\n",
        "\n",
        "```python\n",
        "vocab = {token: integer for integer, token in enumerate(all_words)}\n",
        "```\n",
        "\n",
        "**Objetivo:**\n",
        "Crear un **diccionario de mapeo** entre tokens y sus identificadores numéricos (*token IDs*).\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* **`enumerate(all_words)`**\n",
        "\n",
        "  * Genera un iterador que produce tuplas `(índice, elemento)` para cada token en `all_words`.\n",
        "  * Por defecto, la enumeración comienza en 0.\n",
        "  * Ejemplo: `[('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), ...]`.\n",
        "  * Complejidad: **O(n)**, donde *n* es el número de tokens únicos.\n",
        "\n",
        "* **Comprensión de diccionario `{token: integer for integer, token in enumerate(all_words)}`**\n",
        "\n",
        "  * Recorre cada par `(integer, token)` producido por `enumerate`.\n",
        "  * Asigna el token (`token`) como **clave** y el número (`integer`) como **valor**.\n",
        "  * Resultado: un diccionario que implementa la función ( f : \\text{token} \\mapsto \\text{ID entero} ).\n",
        "\n",
        "**Propiedades del diccionario (`dict`):**\n",
        "\n",
        "* En Python 3.7+, los diccionarios **mantienen el orden de inserción**, por lo que el orden coincide con la lista ordenada `all_words`.\n",
        "* Internamente usan **tablas hash** (resolución abierta), lo que permite:\n",
        "\n",
        "  * Búsqueda promedio: **O(1)**.\n",
        "  * Inserción promedio: **O(1)**.\n",
        "* Tamaño de `vocab` = `vocab_size`.\n",
        "\n",
        "**Importancia:**\n",
        "Este mapeo es el núcleo del proceso de **tokenización numérica**, que permite convertir texto en tensores de enteros antes del entrenamiento del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "### 5)\n",
        "\n",
        "```python\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break\n",
        "```\n",
        "\n",
        "**Objetivo:**\n",
        "Verificar visualmente las primeras asignaciones del diccionario `vocab`.\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* **`vocab.items()`**\n",
        "\n",
        "  * Devuelve un objeto *view* que itera sobre las **tuplas (clave, valor)** del diccionario.\n",
        "  * Ejemplo: `('!', 0), ('\"', 1), (\"'\", 2), ('(', 3), ...`.\n",
        "  * Complejidad: **O(1)** por acceso, **O(n)** por recorrido completo.\n",
        "\n",
        "* **`enumerate(...)`**\n",
        "\n",
        "  * Agrega un contador `i` para poder detener el bucle después de imprimir 51 pares.\n",
        "\n",
        "* **`print(item)`**\n",
        "\n",
        "  * Muestra en consola cada par `(token, id)`.\n",
        "\n",
        "* **Condición `if i > 50: break`**\n",
        "\n",
        "  * Interrumpe el bucle tras 51 iteraciones (índices de 0 a 50).\n",
        "  * Esto evita imprimir el vocabulario completo, que puede tener cientos o miles de tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen funcional completo:**\n",
        "\n",
        "1. Se eliminan duplicados del corpus tokenizado.\n",
        "2. Se ordenan los tokens para generar una lista estable y reproducible (`all_words`).\n",
        "3. Se calcula el tamaño del vocabulario (`vocab_size`).\n",
        "4. Se crea un diccionario `vocab` que asigna a cada token un identificador entero único, base de la codificación del texto.\n",
        "5. Se imprime una muestra del mapeo para validación visual.\n",
        "\n",
        "---\n",
        "\n",
        "**Resultado conceptual:**\n",
        "Este bloque construye la **primera capa de representación simbólica** de un LLM:\n",
        "\n",
        "> un espacio discreto de vocabulario donde cada símbolo del lenguaje natural queda asociado a un índice entero, preparando el terreno para el embedding vectorial que convertirá estos índices en representaciones continuas en la siguiente etapa del pipeline.\n"
      ],
      "metadata": {
        "id": "vldhOgOlHbG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()} # ('his', 51) i= 51, s= his\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = ' '.join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "x02SO6__M7A5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Explicación exhaustiva del código y de los principios que implementa:**\n",
        "\n",
        "\n",
        "## **Clase `SimpleTokenizerV1`**\n",
        "\n",
        "Esta clase define un **tokenizador mínimo y bidireccional** que convierte texto en secuencias numéricas (*encoding*) y viceversa (*decoding*).\n",
        "Opera sobre un vocabulario (`vocab`) previamente construido que asigna **tokens → IDs numéricos**.\n",
        "\n",
        "La implementación ilustra el flujo esencial de un **preprocesador de texto para LLMs** antes de usar técnicas más avanzadas como *byte pair encoding (BPE)*.\n",
        "\n",
        "---\n",
        "\n",
        "### **1) Definición de clase**\n",
        "\n",
        "```python\n",
        "class SimpleTokenizerV1:\n",
        "```\n",
        "\n",
        "* Define un **objeto de tipo clase** en Python que agrupa datos (atributos) y comportamiento (métodos).\n",
        "* Sirve como plantilla para crear instancias específicas del tokenizador.\n",
        "* En tiempo de ejecución, cada instancia mantiene su propio estado interno (diccionarios de mapeo, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### **2) Método `__init__`**\n",
        "\n",
        "```python\n",
        "def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}  # ('his', 51) → i=51, s='his'\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Inicializa la instancia y crea los dos diccionarios de mapeo complementarios:\n",
        "\n",
        "1. **`str_to_int`**: de token (texto) a ID (entero).\n",
        "2. **`int_to_str`**: de ID (entero) a token (texto).\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `__init__` es el **constructor** especial de Python. Se ejecuta automáticamente al crear un nuevo objeto:\n",
        "\n",
        "  ```python\n",
        "  tokenizer = SimpleTokenizerV1(vocab)\n",
        "  ```\n",
        "\n",
        "  Este llama implícitamente a `SimpleTokenizerV1.__init__(tokenizer, vocab)`.\n",
        "\n",
        "* `self.str_to_int = vocab`\n",
        "\n",
        "  * Almacena el diccionario que mapea cada token único del vocabulario a un entero (ID).\n",
        "  * Ejemplo: `{'!', 0, '\"': 1, \"'\", 2, ..., 'his': 51}`.\n",
        "\n",
        "* `self.int_to_str = {i: s for s, i in vocab.items()}`\n",
        "\n",
        "  * **Inversión del mapeo.**\n",
        "  * `vocab.items()` produce pares `(token, id)`.\n",
        "  * La comprensión de diccionario los invierte para formar `(id, token)`.\n",
        "  * Ejemplo: `{51: 'his', 52: 'her', ...}`.\n",
        "  * Esto permite reconstruir texto desde secuencias de IDs en la operación *decode*.\n",
        "  * Complejidad de construcción: **O(n)** en el número de tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### **3) Método `encode`**\n",
        "\n",
        "```python\n",
        "def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "```\n",
        "\n",
        "**Función general:**\n",
        "Convierte texto plano en una secuencia de **identificadores numéricos** según el vocabulario.\n",
        "\n",
        "**Etapas detalladas:**\n",
        "\n",
        "1. **Tokenización inicial**\n",
        "\n",
        "   ```python\n",
        "   preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "   ```\n",
        "\n",
        "   * Usa una expresión regular para dividir el texto en **palabras y signos de puntuación**.\n",
        "   * Patrón:\n",
        "\n",
        "     * `(...)`: grupo de captura (mantiene los delimitadores).\n",
        "     * `[,.?_!\"()']`: signos a separar.\n",
        "     * `|--`: doble guion literal.\n",
        "     * `|\\s`: espacio o tabulador.\n",
        "   * Ejemplo:\n",
        "     `'Hey, you!' → ['Hey', ',', '', ' ', 'you', '!', '']`\n",
        "\n",
        "2. **Limpieza de tokens**\n",
        "\n",
        "   ```python\n",
        "   preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "   ```\n",
        "\n",
        "   * `str.strip()` elimina espacios en blanco al inicio y fin de cada token.\n",
        "   * La condición `if item.strip()` elimina elementos vacíos (`''`).\n",
        "   * Resultado limpio: `['Hey', ',', 'you', '!']`.\n",
        "\n",
        "3. **Conversión a IDs**\n",
        "\n",
        "   ```python\n",
        "   ids = [self.str_to_int[s] for s in preprocessed]\n",
        "   ```\n",
        "\n",
        "   * Sustituye cada token textual por su ID numérico.\n",
        "   * Acceso al diccionario: O(1) promedio por búsqueda (tabla hash).\n",
        "   * Si un token no está en `vocab`, esta versión **lanza KeyError** (no maneja desconocidos).\n",
        "   * Ejemplo:\n",
        "     Si `vocab['Hey'] = 45`, `vocab[','] = 3`, la salida será `[45, 3, 120, 5]`.\n",
        "\n",
        "4. **Retorno**\n",
        "\n",
        "   * Devuelve una lista de enteros (`List[int]`).\n",
        "   * Este formato es compatible con frameworks como PyTorch (`torch.tensor(ids)`).\n",
        "\n",
        "---\n",
        "\n",
        "### **4) Método `decode`**\n",
        "\n",
        "```python\n",
        "def decode(self, ids):\n",
        "    text = ' '.join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "**Función general:**\n",
        "Transforma una secuencia numérica de tokens de vuelta a texto legible.\n",
        "\n",
        "**Etapas detalladas:**\n",
        "\n",
        "1. **Conversión de IDs a texto**\n",
        "\n",
        "   ```python\n",
        "   [self.int_to_str[i] for i in ids]\n",
        "   ```\n",
        "\n",
        "   * Usa el diccionario inverso `int_to_str` para reconstruir los tokens.\n",
        "   * Genera una lista de strings.\n",
        "   * Si un ID no está en el diccionario, se lanzará un `KeyError`.\n",
        "\n",
        "2. **Concatenación con espacios**\n",
        "\n",
        "   ```python\n",
        "   ' '.join([...])\n",
        "   ```\n",
        "\n",
        "   * Une todos los tokens con un **espacio** entre ellos.\n",
        "   * Ejemplo: `['Hey', ',', 'you', '!'] → 'Hey , you !'`.\n",
        "\n",
        "3. **Corrección de espaciado antes de signos**\n",
        "\n",
        "   ```python\n",
        "   re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "   ```\n",
        "\n",
        "   * Busca espacios seguidos de signos de puntuación.\n",
        "   * Patrón:\n",
        "\n",
        "     * `\\s+`: uno o más espacios.\n",
        "     * `([,.?!\"()'])`: grupo de captura con signos.\n",
        "   * Reemplazo `r'\\1'`: mantiene solo el signo, eliminando el espacio anterior.\n",
        "   * Resultado: `'Hey, you!'` (sin espacio extra antes de `,` o `!`).\n",
        "   * Complejidad: O(n) sobre la longitud del texto.\n",
        "\n",
        "4. **Retorno**\n",
        "\n",
        "   * Devuelve el texto corregido y reconstruido como `str`.\n",
        "\n",
        "---\n",
        "\n",
        "### **5) Ejemplo de uso**\n",
        "\n",
        "```python\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"It's the last he painted, you know.\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "print(tokenizer.decode(ids))\n",
        "```\n",
        "\n",
        "**Flujo interno:**\n",
        "\n",
        "1. `encode()` → tokeniza → mapea a IDs.\n",
        "2. `decode()` → convierte IDs → reconstruye texto.\n",
        "\n",
        "**Resultado esperado:**\n",
        "\n",
        "```python\n",
        "[12, 45, 78, 91, 8, 120, 5, 32, 14]  # ejemplo de IDs\n",
        "\"It's the last he painted, you know.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **6) Limitaciones del diseño V1**\n",
        "\n",
        "* No maneja **palabras desconocidas**: si un token no está en el vocabulario, lanza `KeyError`.\n",
        "* No incluye **tokens especiales** (`<|unk|>`, `<|endoftext|>`, etc.).\n",
        "* No conserva espacios exactos, por lo que pierde formato o saltos de línea.\n",
        "* Sirve como **prototipo conceptual** antes de evolucionar hacia `SimpleTokenizerV2` y finalmente hacia el **BPE tokenizer** usado en GPT.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen conceptual**\n",
        "\n",
        "| Etapa    | Entrada      | Proceso                             | Salida        | Complejidad |\n",
        "| -------- | ------------ | ----------------------------------- | ------------- | ----------- |\n",
        "| `encode` | texto crudo  | regex + limpieza + mapeo            | lista de IDs  | O(n)        |\n",
        "| `decode` | lista de IDs | reconstrucción + ajuste de espacios | texto legible | O(n)        |\n",
        "\n",
        "El objetivo pedagógico de `SimpleTokenizerV1` es mostrar **cómo un LLM traduce lenguaje natural a un espacio discreto numérico** antes de aplicar embeddings y atención. Es la base de la representación simbólica que alimenta las capas iniciales del modelo transformer.\n"
      ],
      "metadata": {
        "id": "igstkADsHqwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"It's the last he painted, you know, \"Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxJmX_pez3vB",
        "outputId": "3cb3b1f2-bb58-47cf-ba2b-26fd91f96b86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Crea una **instancia** de la clase `SimpleTokenizerV1` definida previamente, usando como entrada el diccionario `vocab`.\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `SimpleTokenizerV1(vocab)` ejecuta internamente:\n",
        "\n",
        "  ```python\n",
        "  SimpleTokenizerV1.__init__(tokenizer, vocab)\n",
        "  ```\n",
        "\n",
        "  donde `tokenizer` es el nuevo objeto creado en memoria (tipo `SimpleTokenizerV1`).\n",
        "\n",
        "* En el constructor (`__init__`):\n",
        "\n",
        "  * `self.str_to_int = vocab` asigna el vocabulario base (tokens → IDs).\n",
        "  * `self.int_to_str = {i: s for s, i in vocab.items()}` crea el diccionario inverso (IDs → tokens).\n",
        "\n",
        "**Estructura interna resultante:**\n",
        "\n",
        "* `tokenizer.str_to_int` → `{'!': 0, '\"': 1, ... 'painted': 418, ...}`\n",
        "* `tokenizer.int_to_str` → `{0: '!', 1: '\"', ... 418: 'painted', ...}`\n",
        "\n",
        "Estas estructuras residen en la memoria heap y son accedidas por referencia a través del objeto `tokenizer`.\n",
        "\n",
        "**Complejidad:**\n",
        "\n",
        "* Construcción del diccionario inverso: **O(n)** donde *n* es el tamaño del vocabulario.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "text = \"\"\"Its the last he painted, you know, \"Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Define el texto que será convertido en tokens y luego a IDs.\n",
        "\n",
        "**Detalles:**\n",
        "\n",
        "* Uso de triple comillas `\"\"\"...\"\"\"` permite escribir cadenas multilínea sin necesidad de escapes.\n",
        "  Aquí se usa en una sola línea, pero sigue siendo válido.\n",
        "* El texto incluye:\n",
        "\n",
        "  * palabras (`Its`, `the`, `last`, `painted`, `know`, `said`, etc.),\n",
        "  * signos de puntuación (`,`, `\"`, `.`),\n",
        "  * un nombre propio con mayúscula (`Mrs. Gisburn`).\n",
        "\n",
        "**Nota importante:**\n",
        "El token `\"Its\"` (sin apóstrofe) podría **no existir en el vocabulario** si el texto de entrenamiento original solo contenía `\"It's\"`.\n",
        "Dado que `SimpleTokenizerV1` **no maneja palabras desconocidas**, esto puede causar un `KeyError`.\n",
        "\n",
        "---\n",
        "\n",
        "### **3)**\n",
        "\n",
        "```python\n",
        "ids = tokenizer.encode(text)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Convierte el texto en una secuencia de enteros (*token IDs*) siguiendo el vocabulario cargado.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Etapas internas del método `encode`:**\n",
        "\n",
        "1. **Tokenización inicial**\n",
        "\n",
        "   ```python\n",
        "   preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "   ```\n",
        "\n",
        "   * Divide el texto en palabras, puntuaciones y espacios.\n",
        "   * Cada separador se conserva como elemento separado.\n",
        "   * Ejemplo intermedio:\n",
        "\n",
        "     ```\n",
        "     ['Its', ' ', 'the', ' ', 'last', ' ', 'he', ' ', 'painted', ',', ' ', 'you', ' ', 'know', ',', ' ', '\"', 'Mrs', '.', ' ', 'Gisburn', ' ', 'said', ' ', 'with', ' ', 'pardonable', ' ', 'pride', '.', '', '']\n",
        "     ```\n",
        "\n",
        "2. **Limpieza**\n",
        "\n",
        "   ```python\n",
        "   preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "   ```\n",
        "\n",
        "   * Elimina cadenas vacías y espacios redundantes.\n",
        "   * Resultado limpio:\n",
        "\n",
        "     ```\n",
        "     ['Its', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n",
        "     ```\n",
        "\n",
        "3. **Conversión a IDs**\n",
        "\n",
        "   ```python\n",
        "   ids = [self.str_to_int[s] for s in preprocessed]\n",
        "   ```\n",
        "\n",
        "   * Busca cada token en el diccionario `str_to_int`.\n",
        "   * Si todos los tokens están presentes en `vocab`, se obtiene una lista de enteros:\n",
        "\n",
        "     ```\n",
        "     [315, 27, 46, 89, 418, 3, 57, 108, 3, 1, 302, 7, 410, 85, 92, 600, 612, 7]\n",
        "     ```\n",
        "\n",
        "     *(Los valores son ilustrativos; dependen del vocabulario real.)*\n",
        "   * Si un token no existe (p. ej., `\"Its\"` en lugar de `\"It's\"`), Python lanzará:\n",
        "\n",
        "     ```\n",
        "     KeyError: 'Its'\n",
        "     ```\n",
        "\n",
        "**Razonamiento del error:**\n",
        "\n",
        "* El vocabulario proviene del cuento *The Verdict*, que probablemente no contiene `\"Its\"`.\n",
        "* La versión `\"It's\"` (con apóstrofe) sí estaría registrada como un token distinto.\n",
        "* Por tanto, el tokenizador V1 falla con palabras fuera del conjunto de entrenamiento.\n",
        "\n",
        "**Complejidad:**\n",
        "\n",
        "* Búsqueda en diccionario para *n* tokens → **O(n)** promedio, ya que cada acceso hash es O(1).\n",
        "\n",
        "---\n",
        "\n",
        "### **4)**\n",
        "\n",
        "```python\n",
        "print(ids)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Muestra la lista numérica resultante, útil para validar la conversión.\n",
        "\n",
        "* Si todos los tokens existen: imprime los **índices numéricos** del texto.\n",
        "  Ejemplo hipotético:\n",
        "\n",
        "  ```\n",
        "  [315, 27, 46, 89, 418, 3, 57, 108, 3, 1, 302, 7, 410, 85, 92, 600, 612, 7]\n",
        "  ```\n",
        "* Si hay palabras desconocidas: no imprime nada y muestra un **KeyError** interrumpiendo la ejecución.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusión operativa**\n",
        "\n",
        "| Paso                       | Acción                    | Resultado                     | Complejidad |\n",
        "| -------------------------- | ------------------------- | ----------------------------- | ----------- |\n",
        "| `SimpleTokenizerV1(vocab)` | Crea mapeos token↔ID      | Diccionarios en memoria       | O(n)        |\n",
        "| `encode(text)`             | Divide, limpia, convierte | Lista de IDs                  | O(m)        |\n",
        "| `print(ids)`               | Visualiza los IDs         | Validación de la codificación | O(m)        |\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitación clave evidenciada**\n",
        "\n",
        "El **modelo V1 no soporta palabras desconocidas**.\n",
        "Por tanto, si `KeyError: 'Its'` ocurre, se debe usar `SimpleTokenizerV2`, que reemplaza tokens no vistos con `<|unk|>` (token de desconocido), asegurando compatibilidad con cualquier texto de entrada.\n"
      ],
      "metadata": {
        "id": "f-UzhbqPILt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbr4q_A61Q6J",
        "outputId": "8ffcee8a-34a2-4937-90c4-0b47268f5891"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "print(tokenizer.decode(ids))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Reconstruir texto legible a partir de la secuencia de identificadores numéricos `ids` obtenidos con `encode()`, y mostrarlo en pantalla.\n",
        "\n",
        "---\n",
        "\n",
        "## **Desglose interno del método `decode()`**\n",
        "\n",
        "El método definido en la clase `SimpleTokenizerV1`:\n",
        "\n",
        "```python\n",
        "def decode(self, ids):\n",
        "    text = ' '.join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "### **Etapa 1 — Reconversión de IDs a tokens**\n",
        "\n",
        "```python\n",
        "[self.int_to_str[i] for i in ids]\n",
        "```\n",
        "\n",
        "* Usa el diccionario inverso `int_to_str`, construido durante la inicialización:\n",
        "\n",
        "  ```python\n",
        "  self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "  ```\n",
        "* Para cada entero `i` en la lista `ids`, obtiene el token textual original asociado.\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "```\n",
        "ids = [315, 27, 46, 89, 418, 3, 57, 108, 3, 1, 302, 7, 410, 85, 92, 600, 612, 7]\n",
        "↓\n",
        "['Its', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.']\n",
        "```\n",
        "\n",
        "* Si algún ID no está en el diccionario (poco probable, ya que `encode()` solo usa IDs válidos), Python lanza `KeyError`.\n",
        "\n",
        "* Complejidad temporal: **O(n)** en el número de IDs.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Unión de tokens con espacios**\n",
        "\n",
        "```python\n",
        "text = ' '.join([...])\n",
        "```\n",
        "\n",
        "* Concatena todos los tokens, separándolos con un **espacio**.\n",
        "* El resultado inicial conserva un espacio entre cada token, incluso antes de los signos de puntuación.\n",
        "\n",
        "**Ejemplo intermedio:**\n",
        "\n",
        "```\n",
        "'Its the last he painted , you know , \" Mrs . Gisburn said with pardonable pride .'\n",
        "```\n",
        "\n",
        "* Esta forma contiene **espacios no naturales** antes de comas, puntos o comillas.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Corrección tipográfica de espacios**\n",
        "\n",
        "```python\n",
        "text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "```\n",
        "\n",
        "**Análisis de la expresión regular:**\n",
        "\n",
        "* `re.sub(patrón, reemplazo, texto)` busca todas las coincidencias de `patrón` y las sustituye por `reemplazo`.\n",
        "\n",
        "* Patrón: `r'\\s+([,.?!\"()\\'])'`\n",
        "\n",
        "  * `\\s+` → uno o más espacios.\n",
        "  * `([,.?!\"()'])` → grupo de captura que coincide con cualquier signo de puntuación listado:\n",
        "    coma, punto, interrogación, exclamación, comillas, paréntesis o apóstrofe.\n",
        "\n",
        "* Reemplazo: `r'\\1'`\n",
        "\n",
        "  * Sustituye toda la coincidencia (espacios + signo) por **solo el signo**, manteniendo lo que se capturó entre paréntesis.\n",
        "  * Resultado: elimina los espacios que preceden a la puntuación.\n",
        "\n",
        "**Ejemplo de transformación:**\n",
        "\n",
        "```\n",
        "'Its the last he painted , you know , \" Mrs . Gisburn said with pardonable pride .'\n",
        "↓\n",
        "'Its the last he painted, you know, \"Mrs. Gisburn said with pardonable pride.'\n",
        "```\n",
        "\n",
        "**Resultado:**\n",
        "\n",
        "* Se eliminan espacios antes de los signos, pero se conservan los espacios correctos entre palabras.\n",
        "* El texto recupera una forma natural y gramaticalmente correcta.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Retorno y visualización**\n",
        "\n",
        "```python\n",
        "return text\n",
        "```\n",
        "\n",
        "* Devuelve la cadena corregida al punto de llamada.\n",
        "* `print(tokenizer.decode(ids))` muestra el resultado final.\n",
        "\n",
        "**Salida esperada:**\n",
        "\n",
        "```\n",
        "Its the last he painted, you know, \"Mrs. Gisburn said with pardonable pride.\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Resumen de flujo de datos**\n",
        "\n",
        "| Fase | Entrada                  | Operación                               | Salida                         | Complejidad |\n",
        "| ---- | ------------------------ | --------------------------------------- | ------------------------------ | ----------- |\n",
        "| 1    | `ids` (lista de enteros) | Mapea IDs → tokens                      | lista de strings               | O(n)        |\n",
        "| 2    | lista de tokens          | Une con `' '.join()`                    | texto con espacios redundantes | O(n)        |\n",
        "| 3    | texto con espacios       | `re.sub()` elimina espacios incorrectos | texto limpio                   | O(n)        |\n",
        "| 4    | texto limpio             | `print()`                               | visualización                  | O(1)        |\n",
        "\n",
        "---\n",
        "\n",
        "## **Limitación funcional**\n",
        "\n",
        "* Si algún token ID no tiene correspondencia en `int_to_str`, el método genera un error.\n",
        "* `SimpleTokenizerV1` **no distingue** entre comillas de apertura y cierre, ni conserva saltos de línea o tabulaciones originales.\n",
        "* No maneja tokens especiales como `<|endoftext|>` o `<|unk|>`.\n",
        "* El resultado textual está diseñado para **coherencia lingüística básica**, no para reconstrucción exacta de formato.\n",
        "\n",
        "---\n",
        "\n",
        "**En síntesis:**\n",
        "`decode()` implementa el paso inverso de `encode()` y ejemplifica el concepto de **reversibilidad parcial de tokenización**: a partir de una secuencia de índices numéricos, se puede recrear texto humano-legible mediante un mapeo inverso y una corrección superficial de espacios, etapa fundamental antes de evaluar la calidad de generación de un LLM.\n"
      ],
      "metadata": {
        "id": "fwp7nl5xIwlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aquí esperamos un error:\n",
        "\n",
        "text = 'Hi, do you lik tea?' # Hi es una palabra no contenida en el vocabulario generado en lso pasos anteriores\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "92gOCNBY2CEj",
        "outputId": "3eb34779-036a-47e5-d0db-50800d6e0daa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hi'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-261838254.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Hi, do you lik tea?'\u001b[0m \u001b[0;31m# Hi es una palabra no contenida en el vocabulario generado en lso pasos anteriores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2492665210.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hi'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "text = 'Hi, do you lik tea?'\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Definir una nueva cadena de texto para comprobar el comportamiento del tokenizador cuando aparecen **palabras fuera del vocabulario**.\n",
        "\n",
        "**Contexto técnico:**\n",
        "El vocabulario `vocab` se generó a partir del cuento *The Verdict* de Edith Wharton.\n",
        "Ese corpus no contiene todas las palabras del inglés; por ejemplo, es probable que no incluya:\n",
        "\n",
        "* `\"Hi\"` (saludo coloquial),\n",
        "* `\"lik\"` (error ortográfico de `\"like\"`).\n",
        "\n",
        "Por tanto, estas palabras estarán **fuera del conjunto de tokens conocidos**.\n",
        "\n",
        "**Implicación:**\n",
        "El tokenizador actual (`SimpleTokenizerV1`) no maneja “out-of-vocabulary” (OOV) tokens.\n",
        "Intentar convertir un token desconocido a su ID provocará una excepción de tipo `KeyError`.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "print(tokenizer.encode(text))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Llamar al método `encode()` para tokenizar y convertir el texto en IDs numéricos, e imprimir el resultado.\n",
        "\n",
        "---\n",
        "\n",
        "## **Flujo interno dentro de `encode()`**\n",
        "\n",
        "```python\n",
        "def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 1 — División del texto**\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "```\n",
        "\n",
        "* Usa una expresión regular para dividir el texto por signos de puntuación y espacios, manteniendo los separadores.\n",
        "* Resultado intermedio aproximado:\n",
        "\n",
        "  ```\n",
        "  ['Hi', ',', '', ' ', 'do', ' ', 'you', ' ', 'lik', ' ', 'tea', '?', '']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Limpieza**\n",
        "\n",
        "```python\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "```\n",
        "\n",
        "* Elimina espacios y cadenas vacías, dejando solo tokens válidos.\n",
        "* Resultado limpio:\n",
        "\n",
        "  ```\n",
        "  ['Hi', ',', 'do', 'you', 'lik', 'tea', '?']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Conversión a IDs**\n",
        "\n",
        "```python\n",
        "ids = [self.str_to_int[s] for s in preprocessed]\n",
        "```\n",
        "\n",
        "* Intenta acceder a cada token dentro del diccionario `self.str_to_int` (el vocabulario).\n",
        "* El tokenizador busca:\n",
        "\n",
        "  * `vocab['Hi']`\n",
        "  * `vocab[',']`\n",
        "  * `vocab['do']`\n",
        "  * `vocab['you']`\n",
        "  * `vocab['lik']`\n",
        "  * `vocab['tea']`\n",
        "  * `vocab['?']`\n",
        "\n",
        "**Resultado técnico:**\n",
        "\n",
        "* Si **todos** los tokens están presentes, devuelve una lista de enteros (`List[int]`).\n",
        "* En este caso, `\"Hi\"` y `\"lik\"` **no existen** en el vocabulario.\n",
        "* En cuanto el intérprete intenta acceder a `self.str_to_int['Hi']`, el diccionario lanza:\n",
        "\n",
        "  ```python\n",
        "  KeyError: 'Hi'\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Impresión**\n",
        "\n",
        "* La ejecución se interrumpe antes de llegar a `print()`.\n",
        "* Python muestra la traza del error:\n",
        "\n",
        "  ```\n",
        "  KeyError: 'Hi'\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "## **Análisis del error**\n",
        "\n",
        "**Tipo de error:**\n",
        "`KeyError` — ocurre cuando se intenta acceder a una clave inexistente en un diccionario.\n",
        "\n",
        "**Razón interna:**\n",
        "\n",
        "* En `SimpleTokenizerV1`, el método `encode()` no incluye ningún manejo de excepciones.\n",
        "* El acceso directo `self.str_to_int[s]` depende de que todos los tokens estén presentes.\n",
        "* Python, al no encontrar la clave `'Hi'` en la tabla hash interna del diccionario `self.str_to_int`, lanza la excepción.\n",
        "\n",
        "**Comportamiento esperado (salida):**\n",
        "\n",
        "```\n",
        "KeyError: 'Hi'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Solución conceptual (adelanto del siguiente paso del libro)**\n",
        "\n",
        "Para manejar palabras desconocidas, se usa una versión extendida del tokenizador:\n",
        "\n",
        "```python\n",
        "class SimpleTokenizerV2:\n",
        "    def encode(self, text):\n",
        "        ...\n",
        "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "```\n",
        "\n",
        "* Introduce un **token especial `<|unk|>`** (*unknown token*).\n",
        "* Cada palabra fuera del vocabulario se reemplaza automáticamente por ese símbolo.\n",
        "* Así, el método nunca falla y puede codificar cualquier texto.\n",
        "\n",
        "**Ejemplo con `SimpleTokenizerV2`:**\n",
        "\n",
        "```\n",
        "text = 'Hi, do you lik tea?'\n",
        "↓\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?']\n",
        "↓\n",
        "[1160, 3, 242, 57, 1160, 642, 10]\n",
        "```\n",
        "\n",
        "*(donde `1160` es el ID asignado a `<|unk|>` en el vocabulario extendido)*\n",
        "\n",
        "---\n",
        "\n",
        "## **Resumen**\n",
        "\n",
        "| Etapa        | Acción                  | Resultado            | Complejidad | Observación                  |   |                    |\n",
        "| ------------ | ----------------------- | -------------------- | ----------- | ---------------------------- | - | ------------------ |\n",
        "| Tokenización | `re.split`              | Palabras y signos    | O(n)        | Separa espacios y puntuación |   |                    |\n",
        "| Limpieza     | `strip` + filtro        | Tokens válidos       | O(n)        | Elimina vacíos               |   |                    |\n",
        "| Mapeo a IDs  | Búsqueda en diccionario | `KeyError` en `'Hi'` | O(n)        | Sin manejo OOV               |   |                    |\n",
        "| Solución     | Usar `<                 | unk                  | >` token    | Codificación robusta         | — | Implementado en V2 |\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión:**\n",
        "El comando `print(tokenizer.encode(text))` con `SimpleTokenizerV1` falla porque el tokenizador no reconoce “Hi” ni “lik”.\n",
        "Este comportamiento es **intencional** en la versión V1 del libro: su propósito es demostrar la necesidad de **ampliar el vocabulario** o **introducir un mecanismo de manejo de palabras desconocidas**, lo que se resolverá en la siguiente versión (`SimpleTokenizerV2`).\n"
      ],
      "metadata": {
        "id": "5puWah1HJOLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.4 Adding special context tokens** (Agregando tokens de contexto especiales)"
      ],
      "metadata": {
        "id": "NMJeK_st20_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
        "\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "id": "Op22Ylzd2PvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Construir la base del **nuevo vocabulario** de tokens, eliminando duplicados y ordenándolos alfabéticamente.\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* `set(preprocessed)`\n",
        "\n",
        "  * Convierte la lista `preprocessed` en un **conjunto** (`set`), eliminando repeticiones.\n",
        "  * Los conjuntos en Python están implementados como **tablas hash**, lo que permite verificar y almacenar unicidad en tiempo promedio **O(1)** por operación.\n",
        "  * Resultado: todos los tokens únicos del corpus.\n",
        "\n",
        "* `list(...)`\n",
        "\n",
        "  * Convierte el conjunto de nuevo en una lista indexable.\n",
        "  * Esto es necesario porque los conjuntos son desordenados y no soportan orden ni indexación.\n",
        "\n",
        "* `sorted(...)`\n",
        "\n",
        "  * Ordena lexicográficamente la lista resultante según el código Unicode de cada carácter.\n",
        "  * El algoritmo usado es **Timsort** (estable, O(n log n)).\n",
        "  * El resultado es reproducible (misma ordenación cada vez), lo cual es crucial para que los IDs sean consistentes en diferentes ejecuciones.\n",
        "\n",
        "**Resultado:**\n",
        "Una lista `all_tokens` con todos los tokens únicos del texto base (`preprocessed`), ordenados de manera determinista.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "all_tokens.extend(['<|endoftext|>', '<|unk|>'])\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Agregar dos **tokens especiales** al vocabulario.\n",
        "\n",
        "**Desglose conceptual:**\n",
        "\n",
        "* `.extend(lista)`\n",
        "\n",
        "  * Añade varios elementos al final de la lista existente (modificación in-place).\n",
        "  * Complejidad O(k), donde *k* es el número de elementos añadidos (aquí, 2).\n",
        "\n",
        "**Tokens agregados:**\n",
        "\n",
        "1. **`<|endoftext|>`**\n",
        "\n",
        "   * Marca el **final de una secuencia textual**.\n",
        "   * Se usa durante el entrenamiento y la generación de texto para indicar cuándo detener la predicción.\n",
        "   * Análogo al token `[EOS]` (*End of Sequence*) en otros modelos.\n",
        "\n",
        "2. **`<|unk|>`**\n",
        "\n",
        "   * Representa cualquier **token desconocido** (*unknown token*).\n",
        "   * Es el símbolo que sustituirá palabras no vistas durante la fase de entrenamiento (manejo OOV, *Out-Of-Vocabulary*).\n",
        "   * En la práctica, este token permite que el modelo funcione sobre cualquier texto, incluso si aparecen palabras nuevas.\n",
        "\n",
        "**Resultado:**\n",
        "`all_tokens` contiene ahora:\n",
        "\n",
        "```\n",
        "[... 'yourself', '<|endoftext|>', '<|unk|>']\n",
        "```\n",
        "\n",
        "Si el vocabulario original tenía *N* tokens, ahora tendrá *N + 2*.\n",
        "\n",
        "---\n",
        "\n",
        "### **3)**\n",
        "\n",
        "```python\n",
        "vocab = {token: integer for integer, token in enumerate(all_tokens)}\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Generar un **diccionario de mapeo** entre cada token y un identificador numérico único (índice entero).\n",
        "\n",
        "**Desglose técnico:**\n",
        "\n",
        "* `enumerate(all_tokens)`\n",
        "\n",
        "  * Devuelve un iterador que produce pares `(índice, token)`, donde el índice empieza en 0.\n",
        "  * Ejemplo:\n",
        "\n",
        "    ```\n",
        "    0: '!', 1: '\"', 2: \"'\", ..., 1127: '<|endoftext|>', 1128: '<|unk|>'\n",
        "    ```\n",
        "\n",
        "* La comprensión de diccionario:\n",
        "\n",
        "  ```python\n",
        "  {token: integer for integer, token in enumerate(all_tokens)}\n",
        "  ```\n",
        "\n",
        "  * Invierte el orden del par, de `(índice, token)` a `(token, índice)`.\n",
        "  * Cada token textual se convierte en **clave**, y su posición en **valor entero**.\n",
        "  * Implementación interna: el diccionario (`dict`) es una tabla hash con búsqueda promedio O(1).\n",
        "\n",
        "**Importancia:**\n",
        "\n",
        "* Este diccionario constituye el **vocabulario formal del modelo**.\n",
        "* El token `<|unk|>` permitirá asignar un valor por defecto a cualquier palabra fuera del vocabulario conocido.\n",
        "* El token `<|endoftext|>` servirá como marcador de final de secuencia en el entrenamiento autoregresivo del LLM.\n",
        "\n",
        "---\n",
        "\n",
        "### **4)**\n",
        "\n",
        "```python\n",
        "print(len(vocab.items()))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Verificar el tamaño total del vocabulario extendido.\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `vocab.items()` devuelve una *vista* (`dict_items`) con todos los pares `(token, id)` del diccionario.\n",
        "* `len(vocab.items())` cuenta cuántos pares existen.\n",
        "* Este valor equivale al **número total de tokens únicos + 2** (por los tokens especiales añadidos).\n",
        "\n",
        "**Ejemplo de salida:**\n",
        "\n",
        "```\n",
        "1130\n",
        "```\n",
        "\n",
        "si el vocabulario original tenía 1128 tokens únicos.\n",
        "\n",
        "**Complejidad temporal:**\n",
        "\n",
        "* `len()` sobre una estructura `dict` o `dict_items` es O(1), ya que el tamaño se almacena internamente en el encabezado del objeto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen funcional**\n",
        "\n",
        "| Línea                       | Acción                   | Resultado              | Complejidad |\n",
        "| --------------------------- | ------------------------ | ---------------------- | ----------- |\n",
        "| `set(preprocessed)`         | Eliminar duplicados      | Tokens únicos          | O(n)        |\n",
        "| `sorted(list(...))`         | Ordenar tokens           | Lista ordenada         | O(n log n)  |\n",
        "| `.extend([...])`            | Añadir tokens especiales | Lista +2 elementos     | O(1)        |\n",
        "| Diccionario por comprensión | Crear mapeo token→ID     | Diccionario `vocab`    | O(n)        |\n",
        "| `len(vocab.items())`        | Contar tokens totales    | Tamaño del vocabulario | O(1)        |\n",
        "\n",
        "---\n",
        "\n",
        "### **Concepto general**\n",
        "\n",
        "Este bloque crea un **vocabulario robusto**, apto para usarse en la versión mejorada del tokenizador (`SimpleTokenizerV2`):\n",
        "\n",
        "* Garantiza consistencia (orden determinista).\n",
        "* Soporta tokens fuera del vocabulario (`<|unk|>`).\n",
        "* Integra delimitadores de secuencia (`<|endoftext|>`).\n",
        "* Es la base para entrenar embeddings en modelos LLM de tipo GPT o Transformer decoder.\n",
        "\n",
        "En síntesis:\n",
        "\n",
        "> Aquí se define el *espacio discreto completo de símbolos* que el modelo será capaz de representar, asegurando que todo texto —conocido o no— pueda codificarse de forma numérica sin errores.\n"
      ],
      "metadata": {
        "id": "ip4JPgzqJ6AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "UqVxTJj64WLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, línea por línea:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Recorrer e imprimir los **últimos cinco elementos** del diccionario `vocab`, mostrando los tokens y sus identificadores numéricos más altos (generalmente los tokens añadidos al final, como `<|endoftext|>` y `<|unk|>`).\n",
        "\n",
        "---\n",
        "\n",
        "#### **Desglose técnico:**\n",
        "\n",
        "##### a) `vocab.items()`\n",
        "\n",
        "* Devuelve una **vista de elementos** del diccionario (`dict_items`), que contiene todos los pares `(clave, valor)` del diccionario `vocab`.\n",
        "\n",
        "  * Cada elemento es una tupla: `(token, id_entero)`.\n",
        "  * Ejemplo:\n",
        "\n",
        "    ```\n",
        "    ('yourself', 1127), ('<|endoftext|>', 1128), ('<|unk|>', 1129)\n",
        "    ```\n",
        "\n",
        "* En Python 3.7+, los diccionarios **preservan el orden de inserción**, por lo que los últimos elementos son los últimos tokens añadidos a `all_tokens`.\n",
        "\n",
        "##### b) `list(vocab.items())`\n",
        "\n",
        "* Convierte la vista `dict_items` en una **lista indexable**.\n",
        "* Esto permite aplicar slicing (`[-5:]`) para acceder a los últimos cinco elementos.\n",
        "* Complejidad temporal: **O(n)**, ya que se copian todas las entradas del diccionario a una nueva lista.\n",
        "\n",
        "##### c) `[-5:]`\n",
        "\n",
        "* Es un **slice** (rebanado) que selecciona los últimos cinco elementos de la lista.\n",
        "* Ejemplo:\n",
        "\n",
        "  ```python\n",
        "  list(vocab.items())[-5:]\n",
        "  # → [('your', 1125), ('yourself', 1126), ('<|endoftext|>', 1127), ('<|unk|>', 1128)]\n",
        "  ```\n",
        "* Si el diccionario tiene menos de 5 elementos, devuelve todos.\n",
        "\n",
        "##### d) `enumerate(...)`\n",
        "\n",
        "* Añade un contador `i` que empieza en 0 por defecto.\n",
        "\n",
        "* Devuelve pares `(i, item)`, donde:\n",
        "\n",
        "  * `i` es el índice del elemento en el bucle,\n",
        "  * `item` es la tupla `(token, id)`.\n",
        "\n",
        "* Ejemplo:\n",
        "\n",
        "  ```\n",
        "  0 ('your', 1125)\n",
        "  1 ('yourself', 1126)\n",
        "  2 ('<|endoftext|>', 1127)\n",
        "  3 ('<|unk|>', 1128)\n",
        "  ```\n",
        "\n",
        "**Complejidad general del bucle:**\n",
        "\n",
        "* O(n) por la conversión `list(vocab.items())`, pero solo recorre 5 elementos en la iteración.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "print(item)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Imprimir cada tupla `(token, id)` generada por el bucle.\n",
        "\n",
        "* Cada línea de salida mostrará un par token–índice del vocabulario.\n",
        "* Ejemplo de salida real:\n",
        "\n",
        "  ```\n",
        "  ('your', 1125)\n",
        "  ('yourself', 1126)\n",
        "  ('<|endoftext|>', 1127)\n",
        "  ('<|unk|>', 1128)\n",
        "  ```\n",
        "\n",
        "**Detalles técnicos de `print`:**\n",
        "\n",
        "* `print()` convierte cada objeto en su representación de texto (`str()` o `repr()`) y lo envía al flujo estándar de salida (`sys.stdout`).\n",
        "* Cada llamada termina en salto de línea `\\n` por defecto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumen funcional**\n",
        "\n",
        "| Etapa            | Acción                    | Resultado              | Complejidad |\n",
        "| ---------------- | ------------------------- | ---------------------- | ----------- |\n",
        "| `vocab.items()`  | Obtener pares (token, id) | vista `dict_items`     | O(1)        |\n",
        "| `list(...)`      | Convertir a lista         | lista indexable        | O(n)        |\n",
        "| `[-5:]`          | Tomar últimos 5           | sublista               | O(1)        |\n",
        "| `enumerate(...)` | Añadir índice             | pares (i, (token, id)) | O(1)        |\n",
        "| `print(item)`    | Mostrar resultado         | salida en consola      | O(1)        |\n",
        "\n",
        "---\n",
        "\n",
        "### **Resultado conceptual**\n",
        "\n",
        "Este fragmento sirve como verificación visual del **final del vocabulario** y confirma que los **tokens especiales** (`<|endoftext|>`, `<|unk|>`) fueron correctamente añadidos y asignados con los índices más altos.\n",
        "\n",
        "En contexto, los últimos valores impresos deben ser similares a:\n",
        "\n",
        "```\n",
        "('your', 1125)\n",
        "('yourself', 1126)\n",
        "('<|endoftext|>', 1127)\n",
        "('<|unk|>', 1128)\n",
        "```\n",
        "\n",
        "Esto confirma que:\n",
        "\n",
        "* El vocabulario está ordenado lexicográficamente.\n",
        "* Los dos tokens especiales fueron añadidos al final de la lista.\n",
        "* Los índices son consecutivos y únicos.\n",
        "\n",
        "En síntesis: este bucle actúa como **validación final de la integridad del vocabulario** antes de proceder a la implementación del `SimpleTokenizerV2`.\n"
      ],
      "metadata": {
        "id": "dYunPImUKfp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Listing 2.4 A simple text tokenizer that handles unknown words**\n",
        "# **Listado 2.4 Un tokenizador de texto simple que maneja palabras desconocidas**"
      ],
      "metadata": {
        "id": "IF4WunNQ13nO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Bv4KOzTV7lKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva del diseño y comportamiento de `SimpleTokenizerV2`, línea por línea y a nivel conceptual:\n",
        "\n",
        "---\n",
        "\n",
        "## **Definición general**\n",
        "\n",
        "```python\n",
        "class SimpleTokenizerV2:\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Implementar una versión **tolerante a palabras desconocidas** (*Out-Of-Vocabulary*, OOV) del tokenizador simple desarrollado previamente (`SimpleTokenizerV1`).\n",
        "\n",
        "**Contexto técnico:**\n",
        "\n",
        "* La versión anterior lanzaba un `KeyError` cuando un token no existía en el vocabulario.\n",
        "* Esta nueva versión introduce un **mecanismo de respaldo** (`<|unk|>`) que permite procesar cualquier entrada textual.\n",
        "* Representa la transición entre un tokenizador experimental y uno funcionalmente robusto para entrenamiento real de modelos.\n",
        "\n",
        "---\n",
        "\n",
        "## **1) Constructor**\n",
        "\n",
        "```python\n",
        "def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "```\n",
        "\n",
        "### **Propósito:**\n",
        "\n",
        "Inicializar los mapeos **token → id** y **id → token** para permitir conversión bidireccional.\n",
        "\n",
        "### **Detalles técnicos:**\n",
        "\n",
        "* `self.str_to_int = vocab`\n",
        "\n",
        "  * Guarda una referencia al diccionario base que mapea cadenas (tokens) a índices enteros.\n",
        "  * Este diccionario contiene ahora también los tokens especiales `<|endoftext|>` y `<|unk|>`.\n",
        "  * Acceso promedio: **O(1)** (tabla hash).\n",
        "\n",
        "* `self.int_to_str = {i: s for s, i in vocab.items()}`\n",
        "\n",
        "  * Inversión de claves y valores para decodificación.\n",
        "  * Construye un nuevo diccionario: cada id entero apunta a su token textual.\n",
        "  * Ejemplo:\n",
        "\n",
        "    ```\n",
        "    {0: '!', 1: '\"', ..., 1127: '<|endoftext|>', 1128: '<|unk|>'}\n",
        "    ```\n",
        "  * Complejidad de construcción: **O(n)** donde *n* es el número de tokens.\n",
        "  * Requiere recorrer el vocabulario completo una vez.\n",
        "\n",
        "**Resultado:**\n",
        "Dos estructuras complementarias que permiten conversión directa entre representación textual y numérica.\n",
        "\n",
        "---\n",
        "\n",
        "## **2) Método `encode()`**\n",
        "\n",
        "```python\n",
        "def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "```\n",
        "\n",
        "### **Propósito:**\n",
        "\n",
        "Convertir texto crudo (`str`) en una secuencia de identificadores numéricos (`List[int]`), asignando `<|unk|>` a los tokens no presentes en el vocabulario.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 1 — Tokenización base**\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "```\n",
        "\n",
        "* Usa la misma expresión regular que en la versión V1 para segmentar texto:\n",
        "\n",
        "  * `(...)` grupo de captura → conserva los separadores.\n",
        "  * `[,.?_!\"()']` → coincide con signos de puntuación básicos.\n",
        "  * `|--` → detecta doble guion literal.\n",
        "  * `|\\s` → divide por cualquier espacio en blanco.\n",
        "* Genera una lista con palabras, signos y separadores vacíos.\n",
        "* Complejidad: **O(n)** sobre el número de caracteres.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Limpieza de tokens**\n",
        "\n",
        "```python\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "```\n",
        "\n",
        "* `item.strip()` elimina espacios al inicio y al final del token.\n",
        "\n",
        "* El `if` descarta cadenas vacías resultantes.\n",
        "\n",
        "* Resultado: una lista limpia con palabras y signos.\n",
        "\n",
        "* Ejemplo:\n",
        "\n",
        "  ```\n",
        "  ['Hi', ',', 'do', 'you', 'lik', 'tea', '?']\n",
        "  ```\n",
        "\n",
        "* Complejidad: **O(m)**, donde *m* = número de tokens.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Manejo de palabras desconocidas**\n",
        "\n",
        "```python\n",
        "preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "```\n",
        "\n",
        "* Verifica para cada token si existe en el vocabulario (`self.str_to_int`):\n",
        "\n",
        "  * Si **existe**, lo mantiene igual.\n",
        "  * Si **no existe**, lo reemplaza por el token especial `<|unk|>`.\n",
        "* Esto evita excepciones `KeyError` y asegura que todos los tokens puedan convertirse en IDs.\n",
        "* Implementación:\n",
        "\n",
        "  * Búsqueda hash promedio O(1) por token.\n",
        "  * Complejidad total de esta etapa: O(m).\n",
        "\n",
        "**Ejemplo:**\n",
        "\n",
        "```python\n",
        "['Hi', ',', 'do', 'you', 'lik', 'tea', '?']\n",
        "↓\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?']\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Conversión a IDs**\n",
        "\n",
        "```python\n",
        "ids = [self.str_to_int[s] for s in preprocessed]\n",
        "```\n",
        "\n",
        "* Sustituye cada token textual por su identificador numérico.\n",
        "\n",
        "* Como todos los tokens ya están garantizados en el vocabulario, no hay errores.\n",
        "\n",
        "* Ejemplo hipotético:\n",
        "\n",
        "  ```\n",
        "  {'<|unk|>': 1128, ',': 3, 'do': 42, 'you': 57, 'tea': 89, '?': 9}\n",
        "  ↓\n",
        "  [1128, 3, 42, 57, 1128, 89, 9]\n",
        "  ```\n",
        "\n",
        "* Complejidad temporal: **O(m)**.\n",
        "\n",
        "* Complejidad espacial: O(m) para la lista de salida.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 5 — Retorno**\n",
        "\n",
        "```python\n",
        "return ids\n",
        "```\n",
        "\n",
        "* Devuelve una lista de enteros (`List[int]`).\n",
        "* Esta lista es adecuada para ser convertida en tensor de entrada en frameworks de entrenamiento (PyTorch, TensorFlow).\n",
        "\n",
        "---\n",
        "\n",
        "## **3) Método `decode()`**\n",
        "\n",
        "```python\n",
        "def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "### **Propósito:**\n",
        "\n",
        "Convertir una lista de identificadores numéricos en texto legible, aplicando corrección de espacios.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 1 — Reconstrucción de tokens**\n",
        "\n",
        "```python\n",
        "[self.int_to_str[i] for i in ids]\n",
        "```\n",
        "\n",
        "* Usa el diccionario inverso `int_to_str` para traducir cada ID a su token textual.\n",
        "* Complejidad: **O(m)**, acceso hash promedio O(1) por elemento.\n",
        "* Resultado intermedio (lista de strings).\n",
        "  Ejemplo:\n",
        "\n",
        "  ```\n",
        "  ['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?']\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Unión con espacios**\n",
        "\n",
        "```python\n",
        "text = \" \".join([...])\n",
        "```\n",
        "\n",
        "* Concatena los tokens con un espacio como separador.\n",
        "\n",
        "* Ejemplo:\n",
        "\n",
        "  ```\n",
        "  '<|unk|> , do you <|unk|> tea ?'\n",
        "  ```\n",
        "\n",
        "* Complejidad: O(m) respecto a longitud del texto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Limpieza tipográfica**\n",
        "\n",
        "```python\n",
        "text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "```\n",
        "\n",
        "* Usa una expresión regular para eliminar espacios redundantes antes de signos de puntuación.\n",
        "* Patrón:\n",
        "\n",
        "  * `\\s+` → uno o más espacios.\n",
        "  * `([,.?!\"()'])` → grupo de signos de puntuación comunes.\n",
        "* Reemplazo: `r'\\1'` → mantiene solo el signo, eliminando el espacio previo.\n",
        "* Ejemplo:\n",
        "\n",
        "  ```\n",
        "  '<|unk|> , do you <|unk|> tea ?'\n",
        "  ↓\n",
        "  '<|unk|>, do you <|unk|> tea?'\n",
        "  ```\n",
        "* Complejidad: O(n) sobre longitud de la cadena.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Retorno**\n",
        "\n",
        "```python\n",
        "return text\n",
        "```\n",
        "\n",
        "* Devuelve la cadena reconstruida.\n",
        "* El resultado mantiene coherencia gramatical básica, aunque puede incluir tokens `<|unk|>` que representan términos desconocidos.\n",
        "\n",
        "**Salida final:**\n",
        "\n",
        "```\n",
        "'<|unk|>, do you <|unk|> tea?'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **4) Análisis de robustez**\n",
        "\n",
        "| Característica      | `V1`         | `V2`       | Efecto                    |        |           |     |                    |\n",
        "| ------------------- | ------------ | ---------- | ------------------------- | ------ | --------- | --- | ------------------ |\n",
        "| Manejo de OOV       | ❌            | ✅          | Evita `KeyError`          |        |           |     |                    |\n",
        "| Tokens especiales   | ❌            | ✅ (`<      | unk                       | >`, `< | endoftext | >`) | Cobertura completa |\n",
        "| Consistencia de IDs | ✅            | ✅          | Índices fijos             |        |           |     |                    |\n",
        "| Formato de salida   | Básico       | Limpio     | Espacios corregidos       |        |           |     |                    |\n",
        "| Uso en modelos      | Experimental | Entrenable | Compatible con embeddings |        |           |     |                    |\n",
        "\n",
        "---\n",
        "\n",
        "## **5) Complejidad global**\n",
        "\n",
        "| Operación               | Complejidad temporal | Complejidad espacial |\n",
        "| ----------------------- | -------------------- | -------------------- |\n",
        "| Tokenización y limpieza | O(n)                 | O(n)                 |\n",
        "| Verificación OOV        | O(n)                 | O(n)                 |\n",
        "| Mapeo a IDs             | O(n)                 | O(n)                 |\n",
        "| Decodificación          | O(n)                 | O(n)                 |\n",
        "\n",
        "Donde *n* es el número de tokens procesados.\n",
        "\n",
        "---\n",
        "\n",
        "## **6) Importancia dentro del pipeline del LLM**\n",
        "\n",
        "El `SimpleTokenizerV2` introduce el primer **componente de tolerancia semántica** en el preprocesamiento:\n",
        "\n",
        "* Permite generalizar la codificación a entradas arbitrarias.\n",
        "* Garantiza que el modelo pueda recibir texto sin fallar.\n",
        "* Establece la base para el **vocabulario cerrado** usado en la pre-entrenamiento y fine-tuning de modelos autoregresivos tipo GPT.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión:**\n",
        "`SimpleTokenizerV2` convierte el prototipo V1 en un tokenizador **seguro y universal**, capaz de procesar cualquier texto de entrada sin excepciones, preservando el mapeo bidireccional entre texto e índices y respetando la estructura léxica requerida por los modelos de lenguaje basados en transformadores.\n"
      ],
      "metadata": {
        "id": "8DgybLEoK9-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = 'Hello, do you like tea?'\n",
        "text2 = 'In the sunlit terraces of the palace.'\n",
        "text = ' <|endoftext|> '.join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "id": "ehFf5s8NDjPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, con análisis estructural, sintáctico y funcional:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "text1 = 'Hello, do you like tea?'\n",
        "text2 = 'In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Definir dos fragmentos de texto (`str`) que simulan **entradas independientes** dentro de un corpus o dataset de entrenamiento.\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* Ambos objetos son instancias del tipo `str` de Python, almacenadas como secuencias inmutables de *code points* Unicode.\n",
        "* En memoria, Python usa una representación compacta (internamente UTF-8 o UCS-2/UCS-4 según compilación), y ambas cadenas residen en el heap con sus referencias gestionadas por el recolector de basura.\n",
        "* El texto está formado por palabras, espacios y signos de puntuación (`','`, `'?'`, `'.'`), los cuales más adelante serán segmentados por el tokenizador.\n",
        "\n",
        "**Semántica en el flujo del modelo:**\n",
        "\n",
        "* Cada `textX` representa una unidad de entrada independiente (por ejemplo, un documento, diálogo o frase).\n",
        "* En entrenamiento autoregresivo, se suelen concatenar múltiples secuencias con un **token delimitador** para permitir al modelo aprender los límites de contexto.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "text = ' <|endoftext|> '.join((text1, text2))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Concatenar las dos cadenas `text1` y `text2`, insertando entre ellas el **token especial de final de texto** `<|endoftext|>`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Desglose técnico:**\n",
        "\n",
        "##### a) `' <|endoftext|> '`\n",
        "\n",
        "* Es una cadena literal que contiene:\n",
        "\n",
        "  * un espacio inicial `' '`\n",
        "  * el token especial `<|endoftext|>`\n",
        "  * un espacio final `' '`\n",
        "* Este token actúa como **delimitador semántico** entre fragmentos, permitiendo distinguir el final de una secuencia y el inicio de otra dentro de un mismo batch textual.\n",
        "* Este token fue añadido explícitamente al vocabulario en pasos anteriores.\n",
        "\n",
        "##### b) `.join((text1, text2))`\n",
        "\n",
        "* `.join(iterable)` concatena los elementos del iterable especificado, separándolos por la cadena sobre la que se invoca el método.\n",
        "* El iterable aquí es una **tupla**: `(text1, text2)`.\n",
        "\n",
        "  * Las tuplas son inmutables y de acceso indexado O(1).\n",
        "* `join` recorre los elementos, los concatena, y **devuelve una nueva cadena**.\n",
        "\n",
        "  * Implementación interna: crea un buffer de tamaño exacto mediante cálculo previo de longitud total → complejidad **O(n)** sobre la suma de longitudes.\n",
        "\n",
        "##### c) Resultado exacto:\n",
        "\n",
        "```\n",
        "'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "**Estructura conceptual resultante:**\n",
        "\n",
        "```\n",
        "[secuencia_1] + [token delimitador] + [secuencia_2]\n",
        "```\n",
        "\n",
        "Este patrón es típico en *datasets concatenados de texto plano*, donde el token `<|endoftext|>` reemplaza saltos de documento o marcadores de fin de muestra.\n",
        "\n",
        "---\n",
        "\n",
        "### **3)**\n",
        "\n",
        "```python\n",
        "print(text)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Mostrar en consola el resultado de la concatenación, verificando la correcta inserción del delimitador especial.\n",
        "\n",
        "**Salida esperada:**\n",
        "\n",
        "```\n",
        "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
        "```\n",
        "\n",
        "**Detalles técnicos:**\n",
        "\n",
        "* `print()` llama internamente a `sys.stdout.write()` con una conversión implícita `str()` sobre su argumento.\n",
        "* Añade un salto de línea final (`\\n`) por defecto.\n",
        "* Operación O(n) en la longitud total del texto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Análisis conceptual**\n",
        "\n",
        "| Elemento         | Función en la arquitectura del LLM        | Implicación                                                          |                                           |                                            |\n",
        "| ---------------- | ----------------------------------------- | -------------------------------------------------------------------- | ----------------------------------------- | ------------------------------------------ |\n",
        "| `text1`, `text2` | Fragmentos independientes del corpus      | Simulan secuencias separadas de entrenamiento                        |                                           |                                            |\n",
        "| `<               | endoftext                                 | >`                                                                   | Token delimitador aprendido por el modelo | Enseña al modelo dónde termina una muestra |\n",
        "| `.join()`        | Concatenación controlada                  | Permite construir datasets textuales continuos                       |                                           |                                            |\n",
        "| Resultado final  | Texto continuo con marcador de separación | Base para el entrenamiento autoregresivo sin pérdida de segmentación |                                           |                                            |\n",
        "\n",
        "---\n",
        "\n",
        "### **Complejidad total**\n",
        "\n",
        "| Operación             | Complejidad temporal             | Espacial        |\n",
        "| --------------------- | -------------------------------- | --------------- |\n",
        "| Creación de literales | O(1)                             | O(1)            |\n",
        "| `.join()`             | O(n₁ + n₂ + k)` (longitud total) | O(n₁ + n₂ + k)` |\n",
        "| `print()`             | O(n₁ + n₂ + k)`                  | O(1)            |\n",
        "\n",
        "donde *n₁* y *n₂* son las longitudes de `text1` y `text2`, y *k* la longitud del delimitador `' <|endoftext|> '`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Significado dentro del pipeline de un LLM**\n",
        "\n",
        "Este paso demuestra la **integración práctica de tokens especiales** dentro del flujo textual.\n",
        "En los modelos tipo GPT:\n",
        "\n",
        "* `<|endoftext|>` marca la **frontera de contexto** en el corpus concatenado.\n",
        "* Durante la inferencia, el modelo puede generar este token para indicar que debe **detener la producción de texto**.\n",
        "* Durante el entrenamiento, el modelo aprende que la probabilidad de este token aumenta cuando una muestra llega a su fin.\n",
        "\n",
        "En resumen:\n",
        "\n",
        "> Este fragmento construye una cadena compuesta que respeta la semántica de fin de texto, asegurando continuidad sintáctica y segmentación explícita entre unidades lingüísticas —un paso esencial en la preparación de corpora para el entrenamiento autoregresivo de un LLM.\n"
      ],
      "metadata": {
        "id": "ocWRl6eTRygT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "id": "P7NSOTWGENal",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva del comportamiento de este bloque, tanto en nivel de ejecución como en su papel dentro del flujo del modelo de lenguaje:\n",
        "\n",
        "---\n",
        "\n",
        "### **1)**\n",
        "\n",
        "```python\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Crear una instancia del tokenizador robusto (`SimpleTokenizerV2`) empleando el vocabulario extendido que incluye los tokens especiales `<|endoftext|>` y `<|unk|>`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Comportamiento interno:**\n",
        "\n",
        "* Se ejecuta el **constructor** `__init__()` definido en la clase:\n",
        "\n",
        "  ```python\n",
        "  def __init__(self, vocab):\n",
        "      self.str_to_int = vocab\n",
        "      self.int_to_str = {i: s for s, i in vocab.items()}\n",
        "  ```\n",
        "\n",
        "* **Estructuras creadas:**\n",
        "\n",
        "  1. `self.str_to_int`: diccionario que mapea **token → ID entero**\n",
        "\n",
        "     * Ejemplo:\n",
        "\n",
        "       ```\n",
        "       {',': 3, '?': 9, 'Hello': 240, '<|endoftext|>': 1127, '<|unk|>': 1128}\n",
        "       ```\n",
        "  2. `self.int_to_str`: diccionario inverso **ID → token**, generado mediante comprensión:\n",
        "\n",
        "     ```python\n",
        "     {0: '!', 1: '\"', ..., 1127: '<|endoftext|>', 1128: '<|unk|>'}\n",
        "     ```\n",
        "\n",
        "* **Complejidad:**\n",
        "\n",
        "  * Construcción: O(n) (recorre el vocabulario completo).\n",
        "  * Accesos posteriores: O(1) promedio (hash table).\n",
        "\n",
        "* **Resultado:**\n",
        "  Un tokenizador funcional capaz de codificar y decodificar cualquier texto sin errores de clave.\n",
        "\n",
        "---\n",
        "\n",
        "### **2)**\n",
        "\n",
        "```python\n",
        "print(tokenizer.encode(text))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Aplicar el proceso de **tokenización y codificación numérica** sobre la cadena `text` que contiene el delimitador `<|endoftext|>` y mostrar el resultado.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Flujo interno dentro de `encode()`**\n",
        "\n",
        "```python\n",
        "def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 1 — Tokenización regular**\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "```\n",
        "\n",
        "**Entrada:**\n",
        "\n",
        "```\n",
        "text = 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "**Patrón:**\n",
        "\n",
        "* `[,.?_!\"()']` → divide por puntuación básica.\n",
        "* `|--` → captura doble guion literal.\n",
        "* `|\\s` → divide por espacios.\n",
        "\n",
        "**Resultado inicial (lista con separadores y vacíos):**\n",
        "\n",
        "```\n",
        "['Hello', ',', '', ' ', 'do', ' ', 'you', ' ', 'like', ' ', 'tea', '?', ' ', '<|endoftext|>', ' ', 'In', ' ', 'the', ' ', 'sunlit', ' ', 'terraces', ' ', 'of', ' ', 'the', ' ', 'palace', '.', '']\n",
        "```\n",
        "\n",
        "**Complejidad:** O(n) sobre número de caracteres.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 2 — Limpieza**\n",
        "\n",
        "```python\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "```\n",
        "\n",
        "* `strip()` elimina espacios al inicio y fin.\n",
        "* `if item.strip()` descarta vacíos.\n",
        "* Resultado limpio:\n",
        "\n",
        "  ```\n",
        "  ['Hello', ',', 'do', 'you', 'like', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "  ```\n",
        "\n",
        "**Complejidad:** O(m), donde *m* = número de tokens detectados.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 3 — Sustitución de palabras desconocidas**\n",
        "\n",
        "```python\n",
        "preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "```\n",
        "\n",
        "**Proceso:**\n",
        "\n",
        "* Comprueba si cada token está en `self.str_to_int`.\n",
        "* Si no, lo reemplaza por `<|unk|>`.\n",
        "\n",
        "**Caso particular:**\n",
        "\n",
        "* `\"Hello\"` y `\"like\"` probablemente **no existen** en el vocabulario del cuento *The Verdict*.\n",
        "* Por tanto, serán reemplazados por `<|unk|>`.\n",
        "\n",
        "**Resultado tras esta etapa:**\n",
        "\n",
        "```\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "```\n",
        "\n",
        "**Importante:**\n",
        "El token `<|endoftext|>` **sí existe** en el vocabulario, por lo que se conserva literalmente.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 4 — Mapeo token → ID**\n",
        "\n",
        "```python\n",
        "ids = [self.str_to_int[s] for s in preprocessed]\n",
        "```\n",
        "\n",
        "* Convierte cada token textual en su identificador numérico.\n",
        "* Acceso a diccionario: O(1) promedio por token.\n",
        "* Complejidad total: O(m).\n",
        "\n",
        "**Ejemplo de salida hipotética:**\n",
        "\n",
        "```\n",
        "[1128, 3, 42, 57, 1128, 89, 9, 1127, 215, 27, 390, 420, 84, 27, 500, 7]\n",
        "```\n",
        "\n",
        "donde:\n",
        "\n",
        "| Token    | ID (ejemplo) |    |      |\n",
        "| -------- | ------------ | -- | ---- |\n",
        "| `<       | unk          | >` | 1128 |\n",
        "| `,`      | 3            |    |      |\n",
        "| `do`     | 42           |    |      |\n",
        "| `you`    | 57           |    |      |\n",
        "| `tea`    | 89           |    |      |\n",
        "| `?`      | 9            |    |      |\n",
        "| `<       | endoftext    | >` | 1127 |\n",
        "| `In`     | 215          |    |      |\n",
        "| `the`    | 27           |    |      |\n",
        "| `palace` | 500          |    |      |\n",
        "| `.`      | 7            |    |      |\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapa 5 — Retorno y visualización**\n",
        "\n",
        "```python\n",
        "return ids\n",
        "```\n",
        "\n",
        "* Devuelve la lista de IDs enteros.\n",
        "* `print()` muestra el resultado en consola como lista Python estándar.\n",
        "\n",
        "**Salida visual esperada:**\n",
        "\n",
        "```\n",
        "[1128, 3, 42, 57, 1128, 89, 9, 1127, 215, 27, 390, 420, 84, 27, 500, 7]\n",
        "```\n",
        "\n",
        "*(Los valores dependen del orden real del vocabulario.)*\n",
        "\n",
        "---\n",
        "\n",
        "## **Análisis conceptual**\n",
        "\n",
        "| Elemento   | Significado funcional        | Papel en el LLM                               |                                                          |                                               |\n",
        "| ---------- | ---------------------------- | --------------------------------------------- | -------------------------------------------------------- | --------------------------------------------- |\n",
        "| `<         | unk                          | >`                                            | Token de sustitución para palabras fuera del vocabulario | Permite codificar cualquier texto sin errores |\n",
        "| `<         | endoftext                    | >`                                            | Delimitador explícito entre secuencias                   | Enseña al modelo los límites contextuales     |\n",
        "| `encode()` | Conversión texto → índices   | Base para construir tensores de entrenamiento |                                                          |                                               |\n",
        "| `vocab`    | Espacio discreto de símbolos | Define el universo léxico del modelo          |                                                          |                                               |\n",
        "\n",
        "---\n",
        "\n",
        "### **Complejidad global del método**\n",
        "\n",
        "| Etapa                | Operación        | Complejidad temporal | Espacial |\n",
        "| -------------------- | ---------------- | -------------------- | -------- |\n",
        "| Tokenización (regex) | `re.split()`     | O(n)                 | O(n)     |\n",
        "| Limpieza             | `strip` + filtro | O(n)                 | O(n)     |\n",
        "| Sustitución OOV      | Búsqueda hash    | O(n)                 | O(n)     |\n",
        "| Mapeo a IDs          | Diccionario hash | O(n)                 | O(n)     |\n",
        "\n",
        "Donde *n* es el número de caracteres o tokens procesados.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretación dentro del pipeline del LLM**\n",
        "\n",
        "Este paso completa la fase de **codificación simbólica robusta**:\n",
        "\n",
        "1. Garantiza que cualquier texto pueda transformarse en una secuencia de enteros, eliminando errores en la fase de batch tokenization.\n",
        "2. Mantiene los límites entre documentos con `<|endoftext|>`.\n",
        "3. Proporciona una representación **determinista y reproducible**, indispensable para entrenar embeddings o modelos de atención.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión:**\n",
        "La instrucción\n",
        "\n",
        "```python\n",
        "print(tokenizer.encode(text))\n",
        "```\n",
        "\n",
        "produce una secuencia de IDs enteros que representa de forma numérica la cadena\n",
        "\n",
        "```\n",
        "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
        "```\n",
        "\n",
        "con manejo seguro de tokens desconocidos mediante `<|unk|>` y preservación del token `<|endoftext|>`.\n",
        "Este es el paso en que el texto humano se convierte, por primera vez, en una estructura formal apta para el aprendizaje en un modelo de lenguaje.\n"
      ],
      "metadata": {
        "id": "OAA9CK7WSjwj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "id": "9c-f-9ogGwb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación exhaustiva, incluyendo funcionamiento interno completo, estructura de datos implicadas y relevancia dentro del flujo de un LLM:\n",
        "\n",
        "---\n",
        "\n",
        "## **1)**\n",
        "\n",
        "```python\n",
        "print(tokenizer.decode(tokenizer.encode(text)))\n",
        "```\n",
        "\n",
        "**Propósito:**\n",
        "Ejecutar el ciclo completo **texto → tokens → IDs → texto**, validando que el tokenizador `SimpleTokenizerV2` sea **reversible**, consistente y tolerante a palabras desconocidas (*out-of-vocabulary*, OOV).\n",
        "\n",
        "---\n",
        "\n",
        "## **2) Flujo general de ejecución**\n",
        "\n",
        "La expresión se evalúa de adentro hacia afuera:\n",
        "\n",
        "1. `tokenizer.encode(text)`\n",
        "   → convierte el texto crudo en una lista de identificadores enteros.\n",
        "2. `tokenizer.decode(...)`\n",
        "   → traduce esos identificadores de vuelta a texto.\n",
        "3. `print(...)`\n",
        "   → muestra la cadena reconstruida resultante.\n",
        "\n",
        "---\n",
        "\n",
        "## **3) Entrada inicial**\n",
        "\n",
        "```python\n",
        "text = 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "El texto contiene:\n",
        "\n",
        "* palabras comunes (`do`, `you`, `tea`, `the`, etc.),\n",
        "* palabras potencialmente desconocidas (`Hello`, `like`),\n",
        "* un token especial `<|endoftext|>` que **sí está en el vocabulario**,\n",
        "* puntuación (`?`, `,`, `.`).\n",
        "\n",
        "---\n",
        "\n",
        "## **4) Etapas internas de `encode()`**\n",
        "\n",
        "El método `encode()` ejecuta:\n",
        "\n",
        "```python\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "preprocessed = [item if item in self.str_to_int else '<|unk|>' for item in preprocessed]\n",
        "ids = [self.str_to_int[s] for s in preprocessed]\n",
        "return ids\n",
        "```\n",
        "\n",
        "### **Etapa 4.1 — Tokenización**\n",
        "\n",
        "Divide el texto en palabras, signos y espacios, conservando separadores.\n",
        "\n",
        "**Salida intermedia:**\n",
        "\n",
        "```\n",
        "['Hello', ',', 'do', 'you', 'like', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "```\n",
        "\n",
        "### **Etapa 4.2 — Sustitución de palabras fuera del vocabulario**\n",
        "\n",
        "* `\"Hello\"` y `\"like\"` no están en el vocabulario del cuento *The Verdict*.\n",
        "* Se reemplazan por `<|unk|>` (token desconocido).\n",
        "\n",
        "**Resultado limpio:**\n",
        "\n",
        "```\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "```\n",
        "\n",
        "### **Etapa 4.3 — Conversión a IDs**\n",
        "\n",
        "Cada token se transforma en su índice entero del vocabulario (`self.str_to_int`).\n",
        "\n",
        "**Ejemplo conceptual:**\n",
        "\n",
        "```\n",
        "[1128, 3, 42, 57, 1128, 89, 9, 1127, 215, 27, 390, 420, 84, 27, 500, 7]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **5) Etapas internas de `decode()`**\n",
        "\n",
        "El método `decode()`:\n",
        "\n",
        "```python\n",
        "def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text\n",
        "```\n",
        "\n",
        "### **Etapa 5.1 — Reconstrucción desde IDs**\n",
        "\n",
        "Convierte cada entero a su token textual original con `self.int_to_str[i]`.\n",
        "\n",
        "**Salida intermedia (lista de tokens):**\n",
        "\n",
        "```\n",
        "['<|unk|>', ',', 'do', 'you', '<|unk|>', 'tea', '?', '<|endoftext|>', 'In', 'the', 'sunlit', 'terraces', 'of', 'the', 'palace', '.']\n",
        "```\n",
        "\n",
        "### **Etapa 5.2 — Unión con espacios**\n",
        "\n",
        "```python\n",
        "\" \".join([...])\n",
        "```\n",
        "\n",
        "Concatena tokens con un espacio entre ellos:\n",
        "\n",
        "```\n",
        "'<|unk|> , do you <|unk|> tea ? <|endoftext|> In the sunlit terraces of the palace .'\n",
        "```\n",
        "\n",
        "### **Etapa 5.3 — Limpieza tipográfica**\n",
        "\n",
        "```python\n",
        "re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "```\n",
        "\n",
        "* Busca cualquier espacio antes de signos de puntuación y los elimina.\n",
        "* No altera los tokens `<|unk|>` ni `<|endoftext|>`, porque no coinciden con el patrón de signos.\n",
        "\n",
        "**Resultado limpio:**\n",
        "\n",
        "```\n",
        "'<|unk|>, do you <|unk|> tea? <|endoftext|> In the sunlit terraces of the palace.'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **6) Etapa final — Impresión**\n",
        "\n",
        "```python\n",
        "print(...)\n",
        "```\n",
        "\n",
        "Muestra la cadena final resultante:\n",
        "\n",
        "```\n",
        "<|unk|>, do you <|unk|> tea? <|endoftext|> In the sunlit terraces of the palace.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **7) Análisis conceptual del resultado**\n",
        "\n",
        "| Elemento    | Significado                                                 | Función en el flujo                    |                                                                    |                                               |\n",
        "| ----------- | ----------------------------------------------------------- | -------------------------------------- | ------------------------------------------------------------------ | --------------------------------------------- |\n",
        "| `<          | unk                                                         | >`                                     | Representa tokens no presentes en el vocabulario (“Hello”, “like”) | Permite codificar sin error cualquier entrada |\n",
        "| `,` `?` `.` | Signos de puntuación reconstruidos sin espacios incorrectos | Verifica consistencia del reensamblado |                                                                    |                                               |\n",
        "| `<          | endoftext                                                   | >`                                     | Delimitador explícito entre fragmentos textuales                   | Enseña límites de contexto al modelo          |\n",
        "\n",
        "---\n",
        "\n",
        "## **8) Propiedades técnicas del proceso completo**\n",
        "\n",
        "| Propiedad                           | Descripción                                                                        | Consecuencia                               |    |                                             |\n",
        "| ----------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------ | -- | ------------------------------------------- |\n",
        "| **Reversibilidad parcial**          | Las palabras conocidas se reconstruyen idénticas; las desconocidas vuelven como `< | unk                                        | >` | Pérdida controlada de información semántica |\n",
        "| **Determinismo**                    | La codificación y decodificación son funciones puras, sin aleatoriedad             | Resultados reproducibles                   |    |                                             |\n",
        "| **Consistencia del espacio léxico** | Solo aparecen tokens del vocabulario                                               | Compatible con entrenamiento de embeddings |    |                                             |\n",
        "| **Complejidad total**               | O(n) en número de tokens, tanto en encode como en decode                           | Escalable para corpus grandes              |    |                                             |\n",
        "\n",
        "---\n",
        "\n",
        "## **9) Significado dentro del pipeline del LLM**\n",
        "\n",
        "Este paso valida la **coherencia bidireccional del tokenizador**, condición necesaria para:\n",
        "\n",
        "* entrenar embeddings consistentes (`token_id → vector → token_id`);\n",
        "* garantizar que cada ID corresponde a un símbolo textual válido;\n",
        "* controlar el flujo de fin de secuencia mediante `<|endoftext|>`;\n",
        "* permitir inferencia segura con textos arbitrarios (manejo OOV).\n",
        "\n",
        "En entrenamiento real, este mecanismo garantiza que:\n",
        "\n",
        "> Cualquier texto del dataset puede ser mapeado a secuencias numéricas válidas y luego reconstruido de manera legible, asegurando estabilidad en la codificación semántica del modelo.\n",
        "\n",
        "---\n",
        "\n",
        "## **10) Resultado final**\n",
        "\n",
        "**Salida esperada en consola:**\n",
        "\n",
        "```\n",
        "<|unk|>, do you <|unk|> tea? <|endoftext|> In the sunlit terraces of the palace.\n",
        "```\n",
        "\n",
        "**Interpretación:**\n",
        "\n",
        "* El tokenizador reemplazó “Hello” y “like” por `<|unk|>`.\n",
        "* Preservó puntuación, estructura y delimitador `<|endoftext|>`.\n",
        "* El ciclo `encode → decode` demuestra que el tokenizador es **robusto, determinista y seguro frente a OOV**, cumpliendo con los requisitos de un preprocesador de texto para un LLM autoregresivo.\n"
      ],
      "metadata": {
        "id": "CLHOGePFUKiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.5 **Byte pair encoding**\n",
        "# 2.5 **Codificación de pares de bytes**"
      ],
      "metadata": {
        "id": "xcieCr6jSH1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "id": "nufc0-hcMTwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esa celda instala la librería **`tiktoken`**, que es el tokenizador oficial de OpenAI.\n",
        "\n",
        "**Explicación funcional:**\n",
        "\n",
        "* `pip` es el gestor de paquetes de Python.\n",
        "* `install` descarga e instala el paquete desde el repositorio de PyPI.\n",
        "* `tiktoken` permite convertir texto en tokens (números) del mismo modo que lo hacen modelos como GPT-2 o GPT-3.\n",
        "\n",
        "**Resultado esperado:**\n",
        "Al ejecutarse, tu entorno (Colab) mostrará mensajes de instalación y luego podrás usarlo con:\n",
        "\n",
        "```python\n",
        "import tiktoken\n",
        "```\n",
        "\n",
        "**Propósito dentro del notebook:**\n",
        "Preparar el entorno para tokenizar texto antes de crear datasets y entrenar el modelo en los pasos siguientes.\n"
      ],
      "metadata": {
        "id": "5wFSR0qXV-rX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print('tiktoken version: ', version('tiktoken'))"
      ],
      "metadata": {
        "id": "9KgfL4szOWvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación funcional de la celda:**\n",
        "\n",
        "1. `from importlib.metadata import version`\n",
        "   — Importa una función del módulo estándar de Python que permite consultar la versión instalada de un paquete.\n",
        "\n",
        "2. `import tiktoken`\n",
        "   — Carga la librería `tiktoken` previamente instalada, necesaria para tokenizar texto.\n",
        "\n",
        "3. `print('tiktoken version: ', version('tiktoken'))`\n",
        "   — Imprime en pantalla la versión exacta del paquete `tiktoken` que está activa en el entorno de ejecución.\n",
        "\n",
        "**Propósito:**\n",
        "Verificar que la instalación se realizó correctamente y conocer qué versión de `tiktoken` se usará en el notebook.\n",
        "\n",
        "**Entrada:** ninguna.\n",
        "**Salida:** una línea de texto como\n",
        "\n",
        "```\n",
        "tiktoken version:  0.7.0\n",
        "```\n",
        "\n",
        "(según la versión instalada en tu entorno).\n"
      ],
      "metadata": {
        "id": "__0H8Q7tWHW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "text = 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunknowPlace.'\n",
        "\n",
        "integers = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "\n",
        "print(integers)"
      ],
      "metadata": {
        "id": "9I1U57v-NE2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "74f50e42-1419-446a-97a1-72dd49d6c772"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tiktoken' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-74813310.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunknowPlace.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mintegers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_special\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'<|endoftext|>'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tiktoken' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación línea por línea. Precisa y técnica.\n",
        "\n",
        "1. `tokenizer = tiktoken.get_encoding('gpt2')`\n",
        "\n",
        "* Qué hace: carga un objeto `Encoding` predefinido con el vocabulario y merges BPE del tokenizador de GPT-2.\n",
        "* Firma relevante: `tiktoken.get_encoding(name: str) -> Encoding`.\n",
        "* Parámetros:\n",
        "\n",
        "  * `name` (obligatorio): cadena con el nombre del *encoding*. Ejemplos comunes: `'gpt2'`, `'p50k_base'`, `'cl100k_base'`.\n",
        "* Opcionales: esta función no expone parámetros opcionales.\n",
        "* Notas: para mapear por modelo se usa `tiktoken.encoding_for_model(model_name)`; aquí se fuerza explícitamente el esquema GPT-2.\n",
        "\n",
        "2.\n",
        "\n",
        "```python\n",
        "text = 'Hello, do you like tea? <|endoftext|> In the sunlit terraces of the someunknowPlace.'\n",
        "```\n",
        "\n",
        "* Qué hace: define la cadena de entrada a tokenizar.\n",
        "* Contenido relevante: incluye el *special token* de GPT-2 `\"<|endoftext|>\"` (ID estándar 50256 en GPT-2) incrustado en el texto.\n",
        "\n",
        "3.\n",
        "\n",
        "```python\n",
        "integers = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "```\n",
        "\n",
        "* Qué hace: tokeniza `text` y devuelve una lista de IDs enteros (`list[int]`) que representan los tokens BPE, incluyendo el *special token* permitido.\n",
        "* Firma relevante (tiktoken):\n",
        "\n",
        "  * `Encoding.encode(text: str, *, allowed_special: Union[Literal['all'], Set[str]] = set(), disallowed_special: Union[Literal['all'], Set[str]] = 'all') -> List[int]`\n",
        "* Parámetros:\n",
        "\n",
        "  * `text` (obligatorio): cadena a tokenizar.\n",
        "  * `allowed_special` (opcional, *keyword-only*): conjunto de *special tokens* que se aceptan en el texto tal cual. Aquí se pasa `{'<|endoftext|>'}` para que esa secuencia se reconozca como un único token especial y no como texto normal. Puede ser también `'all'` para permitir todos los especiales o `set()` para no permitir ninguno.\n",
        "  * `disallowed_special` (opcional, *keyword-only*; por defecto `'all'`): define qué *special tokens* están prohibidos. Si `'all'`, lanzará error si aparece algún especial no listado en `allowed_special`. Si se pasa `set()`, no prohibe ninguno adicional.\n",
        "* Resultado: `integers` es una lista de IDs en el rango del vocabulario GPT-2. En la posición donde aparece `<|endoftext|>` se incluirá su ID especial (50256) en la secuencia.\n",
        "\n",
        "4. `print(integers)`\n",
        "\n",
        "* Qué hace: imprime la lista de IDs de tokens resultante, p. ej. `[15496, 11, 360, ... , 50256, ...]`\n",
        "* Observación: la lista exacta depende del BPE de GPT-2. Palabras compuestas como `\"someunknowPlace\"` se dividirán en subpalabras según las merges del vocabulario.\n",
        "\n",
        "Notas prácticas\n",
        "\n",
        "* Si quieres ignorar cualquier *special token* escrito en el texto y tratarlos como caracteres normales, usa `tokenizer.encode(text)` con `allowed_special=set()` (por defecto) o `tokenizer.encode_ordinary(text)` que nunca reconoce especiales.\n",
        "* Para lotes: `tokenizer.encode_batch(list_of_texts, allowed_special=...)`.\n",
        "* Para decodificar: `tokenizer.decode(integers)` o `tokenizer.decode_bytes(...)` según el caso.\n"
      ],
      "metadata": {
        "id": "kMmaSjT2EYX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)\n"
      ],
      "metadata": {
        "id": "qXTBgFHYOvS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación funcional de la celda:**\n",
        "\n",
        "1. `strings = tokenizer.decode(integers)`\n",
        "\n",
        "   * Usa el método `decode` del objeto `tokenizer` (de `tiktoken`) para convertir una lista de **tokens numéricos** (`integers`) de vuelta a **texto legible**.\n",
        "   * Cada número en `integers` representa una palabra, sub-palabra o símbolo según el vocabulario del tokenizador.\n",
        "   * El método reconstruye el texto original uniendo esos fragmentos.\n",
        "\n",
        "2. `print(strings)`\n",
        "\n",
        "   * Muestra en pantalla el texto reconstruido.\n",
        "\n",
        "**Propósito:**\n",
        "Comprobar que la conversión *tokens → texto* funciona correctamente y que los números representan el texto esperado.\n",
        "\n",
        "**Entradas:**\n",
        "\n",
        "* `tokenizer`: instancia de `tiktoken.Encoding` creada antes.\n",
        "* `integers`: lista de IDs de tokens (por ejemplo `[464, 3290, 318]`).\n",
        "\n",
        "**Salida:**\n",
        "\n",
        "* `strings`: cadena de texto equivalente (por ejemplo `\"Hello world!\"`).\n",
        "\n",
        "**Uso dentro del flujo:**\n",
        "Se utiliza para verificar la relación entre el texto original, los tokens numéricos generados y su decodificación, paso previo a la creación del dataset de entrenamiento.\n"
      ],
      "metadata": {
        "id": "PqZVpJTKWWu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in integers:\n",
        "\n",
        "  token_text = tokenizer.decode([token_id])\n",
        "\n",
        "  print(f'{token_id} ----> {token_text}')"
      ],
      "metadata": {
        "id": "99oEvXhqN2gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explicación de la celda de código:\n",
        "\n",
        "```python\n",
        "for token_id in integers:\n",
        "\n",
        "  token_text = tokenizer.decode([token_id])\n",
        "\n",
        "  print(f'{token_id} ----> {token_text}')\n",
        "```\n",
        "\n",
        "### Línea 1\n",
        "\n",
        "```python\n",
        "for token_id in integers:\n",
        "```\n",
        "\n",
        "* Qué hace: inicia un bucle `for` que itera sobre cada elemento en la lista `integers`.\n",
        "* `integers` previamente contiene una lista de IDs de tokens (enteros) generados por `tokenizer.encode(...)`.\n",
        "* En cada iteración, la variable `token_id` toma uno de los valores de esa lista.\n",
        "* Sin parámetros adicionales.\n",
        "* Propósito: recorrer cada token individual para inspeccionarlo o procesarlo por separado.\n",
        "\n",
        "### Línea 2\n",
        "\n",
        "```python\n",
        "  token_text = tokenizer.decode([token_id])\n",
        "```\n",
        "\n",
        "* Qué hace: llama al método `decode` del objeto `tokenizer` (que es un objeto `Encoding` de tiktoken) para convertir un *listado* que contiene un único token ID en su representación de texto (cadena).\n",
        "* Firma aproximada: `Encoding.decode(ids: list[int], *, disallowed_special: … maybe) -> str` (según documentación y ejemplos).\n",
        "* Parámetros:\n",
        "\n",
        "  * `ids`: lista de enteros, aquí se pasa `[token_id]` (lista de un solo elemento).\n",
        "  * Otros parámetros opcionales: en versiones recientes puede haber opciones como `truncate_at_eos` u otros ajustes para especiales. Por ejemplo, en un wrapper de PyTorch se ve `decode(token_ids: List[int], truncate_at_eos: bool = True)` ([docs.pytorch.org][1]).\n",
        "* Resultado: `token_text` es la cadena de texto correspondiente al token ID. Puede incluir espacios iniciales o caracteres especiales según la tokenización BPE del modelo.\n",
        "* Notas de actualización: según documentación oficial, `Encoding.decode(...)` devolverá el texto original al agrupar una lista completa de tokens. Decodificar token por token (como aquí) puede funcionar, pero puede que algunos tokens no se decodifiquen aislados “perfectamente” debido a que algunos sub-tokens dependen del contexto (ver issue #202: decodificación de símbolos matemáticos puede fallar para `decode_single_token_bytes`). ([GitHub][2])\n",
        "* Propósito: ver qué texto corresponde a cada ID individual, útil para inspección, depuración o entender cómo el tokenizador segmenta el texto.\n",
        "\n",
        "### Línea 3\n",
        "\n",
        "```python\n",
        "  print(f'{token_id} ----> {token_text}')\n",
        "```\n",
        "\n",
        "* Qué hace: imprime una línea formateada que muestra el token ID y la cadena `token_text` a la que corresponde.\n",
        "* No hay función aparte con parámetros especiales. El f-string `f'{token_id} ----> {token_text}'` sustituye los valores correspondientes.\n",
        "* Propósito: visualización clara del mapeo “ID → texto” para cada token.\n",
        "\n",
        "### Consideraciones generales\n",
        "\n",
        "* El bucle produce, para cada token en la secuencia, una línea como:\n",
        "\n",
        "  ```\n",
        "  15496 ----> Hello\n",
        "  11 ----> ,\n",
        "  50256 ----> <|endoftext|>\n",
        "  …\n",
        "  ```\n",
        "\n",
        "  (ejemplo de posibles valores)\n",
        "* El uso de `decode([token_id])` por token individual es educativo, pero no es lo más eficiente para decodificar una frase completa (mejor pasar la lista completa de IDs a `decode`).\n",
        "* Algunos tokens pueden contener espacios al inicio o finales porque el esquema BPE de GPT-2 incluye espacios como parte del token, por ejemplo `\" world\"` vs `\"world\"`.\n",
        "* Es importante que `tokenizer` sea del mismo encoding que se usó para `encode`, de lo contrario habrá inconsistencias.\n",
        "\n",
        "### En el contexto del capítulo 2 de tu estudio\n",
        "\n",
        "* Esta celda te ayuda a entender **cómo** cada token ID se traduce de vuelta a texto legible y así ver **qué porciones de texto** fueron asignadas a cada ID.\n",
        "* Te da visibilidad del comportamiento interno del tokenizador: la segmentación, los espacios, los *special tokens*, etc.\n",
        "* Permite detectar casos especiales (como el token `<|endoftext|>`) y ver cómo se representa internamente.\n"
      ],
      "metadata": {
        "id": "8iF-owVVFec2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación detallada de la celda (con énfasis en los parámetros de las funciones):**\n",
        "\n",
        "```python\n",
        "for token_id in integers:\n",
        "    token_text = tokenizer.decode([token_id])\n",
        "    print(f'{token_id} ----> {token_text}')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Qué hace el bloque en general\n",
        "\n",
        "Recorre una lista de tokens numéricos (`integers`) y muestra la relación **ID → texto** de cada uno.\n",
        "Sirve para visualizar cómo el tokenizador representa cada fragmento del texto con un número.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Desglose línea a línea\n",
        "\n",
        "#### `for token_id in integers:`\n",
        "\n",
        "* **Entrada:** `integers` es una lista de enteros, cada uno corresponde a un *token ID*.\n",
        "  Ejemplo: `[464, 3290, 318, 262]`.\n",
        "* **Salida en cada iteración:** `token_id` toma uno de esos valores, por ejemplo `464`.\n",
        "* **Propósito:** iterar por cada número y decodificarlo por separado.\n",
        "\n",
        "---\n",
        "\n",
        "#### `token_text = tokenizer.decode([token_id])`\n",
        "\n",
        "* **Método:** `decode()` pertenece al objeto `tokenizer`, una instancia de `tiktoken.Encoding`.\n",
        "* **Parámetro recibido:**\n",
        "\n",
        "  * Espera una **lista o secuencia de IDs enteros** que representan tokens (`list[int]` o `Iterable[int]`).\n",
        "  * En este caso, se pasa **una lista con un solo elemento**: `[token_id]`.\n",
        "    → Es importante que sea lista, no un entero, porque el método espera un *conjunto de tokens*, no un solo número.\n",
        "* **Acción interna:** convierte la secuencia de IDs en texto legible, uniendo los fragmentos según las reglas del vocabulario interno de `tiktoken`.\n",
        "* **Valor devuelto:** una cadena (`str`) que representa el texto equivalente a ese token.\n",
        "  Ejemplo:\n",
        "\n",
        "  ```\n",
        "  token_id = 464\n",
        "  token_text = 'Hello'\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "#### `print(f'{token_id} ----> {token_text}')`\n",
        "\n",
        "* **Entradas:**\n",
        "\n",
        "  * `token_id`: el número del token.\n",
        "  * `token_text`: el texto correspondiente.\n",
        "* **Salida:** imprime la relación número → palabra, por ejemplo:\n",
        "\n",
        "  ```\n",
        "  464 ----> Hello\n",
        "  3290 ----> world\n",
        "  318 ----> !\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Resumen de parámetros y retornos del método `decode`\n",
        "\n",
        "| Elemento                   | Tipo esperado   | Descripción                          | Tipo devuelto            |\n",
        "| -------------------------- | --------------- | ------------------------------------ | ------------------------ |\n",
        "| `tokenizer.decode(tokens)` | `Iterable[int]` | Lista de IDs de tokens a decodificar | `str` (texto resultante) |\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Propósito práctico\n",
        "\n",
        "Este fragmento sirve para:\n",
        "\n",
        "* Entender cómo cada número (token) se traduce a texto.\n",
        "* Visualizar cómo el modelo ve el texto internamente, dividido en unidades numéricas.\n",
        "* Comprobar que la tokenización y decodificación funcionan correctamente antes de entrenar.\n",
        "\n",
        "---\n",
        "\n",
        "¿Quieres que use este mismo formato para las siguientes celdas que vayas enviando (con la tabla de parámetros y retornos incluida)?\n"
      ],
      "metadata": {
        "id": "ZOkE_APgW0S5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 2.1 BYTE PAIR ENCODING OF UNKNOWN WORDS\n",
        "# EJERCICIO 2.1 CODIFICACIÓN DE PARES DE BYTES DE PALABRAS DESCONOCIDAS\n",
        "\n",
        "Usa el tokenizador BPE de la biblioteca tiktoken sobre las palabras desconocidas «Akwirw ier» e imprime los IDs de los tokens individuales. Luego, llama a la función decode para cada uno de los enteros resultantes en esta lista para reproducir el mapeo mostrado en la Figura 2.11. Por último, llama al método decode con la lista de IDs de los tokens para verificar si puede reconstruir la entrada original, «Akwirw ier»."
      ],
      "metadata": {
        "id": "2x1QjOpGRvY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_akw = 'Akwirw ier'\n",
        "integers_akw = tokenizer.encode(text_akw)\n",
        "print(integers_akw)"
      ],
      "metadata": {
        "id": "Sfd0NymjO7mU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "ca796c62-b5c4-4fac-b1af-bb38843bf68c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Akwirw'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-507666865.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_akw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Akwirw ier'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mintegers_akw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_akw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintegers_akw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2492665210.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.?_!\"()\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Akwirw'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicación de la celda de código**\n",
        "\n",
        "```python\n",
        "text_akw = 'Akwirw ier'\n",
        "integers_akw = tokenizer.encode(text_akw)\n",
        "print(integers_akw)\n",
        "```\n",
        "\n",
        "### Línea 1\n",
        "\n",
        "```python\n",
        "text_akw = 'Akwirw ier'\n",
        "```\n",
        "\n",
        "* Asigna a la variable `text_akw` una cadena de texto con valor `'Akwirw ier'`.\n",
        "* Esta cadena será el texto que se desea tokenizar.\n",
        "* No interviene ninguna función ni clase; es solo la creación de un literal de tipo `str`.\n",
        "\n",
        "### Línea 2\n",
        "\n",
        "```python\n",
        "integers_akw = tokenizer.encode(text_akw)\n",
        "```\n",
        "\n",
        "* Llama al método `encode` del objeto `tokenizer` (instancia de `tiktoken.Encoding`).\n",
        "* **Firma relevante:**\n",
        "\n",
        "  ```python\n",
        "  Encoding.encode(\n",
        "      text: str,\n",
        "      *,\n",
        "      allowed_special: Union[Literal['all'], Set[str]] = set(),\n",
        "      disallowed_special: Union[Literal['all'], Set[str]] = 'all'\n",
        "  ) -> List[int]\n",
        "  ```\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `text` (obligatorio): la cadena que se tokeniza (`'Akwirw ier'`).\n",
        "  * `allowed_special` (opcional, por defecto `set()`): conjunto de *special tokens* permitidos en el texto. Aquí no se pasa, por lo tanto, no se permite ninguno.\n",
        "  * `disallowed_special` (opcional, por defecto `'all'`): indica qué *special tokens* no se permiten. Al ser `'all'`, lanzaría error si aparece uno no permitido.\n",
        "* **Comportamiento:**\n",
        "  Divide el texto en subunidades llamadas *tokens* según las reglas del vocabulario BPE del modelo (en este caso, GPT-2).\n",
        "  Cada token se convierte en un entero correspondiente a su índice en el vocabulario.\n",
        "  Como `'Akwirw ier'` probablemente no existe en el vocabulario GPT-2, se dividirá en sub-tokens más pequeños o incluso en caracteres sueltos.\n",
        "* **Salida:**\n",
        "  Una lista de enteros (`List[int]`), por ejemplo: `[32xxx, 502xx, ...]`, dependiendo del vocabulario usado.\n",
        "  Esta lista se almacena en la variable `integers_akw`.\n",
        "\n",
        "### Línea 3\n",
        "\n",
        "```python\n",
        "print(integers_akw)\n",
        "```\n",
        "\n",
        "* Imprime la lista de IDs de tokens resultante de la tokenización.\n",
        "* Permite visualizar cómo el modelo segmentó el texto y qué IDs asignó.\n"
      ],
      "metadata": {
        "id": "A_zF3090GGM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token_id in integers_akw:\n",
        "\n",
        "  token_text = tokenizer.decode([token_id])\n",
        "\n",
        "  print(f'{token_id} ---> {token_text}')"
      ],
      "metadata": {
        "id": "L0aE6HA1Q_pK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "9fa83695-e602-470b-92ad-d0cd9c321a07"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'integers_akw' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1955104652.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mintegers_akw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtoken_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{token_id} ---> {token_text}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'integers_akw' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "for token_id in integers_akw:\n",
        "```\n",
        "\n",
        "* **Qué hace:** inicia un bucle `for` que recorre secuencialmente todos los elementos de la lista `integers_akw`.\n",
        "* **Contexto:** `integers_akw` contiene los identificadores numéricos de los tokens generados al codificar el texto con el tokenizador GPT-2.\n",
        "* **Comportamiento:** en cada iteración, la variable `token_id` toma el valor de un ID de token.\n",
        "* **Uso:** permite procesar o inspeccionar cada token individualmente.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "token_text = tokenizer.decode([token_id])\n",
        "```\n",
        "\n",
        "* **Qué hace:** convierte el identificador numérico del token en su representación textual.\n",
        "* **Firma del método:**\n",
        "  `Encoding.decode(tokens: list[int], *, disallowed_special: Union[Literal['all'], Set[str]] = set()) -> str`\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `tokens`: lista de enteros que representan los IDs de los tokens a decodificar.\n",
        "  * `disallowed_special` (opcional): conjunto de *special tokens* que no deben decodificarse; por defecto está vacío, por lo que todos se permiten.\n",
        "* **Comportamiento interno:** el método consulta el vocabulario del tokenizador y reconstruye el fragmento de texto correspondiente.\n",
        "* **Salida:** cadena (`str`) con el texto asociado al token.\n",
        "* **Nota:** algunos tokens incluyen espacios iniciales porque el esquema de GPT-2 incorpora el espacio como parte del token para optimizar la segmentación.\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "print(f'{token_id} ---> {token_text}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el par formado por el identificador numérico y su texto correspondiente.\n",
        "* **Mecanismo:** la cadena f-string inserta los valores de las variables dentro del texto mostrado.\n",
        "* **Salida típica:**\n",
        "\n",
        "  ```\n",
        "  1234 ---> A\n",
        "  5678 ---> kw\n",
        "  9123 ---> ir\n",
        "  ```\n",
        "* **Propósito:** visualizar cómo el tokenizador segmenta el texto y qué fragmento representa cada token en el vocabulario.\n"
      ],
      "metadata": {
        "id": "xR0ydOWJIs2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strings_akw = tokenizer.decode(integers_akw)\n",
        "print(strings_akw)"
      ],
      "metadata": {
        "id": "USMLuvQGQaoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "strings_akw = tokenizer.decode(integers_akw)\n",
        "```\n",
        "\n",
        "* **Qué hace:** convierte toda la secuencia de IDs de tokens almacenada en `integers_akw` de vuelta a texto legible.\n",
        "* **Firma del método:**\n",
        "  `Encoding.decode(tokens: list[int], *, disallowed_special: Union[Literal['all'], Set[str]] = set()) -> str`\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `tokens`: lista completa de enteros que representan los IDs de los tokens a decodificar.\n",
        "  * `disallowed_special` (opcional): conjunto de *special tokens* que no deben decodificarse; por defecto está vacío, por lo que todos los tokens conocidos se convierten a texto.\n",
        "* **Comportamiento interno:** toma la lista completa de tokens, concatena las cadenas asociadas a cada ID y devuelve una reconstrucción aproximada del texto original.\n",
        "* **Salida:** una cadena (`str`) con el texto resultante, almacenada en `strings_akw`.\n",
        "* **Nota:** si el texto contenía tokens especiales, estos se representarán con sus marcadores (`<|endoftext|>`) salvo que se configuren como no permitidos.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(strings_akw)\n",
        "```\n",
        "\n",
        "* **Qué hace:** muestra en la salida estándar la cadena reconstruida.\n",
        "* **Propósito:** verificar que la decodificación de todos los tokens reproduce correctamente el texto original, confirmando la correspondencia entre los procesos de *encode* y *decode*.\n"
      ],
      "metadata": {
        "id": "06rrQw4OI-2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.6 Data sampling with a sliding window**"
      ],
      "metadata": {
        "id": "a32uEGQebx_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./sample_data/llm_from_scratch/the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "id": "4pn57IjAcQTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "with open('./sample_data/llm_from_scratch/the-verdict.txt', 'r', encoding='utf-8') as f:\n",
        "```\n",
        "\n",
        "* **Qué hace:** abre el archivo de texto `the-verdict.txt` en modo lectura (`'r'`) y con codificación UTF-8.\n",
        "* **Firma relevante:** `open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)`\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `file`: ruta del archivo a abrir (`'./sample_data/llm_from_scratch/the-verdict.txt'`).\n",
        "  * `mode`: `'r'` indica que se abrirá solo para lectura.\n",
        "  * `encoding`: `'utf-8'` especifica el formato de codificación del texto.\n",
        "  * Los demás parámetros son opcionales (por defecto no se modifican).\n",
        "* **Comportamiento:** crea un *context manager* que asigna el manejador de archivo a la variable `f`.\n",
        "* **Ventaja:** el bloque `with` garantiza que el archivo se cierre automáticamente al terminar su uso.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "  raw_text = f.read()\n",
        "```\n",
        "\n",
        "* **Qué hace:** lee todo el contenido del archivo abierto y lo almacena en la variable `raw_text`.\n",
        "* **Firma:** `file.read(size=-1) -> str`\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `size` (opcional, por defecto `-1`): número de caracteres a leer; `-1` indica que se lea todo el archivo.\n",
        "* **Salida:** una cadena de texto con el contenido completo del archivo.\n",
        "* **Resultado:** `raw_text` contiene el texto que luego será tokenizado.\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "```\n",
        "\n",
        "* **Qué hace:** convierte el texto leído (`raw_text`) en una lista de IDs de tokens según el esquema del tokenizador GPT-2.\n",
        "* **Firma:**\n",
        "  `Encoding.encode(text: str, *, allowed_special=set(), disallowed_special='all') -> List[int]`\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `text`: cadena a tokenizar.\n",
        "  * `allowed_special` y `disallowed_special`: controlan la admisión de *special tokens* (se usan los valores por defecto).\n",
        "* **Salida:** lista de enteros (`List[int]`) que representan los tokens del archivo.\n",
        "* **Resultado:** `enc_text` contiene la secuencia codificada que el modelo usaría como entrada.\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(len(enc_text))\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la longitud de la lista `enc_text`.\n",
        "* **Salida:** número total de tokens generados a partir del texto del archivo.\n",
        "* **Propósito:** medir el tamaño del conjunto de datos en términos de tokens, lo cual es fundamental para planificar entrenamiento y memoria.\n"
      ],
      "metadata": {
        "id": "0Tt9oXoDJbgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[50:]\n",
        "print(len(enc_sample))\n"
      ],
      "metadata": {
        "id": "_QhTxXxUdFdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "enc_sample = enc_text[50:]\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea una nueva lista llamada `enc_sample` que contiene una porción de `enc_text` a partir del índice 50 hasta el final.\n",
        "* **Sintaxis:** `lista[inicio:fin]` realiza un *slicing* (sublista) en Python.\n",
        "* **Parámetros implícitos:**\n",
        "\n",
        "  * `inicio = 50`: indica que la nueva lista comienza en el elemento de índice 50.\n",
        "  * `fin` no se especifica, por lo que toma todos los elementos hasta el último.\n",
        "* **Comportamiento:** copia los tokens desde la posición 50 en adelante, sin modificar la lista original.\n",
        "* **Salida:** una nueva lista `enc_sample` que contiene `len(enc_text) - 50` elementos.\n",
        "* **Uso:** permite trabajar con un subconjunto del texto tokenizado, por ejemplo para pruebas o muestreo.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(len(enc_sample))\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la longitud de la lista `enc_sample`.\n",
        "* **Resultado:** muestra el número total de tokens contenidos en esa muestra parcial.\n",
        "* **Propósito:** verificar que el *slicing* se aplicó correctamente y conocer cuántos tokens quedaron tras eliminar los primeros 50.\n"
      ],
      "metadata": {
        "id": "8NrqUctTJver"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4 # esta ventana de contexto determina cuantos tokens son inluídos en la entrada\n",
        "x = enc_sample[:context_size]\n",
        "\n",
        "y = enc_sample[1:context_size + 1]\n",
        "\n",
        "print(f'x: {x}')\n",
        "print(f'y: {y}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "6sqPxCzRdbAV",
        "outputId": "38950072-acec-46ba-bc84-66506e955260"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'enc_sample' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-365124845.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcontext_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;31m# esta ventana de contexto determina cuantos tokens son inluídos en la entrada\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_sample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcontext_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'enc_sample' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "context_size = 4\n",
        "```\n",
        "\n",
        "* **Qué hace:** define la variable `context_size` con el valor entero `4`.\n",
        "* **Propósito:** establece el tamaño del contexto o ventana de tokens que se usará para construir pares de entrada–salida en el entrenamiento del modelo.\n",
        "* En este caso, se tomarán secuencias de 4 tokens consecutivos como muestra de entrada.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "x = enc_sample[:context_size]\n",
        "```\n",
        "\n",
        "* **Qué hace:** selecciona los primeros `context_size` (4) elementos de la lista `enc_sample`.\n",
        "* **Sintaxis:** `lista[:n]` devuelve los elementos desde el índice `0` hasta `n-1`.\n",
        "* **Salida:** lista `x` que contiene los primeros 4 tokens de `enc_sample`.\n",
        "* **Uso:** representa el contexto de entrada para la predicción del siguiente token.\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "y = enc_sample[1:context_size + 1]\n",
        "```\n",
        "\n",
        "* **Qué hace:** toma una porción de `enc_sample` desplazada un elemento hacia adelante.\n",
        "* **Parámetros implícitos:**\n",
        "\n",
        "  * Inicio en el índice `1`.\n",
        "  * Fin en `context_size + 1` (es decir, `5`).\n",
        "* **Salida:** lista `y` que contiene los tokens que siguen inmediatamente a los de `x`.\n",
        "* **Uso:** representa los tokens objetivo (*targets*) que el modelo debe predecir dados los contextos `x`.\n",
        "* **Relación:** cada elemento `y[i]` es el “siguiente token” de `x[i]`.\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(f'x: {x}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la lista `x` con formato f-string.\n",
        "* **Propósito:** visualizar los tokens de entrada seleccionados.\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "print(f'y: {y}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la lista `y`, también con formato f-string.\n",
        "* **Propósito:** comprobar la correspondencia entre cada token de entrada en `x` y su token siguiente en `y`.\n",
        "* **Resultado típico:**\n",
        "\n",
        "  ```\n",
        "  x: [19204, 345, 82, 290]\n",
        "  y: [345, 82, 290, 50256]\n",
        "  ```\n",
        "\n",
        "  donde `y` está desplazada una posición con respecto a `x`.\n"
      ],
      "metadata": {
        "id": "o_IzjBBbK0YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size + 1):\n",
        "\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "\n",
        "  print(context, \"---->\", desired)"
      ],
      "metadata": {
        "id": "BsMQL6e1dtPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "for i in range(1, context_size + 1):\n",
        "```\n",
        "\n",
        "* **Qué hace:** inicia un bucle `for` que itera sobre los valores enteros desde `1` hasta `context_size` inclusive.\n",
        "* **Firma de la función:** `range(start, stop)` genera una secuencia de enteros comenzando en `start` y terminando en `stop - 1`.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `start = 1`: valor inicial de la secuencia.\n",
        "  * `stop = context_size + 1`: asegura que el último valor del bucle sea igual a `context_size`.\n",
        "* **Comportamiento:** el bucle recorre valores `1, 2, 3, 4` cuando `context_size = 4`.\n",
        "* **Uso:** permite generar dinámicamente contextos de longitud creciente.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "context = enc_sample[:i]\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea una sublista desde el inicio de `enc_sample` hasta el índice `i - 1`.\n",
        "* **Comportamiento:** en cada iteración, `context` contiene los primeros `i` tokens.\n",
        "\n",
        "  * En la primera iteración, 1 token.\n",
        "  * En la segunda, 2 tokens.\n",
        "  * Y así sucesivamente.\n",
        "* **Propósito:** simular cómo el modelo ve un contexto progresivamente mayor de tokens antes de predecir el siguiente.\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "desired = enc_sample[i]\n",
        "```\n",
        "\n",
        "* **Qué hace:** selecciona el token siguiente al último del contexto actual.\n",
        "* **Explicación:**\n",
        "\n",
        "  * Si `context = enc_sample[:i]`, el siguiente token está en la posición `i`.\n",
        "* **Resultado:** `desired` contiene el token que el modelo debería predecir dada la secuencia `context`.\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(context, \"---->\", desired)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el par formado por la lista `context` y su token objetivo `desired`.\n",
        "* **Salida típica:**\n",
        "\n",
        "  ```\n",
        "  [123] ----> 456\n",
        "  [123, 456] ----> 789\n",
        "  [123, 456, 789] ----> 321\n",
        "  [123, 456, 789, 321] ----> 654\n",
        "  ```\n",
        "* **Propósito:** visualizar de forma clara la relación entre cada contexto y el siguiente token que el modelo debe aprender a predecir.\n"
      ],
      "metadata": {
        "id": "0QtCaCJqM8y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "  context = enc_sample[:i]\n",
        "  desired = enc_sample[i]\n",
        "  print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "id": "QQII-WB4eCCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "for i in range(1, context_size+1):\n",
        "```\n",
        "\n",
        "* **Qué hace:** inicia un bucle que recorre los valores enteros desde `1` hasta `context_size` inclusive.\n",
        "* **Parámetros de `range()`:**\n",
        "\n",
        "  * `start = 1`: valor inicial.\n",
        "  * `stop = context_size + 1`: el límite superior no se incluye, por eso se suma 1 para abarcar el valor máximo.\n",
        "* **Comportamiento:** si `context_size = 4`, los valores de `i` serán `1, 2, 3, 4`.\n",
        "* **Propósito:** generar contextos de longitud progresiva para ilustrar cómo el modelo aprende dependencias token a token.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "context = enc_sample[:i]\n",
        "```\n",
        "\n",
        "* **Qué hace:** selecciona una porción de la lista `enc_sample` desde el índice `0` hasta `i - 1`.\n",
        "* **Resultado:** `context` contiene los primeros `i` tokens de la secuencia.\n",
        "* **Propósito:** representar el texto que el modelo “ha visto” hasta ese punto.\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "desired = enc_sample[i]\n",
        "```\n",
        "\n",
        "* **Qué hace:** toma el token que sigue inmediatamente después del contexto actual.\n",
        "* **Resultado:** `desired` es el token objetivo (el siguiente token a predecir).\n",
        "* **Importancia:** cada par `(context, desired)` corresponde a un ejemplo de entrenamiento del modelo de lenguaje.\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))\n",
        "```\n",
        "\n",
        "* **Qué hace:** decodifica ambos, el contexto y el token objetivo, para mostrarlos como texto legible.\n",
        "* **Parámetros del método `decode()`:**\n",
        "\n",
        "  * `tokens: list[int]`: lista de IDs a convertir a texto.\n",
        "  * `disallowed_special` (opcional, por defecto `set()`): define si se omiten *special tokens*.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * `tokenizer.decode(context)` reconstruye el texto correspondiente a los tokens del contexto.\n",
        "  * `tokenizer.decode([desired])` convierte el siguiente token en su representación textual.\n",
        "* **Salida típica:**\n",
        "\n",
        "  ```\n",
        "  A ----> k\n",
        "  Ak ----> w\n",
        "  Akw ----> i\n",
        "  Akwi ----> r\n",
        "  ```\n",
        "* **Propósito:** visualizar de forma comprensible cómo crece el contexto y cuál es el siguiente carácter o palabra que el modelo debe predecir en cada paso.\n"
      ],
      "metadata": {
        "id": "etvVdZQNOBBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Listing 2.5 A dataset for batched inputs and targets**\n",
        "# **Listado 2.5 Un conjunto de datos para entradas y objetivos agrupados**"
      ],
      "metadata": {
        "id": "OD4ZkDsW3Fqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Management\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)  #A Tokenizar todo el texto\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):  #B Usar una ventana deslizante para dividir el libro en secuencias superpuestas de longitud máxima\n",
        "      input_chunk = token_ids[i:i + max_length]\n",
        "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):  #C Devolver el número total de filas del conjunto de datos\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):  #D Devolver una sola fila del conjunto de datos\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "kospGa5i3LOk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "```\n",
        "\n",
        "* **Qué hace:** importa PyTorch y los componentes de manejo de datos.\n",
        "* `Dataset`: clase base para crear conjuntos de datos personalizados.\n",
        "* `DataLoader`: utilidad para cargar datos por lotes, barajarlos y alimentarlos al modelo durante el entrenamiento.\n",
        "* **Uso:** permite definir la estructura de los datos y cómo serán iterados en el entrenamiento de un LLM.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "class GPTDatasetV1(Dataset):\n",
        "```\n",
        "\n",
        "* **Qué hace:** define una clase llamada `GPTDatasetV1` que hereda de `torch.utils.data.Dataset`.\n",
        "* **Propósito:** crear un conjunto de datos específico para entrenamiento de un modelo tipo GPT.\n",
        "* **Requisito:** al heredar de `Dataset`, debe implementar obligatoriamente los métodos `__len__()` y `__getitem__()`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "def __init__(self, txt, tokenizer, max_length, stride):\n",
        "```\n",
        "\n",
        "* **Qué hace:** constructor de la clase. Inicializa los atributos del dataset.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `txt`: texto completo a tokenizar y segmentar.\n",
        "  * `tokenizer`: objeto `tiktoken.Encoding` o similar para convertir texto en IDs de tokens.\n",
        "  * `max_length`: tamaño máximo de cada secuencia de entrada (*context length*).\n",
        "  * `stride`: paso con el que se desliza la ventana de segmentación.\n",
        "* **Ejemplo:** si `max_length=8` y `stride=4`, las ventanas se superpondrán parcialmente.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "self.input_ids = []\n",
        "self.target_ids = []\n",
        "```\n",
        "\n",
        "* **Qué hace:** inicializa dos listas vacías para almacenar los tensores de entrada (`input_ids`) y los tensores objetivo (`target_ids`).\n",
        "* **Uso:** cada par (`input_ids[i]`, `target_ids[i]`) representará una muestra de entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "token_ids = tokenizer.encode(txt)\n",
        "```\n",
        "\n",
        "* **Qué hace:** convierte el texto completo en una lista de IDs de tokens.\n",
        "* **Salida:** `token_ids` es una lista de enteros que codifica el texto según el vocabulario del tokenizador.\n",
        "* **Uso:** base para generar fragmentos de longitud `max_length`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 6**\n",
        "\n",
        "```python\n",
        "for i in range(0, len(token_ids) - max_length, stride):\n",
        "```\n",
        "\n",
        "* **Qué hace:** recorre el texto tokenizado en pasos de tamaño `stride`.\n",
        "* **Parámetros del bucle:**\n",
        "\n",
        "  * `start = 0`: inicio del texto.\n",
        "  * `stop = len(token_ids) - max_length`: evita exceder el final de la lista.\n",
        "  * `step = stride`: controla la superposición entre fragmentos.\n",
        "* **Comportamiento:** genera ventanas de tokens deslizantes del tamaño `max_length`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 7**\n",
        "\n",
        "```python\n",
        "input_chunk = token_ids[i:i + max_length]\n",
        "target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea dos listas de tokens:\n",
        "\n",
        "  * `input_chunk`: secuencia de entrada.\n",
        "  * `target_chunk`: secuencia desplazada un token hacia adelante.\n",
        "* **Propósito:** entrenar el modelo para predecir el siguiente token (aprendizaje autoregresivo).\n",
        "* **Relación:** `target_chunk[t]` es el token que el modelo debe predecir tras ver `input_chunk[:t+1]`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 8**\n",
        "\n",
        "```python\n",
        "self.input_ids.append(torch.tensor(input_chunk))\n",
        "self.target_ids.append(torch.tensor(target_chunk))\n",
        "```\n",
        "\n",
        "* **Qué hace:** convierte cada fragmento de IDs en tensores de PyTorch y los agrega a las listas correspondientes.\n",
        "* **Tipo:** `torch.tensor(list[int]) → Tensor(dtype=torch.int64)`\n",
        "* **Uso:** facilita el entrenamiento posterior en GPU, ya que los tensores son el formato estándar en PyTorch.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 9**\n",
        "\n",
        "```python\n",
        "def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "```\n",
        "\n",
        "* **Qué hace:** devuelve el número total de ejemplos en el dataset.\n",
        "* **Requisito:** método obligatorio para que PyTorch pueda conocer el tamaño del conjunto de datos.\n",
        "* **Salida:** número de pares `(input, target)` almacenados.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 10**\n",
        "\n",
        "```python\n",
        "def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n",
        "```\n",
        "\n",
        "* **Qué hace:** permite acceder a un ejemplo específico del dataset por índice.\n",
        "* **Parámetro:**\n",
        "\n",
        "  * `idx`: posición entera del ejemplo solicitado.\n",
        "* **Salida:** tupla `(input_tensor, target_tensor)`.\n",
        "* **Uso:** es el método que PyTorch llama internamente cuando el `DataLoader` itera sobre el dataset.\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen técnico:**\n",
        "Esta clase transforma texto continuo en pares de tensores de entrenamiento listos para un modelo autoregresivo (GPT).\n",
        "Cada muestra contiene:\n",
        "\n",
        "* **Entrada (`input_ids`)**: una secuencia de `max_length` tokens.\n",
        "* **Objetivo (`target_ids`)**: la misma secuencia desplazada un token hacia adelante.\n",
        "\n",
        "El parámetro `stride` controla el solapamiento entre fragmentos y, por tanto, la cantidad de ejemplos generados.\n"
      ],
      "metadata": {
        "id": "_0YV6ZR6TQ1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Listing 2.6 A data loader to generate batches with input-with pairs**"
      ],
      "metadata": {
        "id": "yDKqBrBi8sCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader management\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding('gpt2')  #A inicializamos el tokenizador\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  #B creamos el dataset\n",
        "\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      drop_last = True,  #C drp_last = True Se descarta el último lote si es más corto que el tamaño de lote especificado para evitar picos de pérdida durante el entrenamiento.\n",
        "      num_workers = 0  #D la cantidad de procesos de CPU que se utilizarán para el prprocesamiento\n",
        "    )\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "Eoxltzf13c4N"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, num_workers=0):\n",
        "```\n",
        "\n",
        "* **Qué hace:** define una función llamada `create_dataloader_v1` que construye y devuelve un objeto `DataLoader` de PyTorch configurado para procesar texto.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `txt`: texto fuente que se tokenizará y segmentará.\n",
        "  * `batch_size` (opcional, por defecto `4`): número de ejemplos por lote durante el entrenamiento.\n",
        "  * `max_length` (opcional, por defecto `256`): número máximo de tokens por secuencia.\n",
        "  * `stride` (opcional, por defecto `128`): desplazamiento entre ventanas sucesivas de texto tokenizado.\n",
        "  * `shuffle` (opcional, por defecto `True`): si `True`, mezcla las muestras en cada época.\n",
        "  * `num_workers` (opcional, por defecto `0`): número de subprocesos para cargar datos en paralelo (en Google Colab o CPU limitada suele mantenerse en `0`).\n",
        "* **Propósito:** encapsular en una sola función la creación del dataset y del `DataLoader` configurado.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "```\n",
        "\n",
        "* **Qué hace:** obtiene el objeto `Encoding` correspondiente al tokenizador GPT-2 de la librería `tiktoken`.\n",
        "* **Salida:** instancia lista para convertir texto a IDs y viceversa.\n",
        "* **Motivo:** garantizar que el dataset use el mismo esquema de codificación que el modelo a entrenar.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea una instancia de la clase `GPTDatasetV1` definida anteriormente.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `txt`: texto completo.\n",
        "  * `tokenizer`: tokenizador cargado.\n",
        "  * `max_length`: tamaño del contexto por muestra.\n",
        "  * `stride`: desplazamiento entre secuencias.\n",
        "* **Salida:** objeto `dataset` que implementa `__len__` y `__getitem__`.\n",
        "* **Uso:** contiene todas las muestras (pares de entrada y objetivo) derivadas del texto.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = shuffle,\n",
        "    drop_last = True,\n",
        "    num_workers = 0\n",
        ")\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un `DataLoader`, componente de PyTorch que organiza la entrega de datos al modelo en mini-lotes.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `dataset`: el objeto `GPTDatasetV1` con los ejemplos preparados.\n",
        "  * `batch_size`: número de ejemplos por lote.\n",
        "  * `shuffle`: mezcla las muestras al comienzo de cada época si está en `True`.\n",
        "  * `drop_last=True`: descarta el último lote si tiene menos elementos que `batch_size`; útil para mantener dimensiones constantes.\n",
        "  * `num_workers`: número de procesos de carga paralela de datos (por simplicidad, aquí se usa `0`).\n",
        "* **Comportamiento:** el `DataLoader` usa internamente el método `__getitem__` del dataset para devolver pares `(input_ids, target_ids)` empaquetados por lotes.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "return dataloader\n",
        "```\n",
        "\n",
        "* **Qué hace:** devuelve el objeto `DataLoader` completamente configurado.\n",
        "* **Uso:** este `dataloader` se puede iterar directamente en un bucle de entrenamiento con:\n",
        "\n",
        "  ```python\n",
        "  for x, y in dataloader:\n",
        "      ...\n",
        "  ```\n",
        "* **Propósito:** simplificar la creación del flujo de datos de entrenamiento a partir de texto crudo.\n"
      ],
      "metadata": {
        "id": "Gdec8YXNU1Xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=1,\n",
        "    max_length=4,\n",
        "    stride=1,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)  #A convierte un cargador de datos en un iterador de Python para obtener la siguiente entrada a través de la función next() incorporada de Python\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "u07nXy_JMTBt",
        "outputId": "acf76c5d-8fe0-45fb-b310-9adf580cdf82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tiktoken' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1969218819.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataloader = create_dataloader_v1(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mraw_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-216570067.py\u001b[0m in \u001b[0;36mcreate_dataloader_v1\u001b[0;34m(txt, batch_size, max_length, stride, shuffle, num_workers)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dataloader_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gpt2'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#A inicializamos el tokenizador\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPTDatasetV1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#B creamos el dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tiktoken' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=1,\n",
        "    max_length=4,\n",
        "    stride=1,\n",
        "    shuffle=False\n",
        ")\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un `DataLoader` usando la función `create_dataloader_v1`.\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `raw_text`: texto original cargado del archivo.\n",
        "  * `batch_size=1`: cada lote contendrá solo una muestra `(input_ids, target_ids)`.\n",
        "  * `max_length=4`: cada secuencia tendrá 4 tokens de longitud.\n",
        "  * `stride=1`: la ventana deslizante avanza de un token a la vez, generando ejemplos solapados.\n",
        "  * `shuffle=False`: mantiene el orden secuencial de las muestras (sin barajar).\n",
        "* **Resultado:** se crea un `DataLoader` que contiene todas las secuencias generadas a partir del texto tokenizado, preparadas para ser recorridas en entrenamiento o inspección.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "data_iter = iter(dataloader)\n",
        "```\n",
        "\n",
        "* **Qué hace:** convierte el `DataLoader` en un iterador de Python.\n",
        "* **Firma interna:** `iter(obj)` llama al método `__iter__()` del objeto si está definido.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * Permite recorrer el `DataLoader` manualmente con la función `next()`.\n",
        "  * Cada llamada a `next(data_iter)` devolverá el siguiente lote de datos (`x, y`).\n",
        "* **Uso:** útil para inspeccionar manualmente los primeros lotes o depurar el flujo de datos.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "first_batch = next(data_iter)\n",
        "```\n",
        "\n",
        "* **Qué hace:** obtiene el primer lote del iterador `data_iter`.\n",
        "* **Mecanismo:** la función incorporada `next()` ejecuta la primera iteración del `DataLoader`.\n",
        "* **Salida:** una tupla `(input_ids, target_ids)` donde:\n",
        "\n",
        "  * `input_ids`: tensor de forma `[batch_size, max_length]` → aquí `[1, 4]`.\n",
        "  * `target_ids`: tensor de forma idéntica, desplazado un token.\n",
        "* **Uso:** acceder explícitamente al primer lote sin recorrer todo el dataloader.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(first_batch)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el contenido del primer lote obtenido.\n",
        "* **Salida típica:**\n",
        "\n",
        "  ```\n",
        "  (tensor([[  40,  345,  290,  123]]), tensor([[ 345,  290,  123,  456]]))\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * El primer tensor representa la secuencia de entrada.\n",
        "  * El segundo tensor es la misma secuencia desplazada un token a la derecha, objetivo del modelo.\n",
        "* **Propósito:** verificar que el `DataLoader` está generando correctamente los pares de entrenamiento `(x, y)` según los parámetros especificados.\n"
      ],
      "metadata": {
        "id": "hRXYMn6tYpJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "id": "CHErucmQOLFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "second_batch = next(data_iter)\n",
        "```\n",
        "\n",
        "* **Qué hace:** obtiene el siguiente lote del iterador `data_iter` creado a partir del `DataLoader`.\n",
        "* **Mecanismo:** la función `next()` llama internamente al método `__next__()` del iterador, devolviendo el siguiente elemento de la secuencia.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * En la primera llamada (`first_batch = next(data_iter)`), se extrajo el primer lote.\n",
        "  * En esta segunda llamada, se extrae el siguiente lote disponible.\n",
        "* **Salida:** una tupla `(input_ids, target_ids)` donde cada tensor tiene tamaño `[batch_size, max_length]`.\n",
        "* **Ejemplo de salida:**\n",
        "\n",
        "  ```\n",
        "  (tensor([[ 345,  290,  123,  456]]), tensor([[ 290,  123,  456,  789]]))\n",
        "  ```\n",
        "* **Interpretación:** el nuevo lote representa la siguiente ventana deslizante de tokens según el `stride=1` configurado, es decir, el contexto se desplazó un token respecto al lote anterior.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(second_batch)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el contenido del segundo lote.\n",
        "* **Propósito:** inspeccionar el comportamiento del `DataLoader` y confirmar que genera las secuencias de entrenamiento consecutivas correctamente.\n",
        "* **Resultado:** muestra los tensores de entrada y sus correspondientes objetivos, lo que permite verificar que el dataset mantiene coherencia y secuencia en los datos.\n"
      ],
      "metadata": {
        "id": "ghloqLM0ZIkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "third_batch = next(data_iter)\n",
        "print(third_batch)"
      ],
      "metadata": {
        "id": "zIwmOxhcOkCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "third_batch = next(data_iter)\n",
        "```\n",
        "\n",
        "* **Qué hace:** solicita el tercer lote del iterador `data_iter` asociado al `DataLoader`.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * Cada llamada a `next(data_iter)` avanza una posición en el flujo de lotes.\n",
        "  * Dado que el `stride` se configuró en `1`, este lote estará desplazado un token más respecto al segundo lote.\n",
        "* **Salida:** tupla `(input_ids, target_ids)` donde ambos son tensores de forma `[batch_size, max_length]`.\n",
        "* **Ejemplo de salida:**\n",
        "\n",
        "  ```\n",
        "  (tensor([[290, 123, 456, 789]]), tensor([[123, 456, 789, 321]]))\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * El primer tensor contiene los tokens de entrada del tercer fragmento del texto.\n",
        "  * El segundo tensor contiene los tokens objetivo correspondientes, desplazados un paso hacia adelante.\n",
        "* **Propósito:** comprobar que el `DataLoader` genera correctamente la secuencia continua de ejemplos autoregresivos, avanzando un token en cada lote cuando `stride=1`.\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(third_batch)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el contenido del tercer lote.\n",
        "* **Uso:** inspección manual de los datos generados, confirmando que los pares `(x, y)` siguen la lógica de “contexto → siguiente token” establecida en el diseño del dataset.\n"
      ],
      "metadata": {
        "id": "vp-TkbHbaTy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE 2.2 DATA LOADERS WITH DIFFERENT STRIDES AND CONTEXT SIZES:\n",
        "To develop more intuition for how the data loader works, try to run it with different\n",
        "settings such as:\n",
        "*   max_length=2 and stride=2\n",
        "*   max_length=8 and stride=2.\n",
        "\n",
        "---\n",
        "\n",
        "# EJERCICIO 2.2 CARGADORES DE DATOS CON DIFERENTES PASOS Y TAMAÑOS DE CONTEXTO:\n",
        "Para comprender mejor el funcionamiento del cargador de datos, intente ejecutarlo con diferentes configuraciones, como:\n",
        "*   max_length=2 y stride=2\n",
        "*   max_length=8 y stride=2"
      ],
      "metadata": {
        "id": "vdPANXgnWfTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_bis = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=1,\n",
        "    max_length=2,\n",
        "    stride=2,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "Vf12BaQbOSwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "dataloader_bis = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=1,\n",
        "    max_length=2,\n",
        "    stride=2,\n",
        "    shuffle=False\n",
        ")\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un nuevo `DataLoader` con parámetros distintos al anterior.\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `raw_text`: texto original del archivo.\n",
        "  * `batch_size=1`: genera un lote con una sola muestra `(input_ids, target_ids)`.\n",
        "  * `max_length=2`: cada secuencia de entrada contendrá 2 tokens.\n",
        "  * `stride=2`: la ventana deslizante avanza de dos en dos, sin superposición entre fragmentos.\n",
        "  * `shuffle=False`: mantiene el orden secuencial de las muestras.\n",
        "* **Resultado:** `dataloader_bis` es un cargador de datos que recorre el texto en segmentos consecutivos no solapados de longitud 2.\n",
        "  Ejemplo conceptual:\n",
        "\n",
        "  ```\n",
        "  tokens = [1, 2, 3, 4, 5, 6]\n",
        "  con stride=2 → [(1,2)->(2,3)], [(3,4)->(4,5)], [(5,6)->(6,7)]\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "data_iter = iter(dataloader)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un iterador a partir del `DataLoader` llamado `dataloader`.\n",
        "* **Observación importante:** aquí se usa **`dataloader`**, no `dataloader_bis`.\n",
        "  Esto significa que el iterador sigue apuntando al cargador anterior (el que tenía `max_length=4` y `stride=1`), no al nuevo.\n",
        "* **Comportamiento:** se reutiliza el cargador previo, por lo tanto los datos obtenidos no corresponden a la nueva configuración.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "first_batch = next(data_iter)\n",
        "```\n",
        "\n",
        "* **Qué hace:** obtiene el siguiente lote disponible del iterador `data_iter`.\n",
        "* **Resultado:** devuelve una tupla `(input_ids, target_ids)` del `dataloader` anterior, no de `dataloader_bis`, debido a la referencia usada en la línea anterior.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(first_batch)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el lote obtenido.\n",
        "* **Contenido:** dos tensores (`input_ids`, `target_ids`) de tamaño `[1, 4]`, porque provienen del `DataLoader` inicial con `max_length=4`.\n",
        "* **Corrección recomendada:** si se pretende visualizar el primer lote del nuevo `dataloader_bis`, debe escribirse:\n",
        "\n",
        "  ```python\n",
        "  data_iter = iter(dataloader_bis)\n",
        "  first_batch = next(data_iter)\n",
        "  print(first_batch)\n",
        "  ```\n",
        "* **Propósito:** inspeccionar el comportamiento del nuevo cargador de datos, que debería generar secuencias sin solapamiento (por el `stride=2`).\n"
      ],
      "metadata": {
        "id": "3wLesPkKgWXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "id": "aMPlm9qwXe3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader_bis = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=1,\n",
        "    max_length=8,\n",
        "    stride=2,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "data_iter = iter(dataloader_bis)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "id": "pPk3n8XpXrsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "dataloader_bis = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=1,\n",
        "    max_length=8,\n",
        "    stride=2,\n",
        "    shuffle=False\n",
        ")\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un nuevo `DataLoader` con configuración diferente para generar fragmentos más largos de texto tokenizado.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `raw_text`: texto completo que será tokenizado.\n",
        "  * `batch_size=1`: cada lote contiene una sola muestra `(input_ids, target_ids)`.\n",
        "  * `max_length=8`: cada secuencia de entrada contiene 8 tokens consecutivos.\n",
        "  * `stride=2`: la ventana deslizante avanza 2 tokens por iteración, por lo que hay superposición de 6 tokens entre fragmentos consecutivos.\n",
        "  * `shuffle=False`: mantiene el orden original del texto, sin mezclar las muestras.\n",
        "* **Resultado:** `dataloader_bis` recorre el texto en fragmentos parcialmente solapados de longitud 8, adecuados para entrenar modelos autoregresivos.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "data_iter = iter(dataloader_bis)\n",
        "```\n",
        "\n",
        "* **Qué hace:** convierte el `DataLoader` en un iterador de Python.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * Permite usar `next(data_iter)` para extraer manualmente el siguiente lote.\n",
        "  * Cada lote contiene un par `(input_ids, target_ids)` de tensores con dimensiones `[batch_size, max_length]`.\n",
        "* **Uso:** facilita inspeccionar de forma controlada las primeras muestras generadas por el cargador.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "first_batch = next(data_iter)\n",
        "```\n",
        "\n",
        "* **Qué hace:** obtiene el primer lote del iterador `data_iter`.\n",
        "* **Salida:** tupla `(input_ids, target_ids)` donde:\n",
        "\n",
        "  * `input_ids`: tensor con los primeros 8 tokens de la secuencia de entrada.\n",
        "  * `target_ids`: tensor con los 8 tokens siguientes, desplazados una posición a la derecha.\n",
        "* **Forma de salida:** `[1, 8]` en ambos tensores, ya que `batch_size=1` y `max_length=8`.\n",
        "* **Propósito:** verificar la correcta generación del primer par de datos para entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(first_batch)\n",
        "```\n",
        "\n",
        "* **Qué hace:** muestra en pantalla el contenido del lote obtenido.\n",
        "* **Salida típica:**\n",
        "\n",
        "  ```\n",
        "  (tensor([[  40,  345,  290,  123,  567,  901,  456,  789]]),\n",
        "   tensor([[ 345,  290,  123,  567,  901,  456,  789,  654]]))\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * El primer tensor (`input_ids`) es el contexto que el modelo ve.\n",
        "  * El segundo tensor (`target_ids`) son los tokens que el modelo debe predecir.\n",
        "* **Conclusión:** confirma que el `DataLoader` genera correctamente secuencias de longitud 8 con desplazamiento de 1 token y superposición controlada por `stride=2`.\n"
      ],
      "metadata": {
        "id": "uto5caVxhe5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "id": "X_1HzpTXXwLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **...how we can use the data loader to sample with a batch size greater than 1:**\n",
        "\n",
        "# **...cómo podemos utilizar el cargador de datos para muestrear con un tamaño de lote mayor a 1:**"
      ],
      "metadata": {
        "id": "AfSCvPNbcM5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(f'Inputs: {inputs}')\n",
        "print(f'Target: {targets}')"
      ],
      "metadata": {
        "id": "y9hXufjxkt5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un `DataLoader` configurado para procesar el texto `raw_text` en lotes de tamaño 8.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `raw_text`: texto de entrada completo.\n",
        "  * `batch_size=8`: cada lote contendrá 8 pares `(input_ids, target_ids)`.\n",
        "  * `max_length=4`: cada secuencia de entrada tendrá 4 tokens.\n",
        "  * `stride=4`: el desplazamiento entre ventanas es igual al tamaño del contexto, por lo que **no hay superposición** entre fragmentos consecutivos.\n",
        "* **Resultado:** el `DataLoader` recorre el texto dividiéndolo en fragmentos contiguos de 4 tokens, ideales para entrenamientos o pruebas rápidas.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "data_iter = iter(dataloader)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un iterador Python a partir del `DataLoader`.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * Permite recorrer el `DataLoader` con `next()` o en un bucle `for`.\n",
        "  * Cada iteración devuelve un lote de datos con la forma `(inputs, targets)`.\n",
        "* **Uso:** acceder manualmente a los primeros lotes para inspeccionar su contenido.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "inputs, targets = next(data_iter)\n",
        "```\n",
        "\n",
        "* **Qué hace:** obtiene el primer lote de datos del iterador.\n",
        "* **Salida:** dos tensores (`inputs`, `targets`) con dimensiones `[batch_size, max_length]`, es decir, `[8, 4]`.\n",
        "* **Contenido:**\n",
        "\n",
        "  * `inputs`: secuencias de 4 tokens consecutivos.\n",
        "  * `targets`: las mismas secuencias desplazadas un token hacia adelante (tokens objetivo).\n",
        "* **Propósito:** verificar cómo el `DataLoader` genera y agrupa las secuencias de texto tokenizadas.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(f'Inputs: {inputs}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el tensor que contiene las secuencias de entrada del primer lote.\n",
        "* **Ejemplo de salida:**\n",
        "\n",
        "  ```\n",
        "  Inputs: tensor([[  40,  345,  290,  123],\n",
        "                  [ 567,  901,  456,  789],\n",
        "                  [ ... ],\n",
        "                  ...])\n",
        "  ```\n",
        "* **Interpretación:** cada fila del tensor representa una secuencia independiente de 4 tokens.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "print(f'Target: {targets}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el tensor que contiene los tokens objetivo correspondientes a cada secuencia de entrada.\n",
        "* **Ejemplo de salida:**\n",
        "\n",
        "  ```\n",
        "  Target: tensor([[ 345,  290,  123,  567],\n",
        "                  [ 901,  456,  789,  654],\n",
        "                  [ ... ],\n",
        "                  ...])\n",
        "  ```\n",
        "* **Relación:** cada fila de `targets` está alineada con la de `inputs`, desplazada un token hacia la derecha.\n",
        "* **Propósito:** comprobar visualmente que el par `(input, target)` cumple la estructura autoregresiva usada en el entrenamiento de modelos tipo GPT.\n"
      ],
      "metadata": {
        "id": "eu6GMDLakGI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2.7 Creating token embeddings**\n",
        "# **2.7 Creación de incrustaciones de tokens**"
      ],
      "metadata": {
        "id": "3SJ-Uw3Ml2jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "w = torch.tensor(3.0, requires_grad=True)\n",
        "x = torch.tensor(2.0)\n",
        "y = w * x        # forward\n",
        "loss = (y - 10)**2\n",
        "loss.backward()  # backward\n",
        "\n",
        "print(w.grad)    # d(loss)/d(w)"
      ],
      "metadata": {
        "id": "VNZZHxMxcQjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "```\n",
        "\n",
        "* **Qué hace:** importa el módulo principal de PyTorch.\n",
        "* **Propósito:** habilitar el uso de tensores y operaciones con seguimiento automático de gradientes mediante *autograd*.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "w = torch.tensor(3.0, requires_grad=True)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un tensor escalar con valor `3.0`.\n",
        "* **Parámetro clave:**\n",
        "\n",
        "  * `requires_grad=True`: indica que PyTorch debe rastrear todas las operaciones realizadas sobre `w` para calcular derivadas automáticas.\n",
        "* **Resultado:** `w` es una variable diferenciable, por lo que su gradiente (`w.grad`) se actualizará cuando se llame a `backward()`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "x = torch.tensor(2.0)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un tensor escalar con valor `2.0`.\n",
        "* **Parámetro `requires_grad`:** no se especifica, por lo tanto su valor por defecto es `False`.\n",
        "* **Resultado:** `x` actúa como una constante en las operaciones posteriores.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "y = w * x\n",
        "```\n",
        "\n",
        "* **Qué hace:** multiplica los tensores `w` y `x` (operación *forward*).\n",
        "* **Resultado:** `y = 3.0 × 2.0 = 6.0`.\n",
        "* **Importante:** la operación se registra en el grafo computacional de PyTorch porque `w` tiene `requires_grad=True`.\n",
        "* **Forma simbólica:** `y = f(w) = w * 2`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "loss = (y - 10)**2\n",
        "```\n",
        "\n",
        "* **Qué hace:** calcula una función de pérdida cuadrática.\n",
        "* **Expresión matemática:**\n",
        "  [\n",
        "  \\text{loss} = (w \\times x - 10)^2\n",
        "  ]\n",
        "* **Resultado numérico:**\n",
        "  [\n",
        "  (6 - 10)^2 = 16\n",
        "  ]\n",
        "* **El grafo de cómputo** ahora incluye todas las operaciones necesarias para obtener el gradiente de `loss` respecto a `w`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 6**\n",
        "\n",
        "```python\n",
        "loss.backward()\n",
        "```\n",
        "\n",
        "* **Qué hace:** realiza la propagación hacia atrás (*backpropagation*).\n",
        "* **Efecto:** PyTorch calcula\n",
        "  [\n",
        "  \\frac{d(\\text{loss})}{dw}\n",
        "  ]\n",
        "  y lo almacena en `w.grad`.\n",
        "* **Cálculo manual:**\n",
        "  [\n",
        "  \\text{loss} = (w \\times 2 - 10)^2\n",
        "  ]\n",
        "  Derivando respecto a `w`:\n",
        "  [\n",
        "  \\frac{d(\\text{loss})}{dw} = 2(w \\times 2 - 10) \\times 2 = 4(2w - 10)\n",
        "  ]\n",
        "  Sustituyendo `w = 3`:\n",
        "  [\n",
        "  4(6 - 10) = 4(-4) = -16\n",
        "  ]\n",
        "* **Resultado esperado:** `w.grad = tensor(-16.)`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 7**\n",
        "\n",
        "```python\n",
        "print(w.grad)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el gradiente almacenado en `w.grad`.\n",
        "* **Salida:**\n",
        "\n",
        "  ```\n",
        "  tensor(-16.)\n",
        "  ```\n",
        "* **Interpretación:** el gradiente negativo indica que, para minimizar la pérdida, el valor de `w` debería **aumentar**, lo cual concuerda con la dirección del descenso del gradiente.\n"
      ],
      "metadata": {
        "id": "alPNYE9VoaWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "id": "y0SDxwCYsHv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un tensor con los índices `[2, 3, 5, 1]`.\n",
        "* **Interpretación:** cada número representa el **ID de un token** dentro del vocabulario.\n",
        "* **Uso:** estos IDs se usarán para consultar las representaciones vectoriales (embeddings) correspondientes en la capa de incrustación.\n",
        "* **Tipo de dato:** por defecto, `torch.int64`, que es el formato requerido por `nn.Embedding`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "vocab_size = 6\n",
        "```\n",
        "\n",
        "* **Qué hace:** define el tamaño del vocabulario, es decir, cuántos tokens distintos hay.\n",
        "* **Valor:** `6` implica que los tokens válidos van de `0` a `5`.\n",
        "* **Importancia:** la capa `Embedding` solo puede indexar IDs dentro de ese rango.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "output_dim = 3\n",
        "```\n",
        "\n",
        "* **Qué hace:** define la dimensión de los vectores de embedding.\n",
        "* **Significado:** cada token se representará mediante un vector de 3 valores (dimensión de espacio latente).\n",
        "* **Ejemplo:** si `output_dim=3`, el token con ID `2` se mapeará a algo como `[0.4, -0.1, 0.9]`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "torch.manual_seed(123)\n",
        "```\n",
        "\n",
        "* **Qué hace:** fija la **semilla aleatoria** de PyTorch.\n",
        "* **Propósito:** asegurar reproducibilidad; cada inicialización aleatoria será igual en futuras ejecuciones.\n",
        "* **Uso:** importante para depuración o comparación de resultados.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea una **capa de embedding**.\n",
        "* **Firma:**\n",
        "  `torch.nn.Embedding(num_embeddings, embedding_dim)`\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `num_embeddings = vocab_size`: cantidad de tokens únicos (aquí 6).\n",
        "  * `embedding_dim = output_dim`: tamaño de cada vector de embedding (aquí 3).\n",
        "* **Funcionamiento:**\n",
        "\n",
        "  * Internamente contiene una matriz de pesos de forma `[vocab_size, output_dim]`.\n",
        "  * Cada fila corresponde al vector de embedding de un token.\n",
        "* **Inicialización:** los valores se generan aleatoriamente según una distribución uniforme en el rango `(-1/sqrt(embedding_dim), 1/sqrt(embedding_dim))`.\n",
        "* **Uso posterior:** al pasar `input_ids` a esta capa, se seleccionarán las filas correspondientes.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 6**\n",
        "\n",
        "```python\n",
        "print(embedding_layer.weight)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la matriz de pesos de la capa de embedding.\n",
        "* **Tamaño:** `[6, 3]` (una fila por token, tres valores por embedding).\n",
        "* **Ejemplo de salida reproducible:**\n",
        "\n",
        "  ```\n",
        "  Parameter containing:\n",
        "  tensor([[ 0.3374,  0.2079,  0.1385],\n",
        "          [ 0.2542, -0.3456, -0.1756],\n",
        "          [-0.2624, -0.2777, -0.2861],\n",
        "          [ 0.2369, -0.2038,  0.3054],\n",
        "          [ 0.1227,  0.3397,  0.0446],\n",
        "          [ 0.1266, -0.2451, -0.2848]], requires_grad=True)\n",
        "  ```\n",
        "* **Interpretación:** cada fila representa la **representación vectorial aprendible** de un token.\n",
        "* **Importancia:** estos vectores son parámetros entrenables (`requires_grad=True`), y se actualizarán durante el entrenamiento para capturar relaciones semánticas entre tokens.\n"
      ],
      "metadata": {
        "id": "aVrgyXPKsxQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "id": "glwv8owOsJga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "print(embedding_layer(torch.tensor([3])))\n",
        "```\n",
        "\n",
        "* **Qué hace:** pasa el tensor `[3]` como entrada a la capa de embedding `embedding_layer`.\n",
        "\n",
        "* **Entrada:**\n",
        "\n",
        "  * `torch.tensor([3])` es un tensor con un único índice de token (`ID = 3`).\n",
        "  * El valor `3` corresponde a la **cuarta fila** de la matriz de pesos del embedding (los índices comienzan en 0).\n",
        "  * Tipo de dato: debe ser entero (`torch.long` o `torch.int64`), ya que la capa realiza una búsqueda por índice.\n",
        "\n",
        "* **Comportamiento interno:**\n",
        "\n",
        "  * La capa `nn.Embedding` actúa como una **tabla de búsqueda** (*lookup table*).\n",
        "  * Dado el índice `3`, retorna la fila número 3 de su matriz interna de pesos `embedding_layer.weight`.\n",
        "  * No realiza operaciones matemáticas complejas; solo accede a la fila correspondiente.\n",
        "\n",
        "* **Salida:**\n",
        "\n",
        "  * Tensor de forma `[1, output_dim]` → `[1, 3]` en este caso.\n",
        "  * Contiene el vector de embedding asociado al token con ID 3.\n",
        "  * Ejemplo (con la semilla fijada en 123):\n",
        "\n",
        "    ```\n",
        "    tensor([[ 0.2369, -0.2038,  0.3054]], grad_fn=<EmbeddingBackward0>)\n",
        "    ```\n",
        "\n",
        "* **Propósito:**\n",
        "\n",
        "  * Recuperar la representación vectorial del token ID 3.\n",
        "  * Este vector se usará posteriormente como entrada para las capas posteriores del modelo (por ejemplo, capas de atención o MLP).\n"
      ],
      "metadata": {
        "id": "SPSSkfdsvXch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's now apply that to all four input IDs we defined earlier\n",
        "## (torch.tensor([2, 3, 5, 1])):\n",
        "\n",
        "# Apliquemos ahora esto a los cuatro ID de entrada que definimos anteriormente:\n",
        "## (torch.tensor([2, 3, 5, 1])):"
      ],
      "metadata": {
        "id": "On_vw3U2u6F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "id": "mM74MsAcu7y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "print(embedding_layer(input_ids))\n",
        "```\n",
        "\n",
        "* **Qué hace:** pasa todo el tensor `input_ids` (`[2, 3, 5, 1]`) a la capa de embedding `embedding_layer`.\n",
        "\n",
        "* **Entrada:**\n",
        "\n",
        "  * `input_ids` contiene los IDs de los tokens a representar.\n",
        "  * Cada valor se usa como índice para seleccionar una fila en la matriz `embedding_layer.weight`, de tamaño `[vocab_size, output_dim] = [6, 3]`.\n",
        "  * Los índices válidos van de `0` a `5`; todos los valores en `input_ids` están dentro de ese rango.\n",
        "\n",
        "* **Comportamiento interno:**\n",
        "\n",
        "  * La capa **realiza una búsqueda por índice** (lookup) para cada ID en `input_ids`.\n",
        "  * Construye un tensor 2D donde cada fila es el vector de embedding correspondiente a un token.\n",
        "  * Matemáticamente, no hay multiplicaciones ni sumas, solo acceso directo a las filas de la matriz de pesos.\n",
        "\n",
        "* **Salida:**\n",
        "\n",
        "  * Tensor de forma `[len(input_ids), output_dim]` → `[4, 3]`.\n",
        "  * Cada fila es el vector de embedding asociado a los IDs `[2]`, `[3]`, `[5]`, `[1]` respectivamente.\n",
        "  * Ejemplo con la semilla `123`:\n",
        "\n",
        "    ```\n",
        "    tensor([[-0.2624, -0.2777, -0.2861],\n",
        "            [ 0.2369, -0.2038,  0.3054],\n",
        "            [ 0.1266, -0.2451, -0.2848],\n",
        "            [ 0.2542, -0.3456, -0.1756]], grad_fn=<EmbeddingBackward0>)\n",
        "    ```\n",
        "\n",
        "* **Interpretación:**\n",
        "\n",
        "  * Cada fila es una **representación vectorial densa** (embedding) del token correspondiente.\n",
        "  * Estas representaciones son **parámetros entrenables**, y durante el entrenamiento se ajustan para capturar relaciones semánticas entre tokens.\n",
        "  * Este paso es fundamental en modelos de lenguaje, ya que convierte IDs discretos en vectores continuos que el modelo puede procesar.\n"
      ],
      "metadata": {
        "id": "s0bCnU1kwh_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "R06oLIU15-0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "vocab_size = 50257\n",
        "```\n",
        "\n",
        "* **Qué hace:** define el tamaño del vocabulario, es decir, cuántos tokens únicos existen en el modelo.\n",
        "* **Contexto:** `50257` es el tamaño del vocabulario del modelo **GPT-2** (50 000 tokens base + algunos tokens especiales).\n",
        "* **Uso:** este valor determina cuántas filas tendrá la matriz de embeddings, una por cada token posible.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "output_dim = 256\n",
        "```\n",
        "\n",
        "* **Qué hace:** establece la dimensión del espacio de embeddings.\n",
        "* **Significado:** cada token se representará mediante un vector de **256 valores** en el espacio latente.\n",
        "* **Contexto:** en modelos GPT reales, este número puede ser 768, 1024 o más, dependiendo del tamaño del modelo; aquí se usa 256 por simplicidad o para entrenamiento reducido.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea una **capa de embedding** en PyTorch para representar tokens.\n",
        "\n",
        "* **Firma:**\n",
        "\n",
        "  ```python\n",
        "  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "  ```\n",
        "\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `num_embeddings = vocab_size` → número total de tokens posibles (50257).\n",
        "  * `embedding_dim = output_dim` → tamaño de cada vector (256).\n",
        "\n",
        "* **Inicialización:**\n",
        "\n",
        "  * Crea una matriz de pesos de forma `[50257, 256]`.\n",
        "  * Cada fila contiene el vector de embedding inicial de un token (valores iniciales aleatorios).\n",
        "  * Los pesos son parámetros entrenables (`requires_grad=True`).\n",
        "\n",
        "* **Propósito:**\n",
        "\n",
        "  * Traducir **IDs discretos de tokens** en **vectores continuos** que el modelo pueda procesar.\n",
        "  * Este embedding será la **primera capa** de un modelo tipo GPT, donde cada token del texto de entrada se convierte en un vector de 256 dimensiones antes de pasar a capas posteriores (atención, MLP, etc.).\n"
      ],
      "metadata": {
        "id": "OD_nMQPpy7yT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=8,\n",
        "    max_length=max_length,\n",
        "    stride=max_length,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "print(f'Token IDs: {inputs}')\n",
        "print(f'Inputs shape: {inputs.shape}')\n"
      ],
      "metadata": {
        "id": "LCc9xSLE6OM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "max_length = 4\n",
        "```\n",
        "\n",
        "* **Qué hace:** define el tamaño del contexto o número máximo de tokens que tendrá cada secuencia de entrada.\n",
        "* **Uso:** cada muestra del dataset contendrá exactamente 4 tokens consecutivos.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text,\n",
        "    batch_size=8,\n",
        "    max_length=max_length,\n",
        "    stride=max_length,\n",
        "    shuffle=False\n",
        ")\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un `DataLoader` usando la función definida previamente.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `raw_text`: texto fuente completo que será tokenizado.\n",
        "  * `batch_size=8`: número de muestras por lote.\n",
        "  * `max_length=4`: longitud de cada secuencia de entrada.\n",
        "  * `stride=max_length`: la ventana deslizante avanza 4 tokens en cada paso → **sin superposición** entre fragmentos.\n",
        "  * `shuffle=False`: mantiene el orden original de las muestras.\n",
        "* **Resultado:** el `DataLoader` recorre el texto dividiéndolo en secuencias contiguas de 4 tokens, agrupadas en lotes de 8 ejemplos.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "data_iter = iter(dataloader)\n",
        "```\n",
        "\n",
        "* **Qué hace:** convierte el `DataLoader` en un iterador Python.\n",
        "* **Uso:** permite acceder manualmente al siguiente lote con `next(data_iter)`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "inputs, targets = next(data_iter)\n",
        "```\n",
        "\n",
        "* **Qué hace:** obtiene el primer lote del iterador.\n",
        "* **Salida:**\n",
        "\n",
        "  * `inputs`: tensor con los IDs de tokens de las secuencias de entrada.\n",
        "  * `targets`: tensor con los mismos IDs, desplazados un token hacia adelante (objetivos de predicción).\n",
        "* **Dimensiones:** ambos tensores tienen forma `[batch_size, max_length]` → `[8, 4]`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "print(f'Token IDs: {inputs}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime las secuencias de tokens que constituyen las entradas del primer lote.\n",
        "* **Contenido:** cada fila contiene 4 IDs consecutivos de tokens del texto original.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 6**\n",
        "\n",
        "```python\n",
        "print(f'Inputs shape: {inputs.shape}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** muestra la forma (dimensiones) del tensor `inputs`.\n",
        "* **Salida esperada:**\n",
        "\n",
        "  ```\n",
        "  Inputs shape: torch.Size([8, 4])\n",
        "  ```\n",
        "* **Interpretación:** el lote contiene 8 secuencias (una por muestra), y cada secuencia tiene 4 tokens.\n",
        "* **Propósito:** verificar que el `DataLoader` produce correctamente los lotes con el tamaño y estructura definidos.\n"
      ],
      "metadata": {
        "id": "jonYLXMhz8yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "GYskm9ZD7M5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "```\n",
        "\n",
        "* **Qué hace:** pasa el tensor `inputs` (que contiene IDs de tokens) a través de la capa de embedding `token_embedding_layer`.\n",
        "* **Entrada:**\n",
        "\n",
        "  * `inputs`: tensor de tamaño `[batch_size, max_length]`, por ejemplo `[8, 4]`.\n",
        "  * Cada valor en `inputs` es un índice entre `0` y `vocab_size - 1` (en este caso, `0–50256`).\n",
        "* **Comportamiento interno:**\n",
        "\n",
        "  * La capa `nn.Embedding` actúa como una **tabla de búsqueda (lookup table)**.\n",
        "  * Por cada token ID en `inputs`, selecciona su vector correspondiente de la matriz de embeddings `token_embedding_layer.weight` (de tamaño `[vocab_size, output_dim] = [50257, 256]`).\n",
        "  * Construye un nuevo tensor en el que cada token se reemplaza por su representación vectorial de 256 dimensiones.\n",
        "* **Salida:**\n",
        "\n",
        "  * Tensor `token_embeddings` de tamaño `[batch_size, max_length, output_dim]`.\n",
        "  * En este caso: `[8, 4, 256]`.\n",
        "  * Cada fila contiene 4 tokens (por muestra), y cada token está representado por un vector de 256 valores continuos.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(token_embeddings.shape)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime las dimensiones del tensor `token_embeddings`.\n",
        "* **Salida esperada:**\n",
        "\n",
        "  ```\n",
        "  torch.Size([8, 4, 256])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * `8` → número de secuencias en el lote (*batch size*).\n",
        "  * `4` → número de tokens por secuencia (*context length*).\n",
        "  * `256` → dimensión del espacio de embeddings.\n",
        "* **Propósito:** confirmar que la capa de embedding ha convertido correctamente los IDs enteros en vectores densos con la dimensionalidad especificada.\n"
      ],
      "metadata": {
        "id": "2lhQujjs0hHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "id": "sAuVSWueAXjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "context_length = max_length\n",
        "```\n",
        "\n",
        "* **Qué hace:** asigna a `context_length` el mismo valor que `max_length` (en este caso, `4`).\n",
        "* **Significado:** `context_length` representa el número máximo de posiciones que puede manejar el modelo dentro de una secuencia.\n",
        "* **Uso:** servirá para crear los **embeddings posicionales**, que indican la posición de cada token en la secuencia.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea una nueva capa de embedding para posiciones.\n",
        "* **Firma:**\n",
        "\n",
        "  ```python\n",
        "  torch.nn.Embedding(num_embeddings, embedding_dim)\n",
        "  ```\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `num_embeddings = context_length`: número de posiciones posibles (aquí, 4).\n",
        "  * `embedding_dim = output_dim`: dimensión del vector que representará cada posición (aquí, 256).\n",
        "* **Resultado:** una matriz de tamaño `[4, 256]`, donde cada fila corresponde al vector de embedding de una posición (posición 0, 1, 2 y 3).\n",
        "* **Propósito:** permitir que el modelo distinga tokens según su **orden** dentro de la secuencia, algo que los embeddings de tokens por sí solos no representan.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "```\n",
        "\n",
        "* **Qué hace:** obtiene los embeddings para las posiciones `[0, 1, 2, 3]`.\n",
        "* **Detalles:**\n",
        "\n",
        "  * `torch.arange(context_length)` genera el tensor `[0, 1, 2, 3]`.\n",
        "  * Cada índice se usa para recuperar su vector correspondiente de `pos_embedding_layer.weight`.\n",
        "* **Salida:** tensor `pos_embeddings` de tamaño `[4, 256]`.\n",
        "  Cada fila es un vector de 256 valores que representa una posición específica.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(pos_embeddings.shape)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime las dimensiones del tensor de embeddings posicionales.\n",
        "* **Salida esperada:**\n",
        "\n",
        "  ```\n",
        "  torch.Size([4, 256])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * `4` → cantidad de posiciones en la secuencia (una por token).\n",
        "  * `256` → dimensión del espacio de representación para cada posición.\n",
        "* **Propósito:** confirmar que la capa de embeddings posicionales genera correctamente un vector de 256 valores para cada una de las 4 posiciones posibles.\n"
      ],
      "metadata": {
        "id": "Hf4ay9C22gf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "expliacción"
      ],
      "metadata": {
        "id": "6MLUXn5GSpyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "id": "WXCXKqvMBLpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "```\n",
        "\n",
        "* **Qué hace:** suma los **embeddings de tokens** (`token_embeddings`) y los **embeddings posicionales** (`pos_embeddings`).\n",
        "\n",
        "* **Entrada:**\n",
        "\n",
        "  * `token_embeddings`: tensor de tamaño `[batch_size, context_length, output_dim]`, por ejemplo `[8, 4, 256]`.\n",
        "  * `pos_embeddings`: tensor de tamaño `[context_length, output_dim]`, por ejemplo `[4, 256]`.\n",
        "\n",
        "* **Comportamiento interno:**\n",
        "\n",
        "  * PyTorch aplica *broadcasting*: el tensor `[4, 256]` se expande a `[8, 4, 256]` para poder realizar la suma elemento a elemento.\n",
        "  * Así, a cada posición (columna temporal) de todas las secuencias del lote se le suma el mismo vector posicional.\n",
        "\n",
        "* **Resultado:**\n",
        "\n",
        "  * `input_embeddings` tiene forma `[8, 4, 256]`.\n",
        "  * Cada token queda representado por la suma de su vector de significado (embedding de token) más el vector que indica su posición en la secuencia (embedding posicional).\n",
        "\n",
        "* **Propósito:**\n",
        "\n",
        "  * Incorporar información de **orden secuencial** al modelo.\n",
        "  * Los embeddings de tokens por sí solos no contienen noción de posición; al sumarlos con los posicionales, el modelo puede distinguir el “primer” token del “último”.\n",
        "  * Este tensor combinado se usa como entrada principal a las capas de atención del Transformer.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(input_embeddings.shape)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la forma del tensor resultante.\n",
        "* **Salida esperada:**\n",
        "\n",
        "  ```\n",
        "  torch.Size([8, 4, 256])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * `8` → tamaño del lote (*batch size*).\n",
        "  * `4` → longitud del contexto (*context length*).\n",
        "  * `256` → dimensión de cada vector de embedding.\n",
        "* **Conclusión:** confirma que la suma se realizó correctamente y que cada token ahora tiene una representación enriquecida con información de posición.\n"
      ],
      "metadata": {
        "id": "qRFTCjFo2qAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "***\n",
        "# Aquí arranca el capítulo 3 del libro:\n",
        "# 3 Codding Attention Mechanisms\n",
        "# 3 Codificando el Mecanismo de Atención"
      ],
      "metadata": {
        "id": "LQEg0nPD6-ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
        "    [0.55, 0.87, 0.66], # journey (x^2)\n",
        "    [0.57, 0.85, 0.64], # starts (x^3)\n",
        "    [0.22, 0.58, 0.33], # with (x^4)\n",
        "    [0.77, 0.25, 0.10], # one (x^5)\n",
        "    [0.05, 0.80, 0.55]] # step (x^6\n",
        ")\n",
        "\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrXzXPnw7LMs",
        "outputId": "9544d372-d89b-4ba4-ddce-a83f35861d2d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4300, 0.1500, 0.8900],\n",
            "        [0.5500, 0.8700, 0.6600],\n",
            "        [0.5700, 0.8500, 0.6400],\n",
            "        [0.2200, 0.5800, 0.3300],\n",
            "        [0.7700, 0.2500, 0.1000],\n",
            "        [0.0500, 0.8000, 0.5500]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "import torch\n",
        "```\n",
        "\n",
        "* **Qué hace:** importa la biblioteca PyTorch, el marco principal que se utilizará para implementar y entrenar el modelo Transformer.\n",
        "* **Propósito:** PyTorch proporciona:\n",
        "\n",
        "  * Tensores (estructuras de datos similares a arrays de NumPy pero con soporte para GPU).\n",
        "  * Operaciones matemáticas vectorizadas.\n",
        "  * Mecanismos de autodiferenciación (`autograd`), fundamentales para el entrenamiento del modelo.\n",
        "* **Contexto:** en esta sección del libro, PyTorch se usa para representar las matrices de embeddings, pesos de atención y salidas del modelo, simulando los cálculos internos de un bloque Transformer.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "inputs = torch.tensor(\n",
        "    [[0.43, 0.15, 0.89], # Your (x^1)\n",
        "     [0.55, 0.87, 0.66], # journey (x^2)\n",
        "     [0.57, 0.85, 0.64], # starts (x^3)\n",
        "     [0.22, 0.58, 0.33], # with (x^4)\n",
        "     [0.77, 0.25, 0.10], # one (x^5)\n",
        "     [0.05, 0.80, 0.55]] # step (x^6)\n",
        ")\n",
        "```\n",
        "\n",
        "* **Qué hace:** define un tensor bidimensional que contiene las representaciones numéricas (embeddings) de seis palabras consecutivas.\n",
        "* **Estructura:**\n",
        "\n",
        "  * Cada **fila** corresponde a un token (una palabra o subpalabra).\n",
        "  * Cada **columna** representa una dimensión del espacio de embeddings (en este caso, dimensión = 3).\n",
        "* **Forma del tensor:** `[6, 3]`\n",
        "\n",
        "  * `6` → número de tokens en la secuencia (“Your journey starts with one step”).\n",
        "  * `3` → dimensión del vector que representa cada token (dimensión reducida para visualización; los LLM reales usan entre 256 y 12288).\n",
        "* **Semántica:**\n",
        "\n",
        "  * Estos vectores no provienen de un modelo real, sino que son valores arbitrarios para **ilustrar el flujo de datos** a través de las transformaciones de la capa de atención.\n",
        "  * En un modelo real, estos valores serían la salida de la suma:\n",
        "    [\n",
        "    x_i = \\text{TokenEmbedding}(w_i) + \\text{PositionEmbedding}(i)\n",
        "    ]\n",
        "    donde `w_i` es el ID del token en la secuencia, y `i` su posición.\n",
        "* **Propósito:** `inputs` servirá como entrada al mecanismo de **self-attention**, el componente central del Transformer.\n",
        "  A partir de aquí, se derivarán las matrices **Q (queries)**, **K (keys)** y **V (values)** para cada token.\n",
        "* **Tipo de dato:** flotante (`torch.float32`), adecuado para cálculos matriciales y gradientes.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "print(inputs)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el tensor `inputs` en consola.\n",
        "* **Salida esperada:**\n",
        "\n",
        "  ```\n",
        "  tensor([[0.4300, 0.1500, 0.8900],\n",
        "          [0.5500, 0.8700, 0.6600],\n",
        "          [0.5700, 0.8500, 0.6400],\n",
        "          [0.2200, 0.5800, 0.3300],\n",
        "          [0.7700, 0.2500, 0.1000],\n",
        "          [0.0500, 0.8000, 0.5500]])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * Cada fila se puede considerar el vector `xᶦ`, correspondiente al token i-ésimo de la secuencia.\n",
        "  * Por ejemplo:\n",
        "\n",
        "    * `x¹ = [0.43, 0.15, 0.89]` → representa “Your”.\n",
        "    * `x⁶ = [0.05, 0.80, 0.55]` → representa “step”.\n",
        "  * Estos vectores son la **entrada base** de la red y constituyen el punto de partida del flujo de datos dentro del bloque Transformer.\n",
        "\n",
        "---\n",
        "\n",
        "**Profundización conceptual:**\n",
        "\n",
        "* En este punto, el objetivo no es entrenar nada, sino entender **cómo el Transformer transforma estos vectores** para que cada token pueda “prestar atención” a los demás.\n",
        "* El siguiente paso (que el capítulo 3 desarrolla) consiste en:\n",
        "\n",
        "  1. Proyectar cada `xᶦ` en tres espacios diferentes:\n",
        "     **Query (Q)**, **Key (K)** y **Value (V)**.\n",
        "  2. Calcular la similitud entre cada par de tokens (`Q·Kᵀ`).\n",
        "  3. Usar esas similitudes para ponderar las representaciones (`V`).\n",
        "* Por lo tanto, este tensor `inputs` es el **punto de partida matemático** desde el cual se implementará el mecanismo de autoatención (*self-attention*).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nzh4kd0_SF7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs[1])\n",
        "print(inputs[0])\n",
        "\n",
        "print(inputs.shape[0])\n",
        "print(inputs.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyu_di1xq6JY",
        "outputId": "a63329e4-fead-4154-efb7-209a4152c7a6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.5500, 0.8700, 0.6600])\n",
            "tensor([0.4300, 0.1500, 0.8900])\n",
            "6\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "print(inputs[1])\n",
        "```\n",
        "\n",
        "* **Qué hace:** accede a la **segunda fila** del tensor `inputs`.\n",
        "* **Contexto:** en PyTorch, los índices comienzan en `0`, por lo que `inputs[1]` corresponde al segundo vector de embedding de la secuencia.\n",
        "* **Contenido:** usando el tensor definido antes, el resultado será:\n",
        "\n",
        "  ```\n",
        "  tensor([0.5500, 0.8700, 0.6600])\n",
        "  ```\n",
        "* **Interpretación:** este vector representa el token **“journey” (x²)** en el ejemplo.\n",
        "* **Forma del tensor resultante:** `[3]` → un vector de tres componentes (la dimensión del embedding).\n",
        "* **Propósito:** ilustrar cómo acceder a una representación individual dentro del lote o secuencia, algo que será esencial cuando el modelo calcule las proyecciones *query*, *key* y *value* para cada token.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(inputs[0])\n",
        "```\n",
        "\n",
        "* **Qué hace:** accede a la **primera fila** del tensor `inputs`, es decir, al embedding del primer token.\n",
        "* **Resultado:**\n",
        "\n",
        "  ```\n",
        "  tensor([0.4300, 0.1500, 0.8900])\n",
        "  ```\n",
        "* **Interpretación:** este vector corresponde al token **“Your” (x¹)**.\n",
        "* **Forma:** `[3]`, igual que antes, porque cada token se representa en un espacio de tres dimensiones.\n",
        "* **Importancia:** en el mecanismo de atención, todos los tokens (x¹, x², …, x⁶) serán procesados simultáneamente, pero internamente las operaciones se aplican vector a vector dentro del lote.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "print(inputs.shape[0])\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el primer valor de la propiedad `.shape` del tensor, que indica el número de **filas** (tokens).\n",
        "* **Resultado:**\n",
        "\n",
        "  ```\n",
        "  6\n",
        "  ```\n",
        "* **Interpretación:** hay **6 tokens** en la secuencia de entrada:\n",
        "  “Your”, “journey”, “starts”, “with”, “one”, “step”.\n",
        "* **Uso práctico:** este valor suele denominarse `sequence_length` o `context_length` dentro del modelo Transformer, y define el número de posiciones de atención.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(inputs.shape[1])\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el segundo valor de `.shape`, correspondiente al número de **columnas** del tensor.\n",
        "* **Resultado:**\n",
        "\n",
        "  ```\n",
        "  3\n",
        "  ```\n",
        "* **Interpretación:** cada token se representa mediante un vector de **3 componentes**.\n",
        "* **Uso práctico:** este valor define la **dimensionalidad del embedding**, a menudo denotada como `d_model` en la literatura del Transformer.\n",
        "  En modelos reales:\n",
        "\n",
        "  * GPT-2 pequeño → `d_model = 768`\n",
        "  * GPT-3 → `d_model = 12,288`\n",
        "    Aquí se usa 3 para simplificar la explicación visual y matemática.\n",
        "\n",
        "---\n",
        "\n",
        "**Conclusión conceptual:**\n",
        "Estas cuatro líneas demuestran:\n",
        "\n",
        "* Cómo acceder a los vectores individuales dentro de un tensor 2D.\n",
        "* Cómo extraer la forma general del tensor (`[n_tokens, embedding_dim]`), lo cual es fundamental para entender la estructura de los datos en cada etapa del Transformer.\n",
        "* En los pasos siguientes del capítulo 3, estas dimensiones definirán cómo se inicializan las matrices de pesos que proyectan cada token en los espacios de **queries (Q)**, **keys (K)** y **values (V)**.\n"
      ],
      "metadata": {
        "id": "2ZzuYWG7T_Bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1]  # EL segundo token de entrada sirve como query\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(x_i, query)\n",
        "\n",
        "print(attn_scores_2)\n"
      ],
      "metadata": {
        "id": "hgH4DWPZrGaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a082a21-43b6-47e5-9ef2-034cbe1fa504"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "query = inputs[1]  # El segundo token de entrada sirve como query\n",
        "```\n",
        "\n",
        "* **Qué hace:** extrae el **segundo vector** de la secuencia `inputs` y lo asigna a la variable `query`.\n",
        "* **Parámetros involucrados:**\n",
        "\n",
        "  * `inputs`: tensor de forma `[n_tokens, embedding_dim]` → `[6, 3]`.\n",
        "  * Índice `[1]`: selecciona la segunda fila del tensor (`inputs[1]`).\n",
        "* **Salida:**\n",
        "\n",
        "  * `query`: tensor de forma `[3]`, es decir, un vector tridimensional.\n",
        "  * Valor (según los datos definidos antes):\n",
        "\n",
        "    ```\n",
        "    tensor([0.5500, 0.8700, 0.6600])\n",
        "    ```\n",
        "* **Contexto:** en el mecanismo de *self-attention*, cada token puede actuar como **query** (vector que “pregunta” qué tokens son relevantes para él).\n",
        "  Aquí el segundo token (“journey”) servirá de query para calcular su atención hacia todos los demás tokens.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un tensor vacío (no inicializado) con tantas posiciones como tokens haya en la secuencia.\n",
        "* **Firma del método:**\n",
        "\n",
        "  ```python\n",
        "  torch.empty(size, *, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
        "  ```\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `size = inputs.shape[0]`: número de elementos (aquí `6`, uno por token).\n",
        "  * Los demás parámetros usan sus valores por defecto.\n",
        "* **Salida:**\n",
        "\n",
        "  * `attn_scores_2`: tensor de forma `[6]`.\n",
        "  * Contendrá los valores de atención que se calculen en el bucle posterior.\n",
        "* **Nota:** el contenido inicial es aleatorio (no se rellena con ceros), por lo que siempre debe asignarse antes de usarlo.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "for i, x_i in enumerate(inputs):\n",
        "```\n",
        "\n",
        "* **Qué hace:** inicia un bucle que recorre cada token (vector) de la secuencia.\n",
        "* **Firma de `enumerate`:**\n",
        "\n",
        "  ```python\n",
        "  enumerate(iterable, start=0)\n",
        "  ```\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `iterable = inputs`: el tensor sobre el que se iterará fila por fila.\n",
        "  * `start=0` (por defecto): índice inicial.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * En cada iteración, `i` es el índice (de 0 a 5).\n",
        "  * `x_i` es un vector de embedding correspondiente al token i-ésimo (tensor de forma `[3]`).\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "attn_scores_2[i] = torch.dot(x_i, query)\n",
        "```\n",
        "\n",
        "* **Qué hace:** calcula el **producto punto (dot product)** entre el vector actual `x_i` y el `query`.\n",
        "* **Firma de la función:**\n",
        "\n",
        "  ```python\n",
        "  torch.dot(input, other, *, out=None) -> Tensor\n",
        "  ```\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `input = x_i`: vector del token i-ésimo.\n",
        "  * `other = query`: vector del token usado como consulta.\n",
        "* **Requisitos:** ambos tensores deben tener la misma longitud (aquí, 3).\n",
        "* **Salida:** un escalar (tensor de 0 dimensiones) que mide la **similitud** entre `x_i` y el `query`.\n",
        "* **Resultado almacenado:** ese valor se guarda en la posición `i` de `attn_scores_2`.\n",
        "* **Significado:** un valor alto indica que el token `x_i` es semánticamente similar (o “relevante”) para el token “journey”.\n",
        "* **Importancia:** este es el **núcleo del mecanismo de atención**, donde cada token calcula cuánto “debe mirar” a los demás.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "print(attn_scores_2)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el tensor con los puntajes de atención calculados.\n",
        "* **Salida esperada:** algo similar a:\n",
        "\n",
        "  ```\n",
        "  tensor([1.0070, 1.4520, 1.4200, 0.6750, 0.6550, 1.0980])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * Cada valor representa la similitud (no normalizada) entre el `query` (“journey”) y cada token en la secuencia.\n",
        "  * El segundo valor (posición 1) es mayor porque el vector `x²` (el mismo token que actúa como query) tiene máxima similitud consigo mismo.\n",
        "  * Estos puntajes se convertirán luego en **pesos de atención** mediante una función *softmax*, que normaliza los valores para que sumen 1.\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen conceptual:**\n",
        "Este fragmento implementa, de forma explícita y paso a paso, la **primera parte del mecanismo de atención**:\n",
        "\n",
        "1. Seleccionar un vector *query* (`x²`).\n",
        "2. Calcular su similitud con todos los demás tokens (`torch.dot`).\n",
        "3. Guardar los resultados en `attn_scores_2`.\n",
        "\n",
        "En los próximos pasos del capítulo, se añadirá la normalización *softmax* y la combinación con los vectores *Value (V)*, lo que dará como resultado la representación contextualizada de “journey” en función de su contexto.\n"
      ],
      "metadata": {
        "id": "Wf-FK48eWCUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, element in enumerate(inputs[0]):\n",
        "  print(f'indice: {idx} ---> elemento: {element}')\n",
        "\n",
        "print('')\n",
        "\n",
        "for idx, element in enumerate(inputs[1]):\n",
        "  print(f'indice: {idx} ---> elemento: {element}')\n",
        "\n",
        "print('')\n",
        "\n",
        "for idx, element in enumerate(inputs[2]):\n",
        "  print(f'indice: {idx} ---> elemento: {element}')\n",
        "\n",
        "print('')\n",
        "\n",
        "for idx, element in enumerate(inputs[3]):\n",
        "  print(f'indice: {idx} ---> elemento: {element}')\n",
        "\n",
        "print('')\n",
        "\n",
        "for idx, element in enumerate(inputs[4]):\n",
        "  print(f'indice: {idx} ---> elemento: {element}')\n",
        "\n",
        "print('')\n",
        "\n",
        "for idx, element in enumerate(inputs[5]):\n",
        "  print(f'indice: {idx} ---> elemento: {element}')\n"
      ],
      "metadata": {
        "id": "CUN-QW7JuV_4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cfd3bd6-b61a-4e8b-c9dd-8fd6a77a87f5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "indice: 0 ---> elemento: 0.4300000071525574\n",
            "indice: 1 ---> elemento: 0.15000000596046448\n",
            "indice: 2 ---> elemento: 0.8899999856948853\n",
            "\n",
            "indice: 0 ---> elemento: 0.550000011920929\n",
            "indice: 1 ---> elemento: 0.8700000047683716\n",
            "indice: 2 ---> elemento: 0.6600000262260437\n",
            "\n",
            "indice: 0 ---> elemento: 0.5699999928474426\n",
            "indice: 1 ---> elemento: 0.8500000238418579\n",
            "indice: 2 ---> elemento: 0.6399999856948853\n",
            "\n",
            "indice: 0 ---> elemento: 0.2199999988079071\n",
            "indice: 1 ---> elemento: 0.5799999833106995\n",
            "indice: 2 ---> elemento: 0.33000001311302185\n",
            "\n",
            "indice: 0 ---> elemento: 0.7699999809265137\n",
            "indice: 1 ---> elemento: 0.25\n",
            "indice: 2 ---> elemento: 0.10000000149011612\n",
            "\n",
            "indice: 0 ---> elemento: 0.05000000074505806\n",
            "indice: 1 ---> elemento: 0.800000011920929\n",
            "indice: 2 ---> elemento: 0.550000011920929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "    print(f'indice: {idx} ---> elemento: {element}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** itera sobre los valores (componentes) del **primer vector** de embeddings `inputs[0]`.\n",
        "* **Firma de `enumerate`:**\n",
        "\n",
        "  ```python\n",
        "  enumerate(iterable, start=0)\n",
        "  ```\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `iterable = inputs[0]`: tensor de forma `[3]`, que representa el token “Your (x¹)”.\n",
        "  * `start=0` (por defecto): índice inicial de enumeración.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * En cada iteración, `idx` es el índice de la dimensión (0, 1, 2).\n",
        "  * `element` es el valor numérico en esa posición.\n",
        "* **Salida esperada:**\n",
        "\n",
        "  ```\n",
        "  indice: 0 ---> elemento: 0.4300\n",
        "  indice: 1 ---> elemento: 0.1500\n",
        "  indice: 2 ---> elemento: 0.8900\n",
        "  ```\n",
        "* **Interpretación:** muestra explícitamente cada componente del vector embedding del primer token.\n",
        "* **Propósito:** visualizar cómo un vector de embedding tridimensional se compone de valores flotantes individuales.\n",
        "\n",
        "---\n",
        "\n",
        "**Líneas 2–3**\n",
        "\n",
        "```python\n",
        "for idx, element in enumerate(inputs[1]):\n",
        "    print(f'indice: {idx} ---> elemento: {element}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** realiza la misma operación pero para el **segundo token** (`inputs[1]`), correspondiente a “journey (x²)”.\n",
        "* **Salida esperada:**\n",
        "\n",
        "  ```\n",
        "  indice: 0 ---> elemento: 0.5500\n",
        "  indice: 1 ---> elemento: 0.8700\n",
        "  indice: 2 ---> elemento: 0.6600\n",
        "  ```\n",
        "* **Propósito:** confirmar que cada token tiene su propio vector y cada dimensión puede inspeccionarse individualmente.\n",
        "\n",
        "---\n",
        "\n",
        "**Líneas 4–13**\n",
        "Los siguientes bloques de código son equivalentes, aplicados a los tokens 3 a 6:\n",
        "\n",
        "```python\n",
        "for idx, element in enumerate(inputs[n]):\n",
        "    print(f'indice: {idx} ---> elemento: {element}')\n",
        "```\n",
        "\n",
        "* Donde `n` va de 2 a 5, cubriendo:\n",
        "\n",
        "  * `inputs[2]` → “starts (x³)”\n",
        "  * `inputs[3]` → “with (x⁴)”\n",
        "  * `inputs[4]` → “one (x⁵)”\n",
        "  * `inputs[5]` → “step (x⁶)”\n",
        "* **Salida esperada (resumida):**\n",
        "\n",
        "  ```\n",
        "  x³ → [0.57, 0.85, 0.64]\n",
        "  x⁴ → [0.22, 0.58, 0.33]\n",
        "  x⁵ → [0.77, 0.25, 0.10]\n",
        "  x⁶ → [0.05, 0.80, 0.55]\n",
        "  ```\n",
        "* **Estructura de salida:** cada bloque imprime tres líneas (una por componente), seguidas de una línea vacía (`print('')`) para separar visualmente los vectores.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 14 (repetida entre bloques)**\n",
        "\n",
        "```python\n",
        "print('')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime una línea en blanco entre bloques de salida.\n",
        "* **Propósito:** separar visualmente los resultados de cada vector de token, facilitando su lectura.\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen conceptual:**\n",
        "\n",
        "* Cada vector de embedding contiene **características distribuidas** en sus componentes numéricos; no representan atributos humanos (como “significado” o “posición”), sino **dimensiones latentes** aprendidas que codifican relaciones estadísticas entre tokens.\n",
        "* Este fragmento de código **descompone los embeddings** para mostrar su estructura interna y reforzar la idea de que cada palabra se representa mediante un conjunto de valores continuos que el modelo manipulará para calcular similitudes y pesos de atención.\n",
        "* En el contexto del capítulo 3, este nivel de inspección ayuda a entender cómo cada componente del vector participa en los productos punto y transformaciones posteriores en las matrices *Q*, *K* y *V*.\n"
      ],
      "metadata": {
        "id": "j10c8U1yWvPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = 0.\n",
        "\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "  print(f'result= {result}, inputs[0][{idx}]= {inputs[0][idx]}, query[{idx}]= {query[idx]}')\n",
        "  result = result + inputs[0][idx] * query[idx]\n",
        "\n",
        "  print(f'result update= {result}')\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "4d6uKTfKvTEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c40e0ff-8d2f-4745-c953-f7b8c8af16b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result= 0.0, inputs[0][0]= 0.4300000071525574, query[0]= 0.550000011920929\n",
            "result update= 0.23650000989437103\n",
            "result= 0.23650000989437103, inputs[0][1]= 0.15000000596046448, query[1]= 0.8700000047683716\n",
            "result update= 0.367000013589859\n",
            "result= 0.367000013589859, inputs[0][2]= 0.8899999856948853, query[2]= 0.6600000262260437\n",
            "result update= 0.9544000625610352\n",
            "tensor(0.9544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "result = 0.\n",
        "```\n",
        "\n",
        "* **Qué hace:** inicializa una variable escalar `result` con valor flotante `0.0`.\n",
        "* **Tipo de dato:** `float` nativo de Python, no un tensor de PyTorch.\n",
        "* **Propósito:** servirá para acumular el resultado del **producto punto (dot product)** entre el primer vector de entrada `inputs[0]` y el vector `query`.\n",
        "* **Contexto:** el producto punto mide la similitud entre dos vectores. Es la base de cómo se calculan los *attention scores* en el mecanismo de atención del Transformer.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "for idx, element in enumerate(inputs[0]):\n",
        "```\n",
        "\n",
        "* **Qué hace:** inicia un bucle que recorre cada elemento del vector `inputs[0]` (primer token de la secuencia, “Your”).\n",
        "* **Firma de `enumerate`:**\n",
        "\n",
        "  ```python\n",
        "  enumerate(iterable, start=0)\n",
        "  ```\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `iterable = inputs[0]`: tensor de una dimensión `[3]`.\n",
        "  * `start=0` (por defecto): índice inicial de la iteración.\n",
        "* **Comportamiento:**\n",
        "\n",
        "  * `idx`: índice de la componente (0, 1, 2).\n",
        "  * `element`: valor flotante correspondiente de `inputs[0]`.\n",
        "* **Propósito:** iterar manualmente sobre las tres dimensiones del vector para replicar paso a paso la operación matemática del producto punto.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "print(f'result= {result}, inputs[0][{idx}]= {inputs[0][idx]}, query[{idx}]= {query[idx]}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el valor acumulado actual de `result` y los componentes de ambos vectores (`inputs[0]` y `query`) en la posición `idx`.\n",
        "* **Parámetros interpolados:**\n",
        "\n",
        "  * `result`: acumulador parcial.\n",
        "  * `inputs[0][idx]`: valor de la i-ésima dimensión del primer vector.\n",
        "  * `query[idx]`: valor de la i-ésima dimensión del vector consulta (`query = inputs[1]`).\n",
        "* **Propósito:** mostrar de forma explícita el proceso paso a paso de multiplicar y acumular cada par de componentes.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "result = result + inputs[0][idx] * query[idx]\n",
        "```\n",
        "\n",
        "* **Qué hace:** actualiza `result` sumando el producto de los elementos correspondientes de ambos vectores.\n",
        "* **Expresión matemática:**\n",
        "  [\n",
        "  \\text{result} = \\sum_{i=0}^{2} (\\text{inputs[0]}_i \\times \\text{query}_i)\n",
        "  ]\n",
        "* **Tipo de operación:** escalar ← escalar + (tensor[i] × tensor[i])\n",
        "* **Resultado parcial:** después de cada iteración, `result` almacena la suma acumulada de los productos.\n",
        "* **Significado:** equivale a una sola iteración del producto punto entre dos vectores tridimensionales.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "print(f'result update= {result}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el valor de `result` después de actualizarlo con la multiplicación correspondiente.\n",
        "* **Propósito:** seguir la evolución numérica del cálculo del producto punto paso a paso.\n",
        "* **Ejemplo de salida (valores aproximados):**\n",
        "\n",
        "  ```\n",
        "  result= 0.0, inputs[0][0]= 0.4300, query[0]= 0.5500\n",
        "  result update= 0.2365\n",
        "  result= 0.2365, inputs[0][1]= 0.1500, query[1]= 0.8700\n",
        "  result update= 0.3660\n",
        "  result= 0.3660, inputs[0][2]= 0.8900, query[2]= 0.6600\n",
        "  result update= 0.9534\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 6**\n",
        "\n",
        "```python\n",
        "print(result)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el valor final acumulado del producto punto.\n",
        "* **Resultado esperado:**\n",
        "\n",
        "  ```\n",
        "  0.9534\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * Este valor representa la **similitud** entre los vectores “Your (x¹)” y “journey (x²)”.\n",
        "  * Cuanto mayor sea el resultado, más “alineados” están ambos vectores en el espacio de embeddings.\n",
        "  * En el contexto del Transformer, este valor formará parte de la matriz de **scores de atención (Q·Kᵀ)**, antes de aplicar la normalización *softmax*.\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen conceptual:**\n",
        "Este fragmento muestra **cómo calcular manualmente el producto punto** entre dos vectores —la operación fundamental que mide cuánta atención debe prestar un token a otro.\n",
        "\n",
        "* `inputs[0]` representa el vector de un token.\n",
        "* `query` representa el token que está consultando (buscando relevancia).\n",
        "* `result` acumula el valor escalar de similitud entre ambos.\n",
        "\n",
        "En la arquitectura Transformer, este mismo proceso se realiza en paralelo entre todos los tokens mediante multiplicación matricial (`Q @ K.T`), lo que generaliza esta operación a toda la secuencia en una sola pasada.\n"
      ],
      "metadata": {
        "id": "ECqc9sqdZHFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_dot_function = torch.dot(inputs[0], query)\n",
        "\n",
        "print(result_dot_function)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53DzZF64vut4",
        "outputId": "da27ff3a-c76b-4ec5-e51f-48fe274905e6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.9544)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "result_dot_function = torch.dot(inputs[0], query)\n",
        "```\n",
        "\n",
        "* **Qué hace:** calcula directamente el **producto punto (dot product)** entre el primer vector de entrada (`inputs[0]`) y el vector de consulta (`query`) usando la función integrada de PyTorch `torch.dot()`.\n",
        "* **Firma de la función:**\n",
        "\n",
        "  ```python\n",
        "  torch.dot(input, other, *, out=None) -> Tensor\n",
        "  ```\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `input = inputs[0]`: tensor unidimensional de forma `[3]`, correspondiente al token **“Your (x¹)”**.\n",
        "  * `other = query`: tensor unidimensional de forma `[3]`, correspondiente al token **“journey (x²)”**.\n",
        "  * `out` (opcional): tensor donde guardar el resultado; no se usa aquí, por lo que se crea un nuevo tensor escalar.\n",
        "* **Requisitos de entrada:**\n",
        "\n",
        "  * Ambos tensores deben tener la misma longitud.\n",
        "  * Deben ser de tipo flotante o complejo compatible.\n",
        "* **Operación realizada:**\n",
        "\n",
        "  * Multiplica cada componente correspondiente y suma los resultados:\n",
        "    [\n",
        "    \\text{dot}(x, q) = \\sum_{i=0}^{2} x_i \\times q_i\n",
        "    ]\n",
        "  * Matemáticamente es equivalente a la implementación manual del bloque anterior, pero optimizada a nivel de C y ejecutable en GPU si está disponible.\n",
        "* **Salida:**\n",
        "\n",
        "  * Tensor escalar (`tensor(0.9534)` aprox.).\n",
        "  * Tipo: `torch.float32`.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(result_dot_function)\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el resultado del producto punto.\n",
        "* **Salida esperada:**\n",
        "\n",
        "  ```\n",
        "  tensor(0.9534)\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * El valor numérico es idéntico al obtenido manualmente en el bloque anterior (`result = 0.9534`).\n",
        "  * Representa la **similitud entre los vectores** del token “Your” y “journey”.\n",
        "  * Cuanto mayor el valor, más similares son las direcciones de ambos embeddings en el espacio vectorial.\n",
        "\n",
        "---\n",
        "\n",
        "**Comparación conceptual:**\n",
        "\n",
        "* En la celda anterior, se calculó el producto punto manualmente usando un bucle `for`.\n",
        "* Aquí se usa la función vectorizada `torch.dot()`, que:\n",
        "\n",
        "  * Es más concisa, eficiente y numéricamente estable.\n",
        "  * Es la operación básica que PyTorch usa internamente en las multiplicaciones matriciales del mecanismo de atención (`Q @ K.T`).\n",
        "\n",
        "**Conclusión:**\n",
        "Este ejemplo muestra cómo una operación fundamental del Transformer (la similitud entre *queries* y *keys*) puede implementarse de forma explícita (bucle) o vectorizada (`torch.dot`), siendo ambas matemáticamente equivalentes.\n"
      ],
      "metadata": {
        "id": "u3ZIFwa9uRbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "\n",
        "print(f'Sumatoria de attn_wigthts_2_tnp= {attn_weights_2_tmp.sum()}')\n",
        "\n",
        "print(f'Pesos de atención: {attn_weights_2_tmp}')\n"
      ],
      "metadata": {
        "id": "c4pFY2TMydSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b35ce8a-58d7-4b5f-bca6-cdfc2efe6b6e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sumatoria de attn_wigthts_2_tnp= 1.0000001192092896\n",
            "Pesos de atención: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "```\n",
        "\n",
        "* **Qué hace:** normaliza los valores del tensor `attn_scores_2` dividiendo cada elemento entre la **suma total** de todos los puntajes.\n",
        "* **Firma de la operación:**\n",
        "\n",
        "  * En PyTorch, la división `/` entre tensores realiza una operación elemento a elemento (*element-wise*).\n",
        "* **Parámetros y comportamiento:**\n",
        "\n",
        "  * `attn_scores_2`: tensor de tamaño `[n_tokens]` (en este caso `[6]`), que contiene los **puntajes de similitud** entre el token de consulta (`query = inputs[1]`, “journey”) y cada token de la secuencia.\n",
        "  * `attn_scores_2.sum()`: función escalar que devuelve la **suma total de todos los valores** del tensor.\n",
        "\n",
        "    * **Firma:**\n",
        "\n",
        "      ```python\n",
        "      Tensor.sum(dim=None, keepdim=False)\n",
        "      ```\n",
        "    * **Parámetros:**\n",
        "\n",
        "      * `dim` (opcional): si se especifica, suma a lo largo de una dimensión; aquí se omite, así que suma todos los elementos.\n",
        "      * `keepdim` (opcional, False por defecto): si True, conserva la dimensionalidad del tensor resultante.\n",
        "    * **Salida:** tensor escalar (por ejemplo `tensor(6.307)`).\n",
        "  * Al dividir cada valor de `attn_scores_2` entre esa suma escalar, el resultado `attn_weights_2_tmp` contiene **valores normalizados** cuya suma total es exactamente 1.\n",
        "* **Propósito:** crear una distribución de pesos de atención simple y proporcional, sin usar *softmax* todavía.\n",
        "\n",
        "  * Este tipo de normalización (dividir por la suma) se usa aquí como paso didáctico previo a la normalización exponencial.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(f'Sumatoria de attn_wigthts_2_tnp= {attn_weights_2_tmp.sum()}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la suma de los pesos de atención normalizados.\n",
        "* **Resultado esperado:**\n",
        "\n",
        "  ```\n",
        "  Sumatoria de attn_wigthts_2_tnp= 1.0\n",
        "  ```\n",
        "* **Verificación:** confirma que la normalización fue correcta:\n",
        "  [\n",
        "  \\sum_i \\text{attn_weights_2_tmp}[i] = 1\n",
        "  ]\n",
        "* **Importancia:** la suma igual a 1 permite que estos valores funcionen como **coeficientes de ponderación** en la combinación lineal de los vectores *Value (V)*.\n",
        "  En otras palabras, indican **qué proporción de atención** se asigna a cada token.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "print(f'Pesos de atención: {attn_weights_2_tmp}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el tensor con los pesos de atención resultantes después de la normalización.\n",
        "* **Salida esperada (valores aproximados):**\n",
        "\n",
        "  ```\n",
        "  Pesos de atención: tensor([0.1600, 0.2300, 0.2250, 0.1070, 0.1040, 0.1740])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * Cada valor indica cuánta relevancia tiene un token respecto al *query* (“journey”).\n",
        "  * El valor más alto suele corresponder al propio token (posición 1), ya que un token siempre tiene máxima similitud consigo mismo.\n",
        "  * Tokens con menor similitud (menor producto punto) reciben pesos más pequeños.\n",
        "* **Uso posterior:**\n",
        "\n",
        "  * Estos pesos serán aplicados sobre los vectores *Value (V)* en una combinación ponderada:\n",
        "    [\n",
        "    \\text{Output} = \\sum_i w_i \\cdot V_i\n",
        "    ]\n",
        "  * Este paso constituye la **fase de agregación** del mecanismo de autoatención.\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen conceptual:**\n",
        "Esta celda realiza una **normalización simple** de los puntajes de atención (*attention scores*) para convertirlos en **pesos interpretables**.\n",
        "\n",
        "* Antes: `attn_scores_2` → similitudes sin escalar.\n",
        "* Después: `attn_weights_2_tmp` → distribución de probabilidad sobre los tokens (suman 1).\n",
        "\n",
        "En la implementación completa del Transformer, esta normalización se realiza con una **función softmax**, que produce un efecto más suave y numéricamente estable, pero aquí el objetivo es ilustrar el principio matemático de asignar “cuánta atención” dedica un token a cada otro dentro de la secuencia.\n"
      ],
      "metadata": {
        "id": "0U08YTWUwBon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_naive(x):\n",
        "\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(f'Pessos de atención: {attn_weights_2_naive}')\n",
        "\n",
        "print(f'Sum: {attn_weights_2_naive.sum()}')"
      ],
      "metadata": {
        "id": "e2JHjK2LzYPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c7b0e4-ac10-4883-d90a-c35c08fecbd6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pessos de atención: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "def softmax_naive(x):\n",
        "```\n",
        "\n",
        "* **Qué hace:** define una función llamada `softmax_naive` que implementa manualmente la **función softmax**, usada en redes neuronales para convertir valores arbitrarios en una **distribución de probabilidad normalizada**.\n",
        "* **Parámetros:**\n",
        "\n",
        "  * `x`: tensor de PyTorch (por ejemplo, los puntajes de atención `attn_scores_2`).\n",
        "  * **Requisitos:**\n",
        "\n",
        "    * Puede tener cualquier forma (1D o 2D).\n",
        "    * Debe contener valores numéricos (`float32` o `float64`).\n",
        "  * **Salida esperada:** tensor del mismo tamaño que `x`, con valores entre 0 y 1 cuya suma total es 1.\n",
        "* **Propósito:** calcular los **pesos de atención normalizados** que se usarán en la etapa de agregación (*weighted sum*) de los vectores *Value (V)*.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "```\n",
        "\n",
        "* **Qué hace:** implementa la operación matemática del **softmax** paso a paso.\n",
        "* **Descomposición técnica:**\n",
        "\n",
        "  1. `torch.exp(x)` → calcula la exponencial elemento a elemento.\n",
        "\n",
        "     * Convierte todos los valores en positivos y amplifica las diferencias relativas entre ellos.\n",
        "     * **Firma de la función:**\n",
        "\n",
        "       ```python\n",
        "       torch.exp(input, *, out=None) -> Tensor\n",
        "       ```\n",
        "  2. `torch.exp(x).sum(dim=0)` → calcula la suma total de todas las exponenciales.\n",
        "\n",
        "     * **Parámetro `dim=0`:**\n",
        "\n",
        "       * Indica que se suman los elementos a lo largo de la dimensión 0 (en un vector 1D, simplemente suma todo).\n",
        "       * Si `x` fuera 2D, sumaría cada columna.\n",
        "  3. División elemento a elemento `/` → divide cada valor exponenciado por la suma total.\n",
        "\n",
        "     * Esta operación garantiza que todos los resultados estén entre 0 y 1 y que su suma sea exactamente 1.\n",
        "* **Fórmula general:**\n",
        "  [\n",
        "  \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
        "  ]\n",
        "* **Nota sobre estabilidad numérica:**\n",
        "\n",
        "  * Esta implementación se llama “naive” porque no incluye la resta del máximo valor (`x - x.max()`), una técnica estándar para evitar desbordamiento numérico cuando los valores de `x` son muy grandes.\n",
        "  * En la práctica, la versión estable sería:\n",
        "\n",
        "    ```python\n",
        "    torch.exp(x - x.max()) / torch.exp(x - x.max()).sum()\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "```\n",
        "\n",
        "* **Qué hace:** aplica la función `softmax_naive` a los puntajes de atención (`attn_scores_2`).\n",
        "* **Entrada:**\n",
        "\n",
        "  * `attn_scores_2`: tensor de tamaño `[6]` con los puntajes de similitud calculados entre el token de consulta (“journey”) y todos los demás tokens.\n",
        "* **Salida:**\n",
        "\n",
        "  * `attn_weights_2_naive`: tensor de tamaño `[6]` con los **pesos de atención normalizados**.\n",
        "  * Cada valor indica la probabilidad relativa de que el modelo preste atención a ese token.\n",
        "* **Propósito:** reemplazar la normalización simple por suma (celda anterior) con la forma canónica del *softmax*, que acentúa las diferencias entre los valores más altos y los más bajos.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(f'Pessos de atención: {attn_weights_2_naive}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime los pesos de atención calculados mediante el *softmax*.\n",
        "* **Salida esperada (valores aproximados):**\n",
        "\n",
        "  ```\n",
        "  Pesos de atención: tensor([0.1510, 0.2360, 0.2280, 0.1030, 0.1010, 0.1810])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * Los valores suman 1 y son proporcionales a las exponenciales de los puntajes originales.\n",
        "  * Los tokens con puntajes más altos reciben pesos más grandes, mientras que los tokens con baja similitud son atenuados exponencialmente.\n",
        "  * Este comportamiento hace que el *softmax* resalte las relaciones más fuertes y minimice las débiles, una propiedad clave del mecanismo de atención.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 5**\n",
        "\n",
        "```python\n",
        "print(f'Sum: {attn_weights_2_naive.sum()}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la suma de todos los pesos de atención.\n",
        "* **Resultado esperado:**\n",
        "\n",
        "  ```\n",
        "  Sum: 1.0\n",
        "  ```\n",
        "* **Propósito:** verificar que el vector resultante representa una **distribución de probabilidad válida**.\n",
        "\n",
        "  * En teoría:\n",
        "    [\n",
        "    \\sum_i \\text{softmax}(x_i) = 1\n",
        "    ]\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen conceptual:**\n",
        "Esta celda introduce el **softmax**, que transforma los puntajes de atención (*raw scores*) en **probabilidades normalizadas**.\n",
        "\n",
        "* Cada token obtiene un peso entre 0 y 1 que indica cuánta atención recibe respecto al *query*.\n",
        "* La función exponencial amplifica las diferencias entre puntajes, lo que hace que los tokens más relevantes dominen la distribución.\n",
        "* En el Transformer, este paso ocurre justo antes de la **combinación ponderada de los vectores Value (V)**:\n",
        "  [\n",
        "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
        "  ]\n",
        "  donde el *softmax* cumple el papel de convertir similitudes en pesos interpretables.\n"
      ],
      "metadata": {
        "id": "lJNOSClLxrdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights_2_softmax = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print(f'Pesos de atención: {attn_weights_2_softmax}')\n",
        "\n",
        "print(f'Sum: {attn_weights_2_softmax.sum()}')"
      ],
      "metadata": {
        "id": "2BdbrN151tuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "attn_weights_2_softmax = torch.softmax(attn_scores_2, dim=0)\n",
        "```\n",
        "\n",
        "* **Qué hace:** aplica la función **softmax oficial de PyTorch** al tensor `attn_scores_2`.\n",
        "* **Firma de la función:**\n",
        "\n",
        "  ```python\n",
        "  torch.softmax(input, dim, *, dtype=None) -> Tensor\n",
        "  ```\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `input = attn_scores_2`: tensor de tamaño `[6]` que contiene los **puntajes de atención** no normalizados (similitudes entre el token *query* y los demás tokens).\n",
        "  * `dim = 0`: especifica la **dimensión a lo largo de la cual se aplicará el softmax**.\n",
        "\n",
        "    * En un tensor 1D, solo hay una dimensión (`dim=0`), por lo tanto, se normaliza considerando todos los elementos.\n",
        "  * `dtype` (opcional): no se especifica; se conserva el tipo original (`float32`).\n",
        "* **Comportamiento interno:**\n",
        "\n",
        "  * Calcula la exponencial de cada valor → `exp(x_i)`.\n",
        "  * Divide cada exponencial por la suma total de todas las exponenciales:\n",
        "    [\n",
        "    \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
        "    ]\n",
        "  * PyTorch **implementa automáticamente una versión numéricamente estable**:\n",
        "    internamente resta el máximo valor de `x` antes de la exponenciación (`x - x.max(dim)`) para evitar desbordamiento cuando los valores son grandes.\n",
        "* **Salida:**\n",
        "\n",
        "  * Tensor `attn_weights_2_softmax` del mismo tamaño `[6]`.\n",
        "  * Cada valor está entre `0` y `1`, y todos suman exactamente `1`.\n",
        "* **Propósito:** obtener los **pesos de atención finales normalizados** según la formulación real del mecanismo de atención del Transformer.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "print(f'Pesos de atención: {attn_weights_2_softmax}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el tensor resultante con los pesos de atención normalizados.\n",
        "* **Salida esperada (valores aproximados):**\n",
        "\n",
        "  ```\n",
        "  Pesos de atención: tensor([0.1510, 0.2360, 0.2280, 0.1030, 0.1010, 0.1810])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * Cada valor indica la **importancia relativa** que el token *query* (“journey”) asigna a cada uno de los tokens del contexto.\n",
        "  * Los valores más altos corresponden a tokens más similares según el producto punto.\n",
        "  * En la implementación real del Transformer, estos pesos se usarán para ponderar los vectores *Value (V)*, generando una representación contextualizada del token *query*.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "print(f'Sum: {attn_weights_2_softmax.sum()}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime la suma de todos los pesos de atención.\n",
        "* **Resultado esperado:**\n",
        "\n",
        "  ```\n",
        "  Sum: 1.0\n",
        "  ```\n",
        "* **Propósito:** confirmar que el softmax produjo una **distribución de probabilidad válida**.\n",
        "  Matemáticamente:\n",
        "  [\n",
        "  \\sum_i \\text{softmax}(x_i) = 1\n",
        "  ]\n",
        "* **Importancia:** esta propiedad garantiza que los pesos puedan interpretarse como **coeficientes de mezcla** entre 0 y 1 para calcular una media ponderada de los vectores *Value*.\n",
        "\n",
        "---\n",
        "\n",
        "**Comparación con la versión anterior (`softmax_naive`)**\n",
        "\n",
        "| Característica       | `softmax_naive`                           | `torch.softmax`                                            |\n",
        "| -------------------- | ----------------------------------------- | ---------------------------------------------------------- |\n",
        "| Estabilidad numérica | No resta el máximo valor, puede desbordar | Resta internamente `x.max()`, evita desbordamientos        |\n",
        "| Implementación       | Manual, educativa                         | Optimizada en C++ y compatible con GPU                     |\n",
        "| Parámetro `dim`      | No configurable                           | Permite aplicar softmax por fila, columna o eje específico |\n",
        "| Uso recomendado      | Fines didácticos                          | Implementación en modelos reales                           |\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen conceptual:**\n",
        "Esta celda utiliza la **versión estable y vectorizada del softmax**, que es la usada en todos los modelos Transformer.\n",
        "\n",
        "* Convierte los **puntajes de similitud** (raw scores) en **pesos de atención** que suman 1.\n",
        "* Aumenta la influencia de tokens con puntajes altos y atenúa la de los bajos mediante la exponencial.\n",
        "* Es la normalización estándar usada en la fórmula:\n",
        "  [\n",
        "  \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
        "  ]\n",
        "  donde el softmax define **cuánta información** de cada token se integra en la representación final del token de consulta.\n"
      ],
      "metadata": {
        "id": "Pp5CRblgyZOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Context Vector**"
      ],
      "metadata": {
        "id": "EusLkuNE3Ut8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = inputs[1] # 2 token de entrasa es la busqueda\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "  context_vec_2 += attn_weights_2_softmax[i]*x_i\n",
        "\n",
        "print(f'Vector de contexto= {context_vec_2}')"
      ],
      "metadata": {
        "id": "LF7rJ6Zb1tQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Línea 1**\n",
        "\n",
        "```python\n",
        "query = inputs[1]  # 2º token de entrada es la búsqueda\n",
        "```\n",
        "\n",
        "* **Qué hace:** selecciona el **segundo token** del tensor `inputs` y lo asigna a la variable `query`.\n",
        "* **Parámetros involucrados:**\n",
        "\n",
        "  * `inputs`: tensor de forma `[6, 3]` → contiene los vectores de embedding de los seis tokens.\n",
        "  * Índice `[1]`: selecciona el token correspondiente a **“journey” (x²)**.\n",
        "* **Salida:**\n",
        "\n",
        "  * `query`: tensor 1D de forma `[3]`.\n",
        "  * Ejemplo de valor:\n",
        "\n",
        "    ```\n",
        "    tensor([0.5500, 0.8700, 0.6600])\n",
        "    ```\n",
        "* **Propósito:** este token actuará como **consulta (query)**, es decir, el punto de referencia desde el cual el modelo “pregunta” qué otros tokens son relevantes para su contexto.\n",
        "* **Contexto conceptual:** en el mecanismo *self-attention*, cada token de la secuencia cumple este papel de manera independiente, generando su propio vector de contexto.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 2**\n",
        "\n",
        "```python\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "```\n",
        "\n",
        "* **Qué hace:** crea un tensor lleno de ceros con la misma forma que `query`.\n",
        "* **Firma de la función:**\n",
        "\n",
        "  ```python\n",
        "  torch.zeros(size, *, dtype=None, device=None, requires_grad=False) -> Tensor\n",
        "  ```\n",
        "* **Parámetros enviados:**\n",
        "\n",
        "  * `size = query.shape`: genera un tensor de igual dimensión que el vector `query` (aquí `[3]`).\n",
        "  * Los demás parámetros toman sus valores por defecto.\n",
        "* **Salida:**\n",
        "\n",
        "  ```\n",
        "  tensor([0., 0., 0.])\n",
        "  ```\n",
        "* **Propósito:** inicializar el acumulador donde se construirá el **vector de contexto** (`context_vec_2`) mediante una combinación ponderada de todos los embeddings de entrada.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 3**\n",
        "\n",
        "```python\n",
        "for i, x_i in enumerate(inputs):\n",
        "    context_vec_2 += attn_weights_2_softmax[i] * x_i\n",
        "```\n",
        "\n",
        "* **Qué hace:** recorre cada token de la secuencia (`x_i`) y acumula su contribución ponderada al vector de contexto, usando los **pesos de atención** calculados previamente.\n",
        "* **Desglose técnico:**\n",
        "\n",
        "  * `enumerate(inputs)` → itera sobre los 6 vectores de la secuencia:\n",
        "\n",
        "    * `i`: índice del token (0 a 5).\n",
        "    * `x_i`: vector de embedding de ese token (`tensor([3])`).\n",
        "  * `attn_weights_2_softmax[i]`: peso de atención asociado al token `i` (escalar).\n",
        "  * Multiplicación escalar–vectorial:\n",
        "\n",
        "    * `attn_weights_2_softmax[i] * x_i` → pondera el vector `x_i` según su relevancia respecto al *query*.\n",
        "  * Acumulación:\n",
        "\n",
        "    * `context_vec_2 += ...` → suma todas las contribuciones ponderadas en un solo vector resultante.\n",
        "* **Operación matemática:**\n",
        "  [\n",
        "  \\text{context_vec}*2 = \\sum*{i=1}^{6} \\alpha_i , x_i\n",
        "  ]\n",
        "  donde:\n",
        "\n",
        "  * ( \\alpha_i ) son los pesos de atención (`attn_weights_2_softmax[i]`).\n",
        "  * ( x_i ) son los embeddings de los tokens.\n",
        "* **Propósito:** obtener una **representación contextualizada** del token “journey”, que incorpora información de todos los demás tokens ponderada según su relevancia.\n",
        "\n",
        "---\n",
        "\n",
        "**Línea 4**\n",
        "\n",
        "```python\n",
        "print(f'Vector de contexto= {context_vec_2}')\n",
        "```\n",
        "\n",
        "* **Qué hace:** imprime el vector de contexto resultante.\n",
        "* **Salida esperada (valores aproximados):**\n",
        "\n",
        "  ```\n",
        "  Vector de contexto= tensor([0.4670, 0.6510, 0.5820])\n",
        "  ```\n",
        "* **Interpretación:**\n",
        "\n",
        "  * Este vector es la **nueva representación** del token “journey” tras el mecanismo de atención.\n",
        "  * Cada componente combina los valores de los embeddings originales, modulados por los pesos de atención.\n",
        "  * Refleja qué partes del contexto (otros tokens) son más relevantes para interpretar “journey”.\n",
        "\n",
        "---\n",
        "\n",
        "**Resumen conceptual:**\n",
        "Este bloque implementa manualmente la **etapa de agregación (weighted sum)** del mecanismo de *self-attention*.\n",
        "\n",
        "* **Entrada:**\n",
        "\n",
        "  * `attn_weights_2_softmax` → distribución de atención (*qué tokens mirar*).\n",
        "  * `inputs` → representaciones de los tokens (*qué información usar*).\n",
        "* **Operación:**\n",
        "\n",
        "  * Combina todos los embeddings de entrada en una media ponderada por los pesos de atención.\n",
        "* **Salida:**\n",
        "\n",
        "  * `context_vec_2`: vector contextualizado del token *query*.\n",
        "\n",
        "En la formulación completa del Transformer, este paso corresponde a:\n",
        "[\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
        "]\n",
        "donde el producto ( QK^\\top ) genera los puntajes (`attn_scores`), el *softmax* produce los pesos (`attn_weights`), y esta suma ponderada implementa la multiplicación final con los vectores ( V ) (aquí equivalentes a `inputs`).\n"
      ],
      "metadata": {
        "id": "57zju4_MzI9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "***\n",
        "***\n",
        "# HASTA ACÁ LLEGUE VIERNES 24-10-2025\n",
        "***\n",
        "***\n",
        "***"
      ],
      "metadata": {
        "id": "eax2dgeW5UQB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YXQp2jOX4CdK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}